#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
================================================================================
|    PIPELINE DE AN√ÅLISE DE NOT√çCIAS - USABILIDADE INTELIGENTE (v6.3)      |
================================================================================
| MELHORIAS DESTA VERS√ÉO (v6.3 - Resumos Din√¢micos e Relat√≥rio Interativo):|
| 1. Sistema Hier√°rquico de Relev√¢ncia:                                    |
|    - P1_CRITICO: Impacto imediato em decis√µes estrat√©gicas e financeiras |
|    - P2_ESTRATEGICO: Tend√™ncias setoriais e oportunidades m√©dio prazo    |
|    - P3_MONITORAMENTO: Contexto de mercado e informa√ß√µes gerais          |
|                                                                            |
| 2. Resumos Din√¢micos por Prioridade:                                     |
|    - P1_CRITICO: Resumos executivos detalhados (2-3 par√°grafos)          |
|    - P2_ESTRATEGICO: Resumos padr√£o densos (1 par√°grafo)                 |
|    - P3_MONITORAMENTO: Resumos concisos (1-2 frases)                     |
|                                                                            |
| 3. Relat√≥rio Interativo no Word:                                         |
|    - T√≠tulos recolh√≠veis com navega√ß√£o por se√ß√µes                        |
|    - Painel de navega√ß√£o ativo para acesso r√°pido                        |
|    - Guia de leitura integrado no documento                              |
|    - Experi√™ncia de usu√°rio otimizada para consumo executivo             |
|                                                                            |
| 4. Performance e Robustez Mantidas:                                      |
|    - Processamento paralelo preservado (10+15 workers)                   |
|    - Mapeamento robusto por IDs e cache inteligente                      |
|    - Formata√ß√£o profissional de fontes √∫nicas                           |
--------------------------------------------------------------------------------
"""


import os
import re
import json
import glob
import time
from datetime import datetime
from typing import List, Dict, Any, Optional, Literal

# Importa√ß√µes para Manipula√ß√£o de Arquivos
from docx import Document
from docx.shared import Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.ns import qn
from docx.oxml import OxmlElement

# Importa√ß√£o da API do Google Gemini
from google import genai
from google.genai import types

# Importa√ß√µes do Pydantic para Valida√ß√£o de Dados
from pydantic import BaseModel, Field, ValidationError

# Importa√ß√£o do PyMuPDF para Chunking de PDFs
import fitz  # PyMuPDF

# Importa√ß√£o para processamento paralelo
import concurrent.futures

# --- CONSTANTES DE CONFIGURA√á√ÉO ---
PDF_DIRECTORY = 'pdfs'
OUTPUT_DIRECTORY = 'relatorios_gerados'
CACHE_DIRECTORY = 'cache_noticias'

# --- CONFIGURA√á√ïES ANTI-TRUNCAMENTO ---
PAGINAS_POR_CHUNK = 5  # N√∫mero de p√°ginas por peda√ßo ao dividir PDFs grandes
LIMITE_PAGINAS_CHUNKING = 50  # PDFs com mais p√°ginas ser√£o automaticamente divididos

# Importa√ß√£o para arquivos tempor√°rios √∫nicos
import tempfile
import threading

# ==============================================================================
# 1. FUN√á√ïES AUXILIARES PARA MIGRA√á√ÉO DE CACHE
# ==============================================================================

def migrar_noticia_cache_legado(noticia_data):
    """
    Migra dados de cache antigos para o novo formato com relevance_score e relevance_reason.
    Se os campos estiverem ausentes, adiciona valores padr√£o baseados na prioridade.
    """
    # Se j√° tem os campos novos, retorna como est√°
    if 'relevance_score' in noticia_data and 'relevance_reason' in noticia_data:
        return noticia_data
    
    # Migra√ß√£o baseada na prioridade
    prioridade = noticia_data.get('prioridade', 'P3_MONITORAMENTO')
    
    # Adiciona valores padr√£o baseados na prioridade
    if prioridade == 'P1_CRITICO':
        noticia_data['relevance_score'] = 85.0
        noticia_data['relevance_reason'] = "Migrado do cache: Classificado como P1_CRITICO"
    elif prioridade == 'P2_ESTRATEGICO':
        noticia_data['relevance_score'] = 65.0  
        noticia_data['relevance_reason'] = "Migrado do cache: Classificado como P2_ESTRATEGICO"
    else:  # P3_MONITORAMENTO
        noticia_data['relevance_score'] = 35.0
        noticia_data['relevance_reason'] = "Migrado do cache: Classificado como P3_MONITORAMENTO"
    
    return noticia_data

def validar_e_migrar_cache(noticias_cache, nome_arquivo):
    """
    Valida e migra dados de cache, tentando recuperar dados antigos quando poss√≠vel.
    """
    noticias_migradas = []
    
    for noticia_data in noticias_cache:
        try:
            # Tenta migrar dados antigos
            noticia_migrada = migrar_noticia_cache_legado(noticia_data)
            
            # Valida com o modelo Pydantic
            noticia_validada = Noticia(**noticia_migrada)
            noticias_migradas.append(noticia_validada.model_dump())
            
        except ValidationError as e:
            # Se ainda assim falhou na valida√ß√£o, pula esta not√≠cia
            print(f"   ‚ö†Ô∏è Not√≠cia inv√°lida ignorada no cache de {nome_arquivo}: {e}")
            continue
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erro ao migrar not√≠cia do cache de {nome_arquivo}: {e}")
            continue
    
    return noticias_migradas

# ==============================================================================
# 2. MODELOS DE VALIDA√á√ÉO DE DADOS COM PYDANTIC
# ==============================================================================

class Noticia(BaseModel):
    """Modelo de valida√ß√£o para not√≠cias extra√≠das dos PDFs."""
    titulo: str = Field(..., min_length=1, description="T√≠tulo da not√≠cia")
    texto_completo: str = Field(..., min_length=1, description="Texto completo da not√≠cia")
    jornal: str = Field(..., min_length=1, description="Nome do jornal")
    autor: Optional[str] = Field(default="N/A", description="Autor da not√≠cia")
    pagina: Optional[str] = Field(default=None, description="P√°gina onde a not√≠cia foi encontrada")
    data: Optional[str] = Field(default=None, description="Data de publica√ß√£o")
    categoria: Optional[str] = Field(default=None, description="Categoria da not√≠cia")
    # Tag de classifica√ß√£o (rigorosa - apenas as 4 v√°lidas)
    tag: Literal['Governo e Politica', 'Economia e Tecnologia', 'Judicionario', 'Empresas Privadas']
    # NOVO CAMPO: Adiciona a prioridade para filtragem
    prioridade: Literal['P1_CRITICO', 'P2_ESTRATEGICO', 'P3_MONITORAMENTO'] = Field(..., description="N√≠vel de prioridade da not√≠cia")
    
    # --- NOVO CAMPO PARA A URL ---
    url: Optional[str] = Field(default=None, description="URL da fonte original da not√≠cia")
    
    # --- CAMPOS PARA RANKING E EXPLAINABILITY (AGORA OPCIONAIS) ---
    relevance_score: Optional[float] = Field(default=None, ge=0, le=100, description="Score de 0 a 100 da relev√¢ncia para a mesa de Special Situations.")
    relevance_reason: Optional[str] = Field(default=None, description="Justificativa curta em qual regra/assunto a not√≠cia se encaixa.")
    
    def model_post_init(self, __context):
        """Adiciona valores padr√£o para campos ausentes ap√≥s a valida√ß√£o inicial."""
        if self.relevance_score is None or self.relevance_reason is None:
            # Aplica migra√ß√£o baseada na prioridade
            if self.prioridade == 'P1_CRITICO':
                self.relevance_score = self.relevance_score or 85.0
                self.relevance_reason = self.relevance_reason or "Migrado: Classificado como P1_CRITICO"
            elif self.prioridade == 'P2_ESTRATEGICO':
                self.relevance_score = self.relevance_score or 65.0
                self.relevance_reason = self.relevance_reason or "Migrado: Classificado como P2_ESTRATEGICO"
            else:  # P3_MONITORAMENTO
                self.relevance_score = self.relevance_score or 35.0
                self.relevance_reason = self.relevance_reason or "Migrado: Classificado como P3_MONITORAMENTO"

class NoticiaResumida(BaseModel):
    """Modelo para not√≠cias resumidas usadas no agrupamento."""
    titulo: str = Field(..., min_length=1, description="T√≠tulo da not√≠cia")
    jornal: str = Field(..., min_length=1, description="Nome do jornal")



class FonteResumo(BaseModel):
    """Modelo para fontes de um resumo."""
    jornal: Optional[str] = Field(default=None, description="Nome do jornal")
    pagina: Optional[str] = Field(default=None, description="P√°gina da not√≠cia")
    autor: Optional[str] = Field(default=None, description="Autor da not√≠cia")
    # --- NOVO CAMPO PARA A URL DA FONTE ---
    url: Optional[str] = Field(default=None, description="URL da fonte")

class ResumoFinal(BaseModel):
    """Modelo de valida√ß√£o para resumos finais."""
    titulo_final: str = Field(..., min_length=1, description="T√≠tulo final do resumo")
    resumo_final: str = Field(..., min_length=1, description="Resumo consolidado")
    fontes: Optional[List[FonteResumo]] = Field(default=[], description="Lista de fontes")
    tag: Literal['Governo e Politica', 'Economia e Tecnologia', 'Judicionario', 'Empresas Privadas']
    # NOVO CAMPO: Adiciona a prioridade para ordena√ß√£o do relat√≥rio
    prioridade: Literal['P1_CRITICO', 'P2_ESTRATEGICO', 'P3_MONITORAMENTO'] = Field(..., description="Prioridade m√°xima do grupo de not√≠cias")

# ==============================================================================
# 3. FUN√á√ÉO AUXILIAR PARA HIPERLINKS NO WORD
# ==============================================================================

def add_hyperlink(paragraph, text, url):
    """
    Adiciona um hiperlink a um par√°grafo.

    Args:
        paragraph: O objeto de par√°grafo do python-docx.
        text: O texto a ser exibido para o link.
        url: A URL para a qual o link aponta.

    Returns:
        O objeto 'run' do hiperlink.
    """
    part = paragraph.part
    r_id = part.relate_to(url, "http://schemas.openxmlformats.org/officeDocument/2006/relationships/hyperlink", is_external=True)

    hyperlink = OxmlElement('w:hyperlink')
    hyperlink.set(qn('r:id'), r_id)

    new_run = OxmlElement('w:r')
    rPr = OxmlElement('w:rPr')

    # Estilo do link (azul e sublinhado)
    c = OxmlElement('w:color')
    c.set(qn('w:val'), '0000FF')
    rPr.append(c)
    u = OxmlElement('w:u')
    u.set(qn('w:val'), 'single')
    rPr.append(u)

    new_run.append(rPr)
    new_run.text = text
    hyperlink.append(new_run)

    paragraph._p.append(hyperlink)

    return hyperlink

# ==============================================================================
# 4. LISTA HIER√ÅRQUICA DE RELEV√ÇNCIA (v6.0 - com Prioridades)
# ==============================================================================

LISTA_RELEVANCIA_HIERARQUICA = {
    "P1_CRITICO": {
        "descricao": "OPORTUNIDADES ACION√ÅVEIS AGORA: Situa√ß√µes de estresse financeiro, M&A e arbitragem legal que demandam a√ß√£o imediata.",
        "assuntos": [
            # Mapeado de f-2, f-3: O n√∫cleo de Special Situations
            "Recupera√ß√£o Judicial (RJ)", "Fal√™ncia", "Pedido de Fal√™ncia", "Assembleia de Credores",
            # Mapeado de f-4, f-5: Ativos estressados
            "Cr√©ditos Inadimplentes (NPLs)", "Cr√©ditos Podres (Distressed Debt)", "Venda de Carteira de NPL",
            # Mapeado de g-2, g-4: Oportunidades de arbitragem legal/fiscal
            "Cr√©dito Tribut√°rio (teses, oportunidades de monetiza√ß√£o)", "Disputas Societ√°rias Relevantes",
            # Mapeado de f-6, f-7: Ativos espec√≠ficos do governo
            "FCVS (Fundo de Compensa√ß√£o de Varia√ß√µes Salariais - apenas not√≠cias sobre liquida√ß√£o ou venda de cr√©ditos)",
            "D√≠vida Ativa (apenas not√≠cias sobre venda de grandes blocos ou securitiza√ß√£o)",
            # Mapeado de f-1 e feedback de classificados: Oportunidades imobili√°rias de grande porte
            "Leil√µes Judiciais de Ativos (apenas im√≥veis ou participa√ß√µes societ√°rias acima de R$10 milh√µes)",
            # Eventos corporativos cr√≠ticos
            "Fus√µes e Aquisi√ß√µes (M&A) - Anunciadas ou em negocia√ß√£o avan√ßada",
            "Crise de Liquidez Aguda", "Quebra de Covenants", "Default de D√≠vida"
        ],
        "empresas": [ # Empresas cujo estresse financeiro ou M&A √© P1
             # Mapeado de j, k e feedback
            "Americanas S.A.", "Oi S.A.", "Casas Bahia (Grupo P√£o de A√ß√∫car)", "Light S.A.",
            "Gol Linhas A√©reas", "Azul Linhas A√©reas", "Petrobras", "Vale",
            "IRB Resseguradora (IRB Brasil RE)"
        ]
    },
    "P2_ESTRATEGICO": {
        "descricao": "MONITORAMENTO ESTRAT√âGICO: Tend√™ncias e eventos que podem gerar oportunidades P1 no futuro.",
        "assuntos": [
            # Mapeado de g-1, g-2, g-3: Mudan√ßas que podem gerar futuras teses
            "Mudan√ßas em Legisla√ß√£o (Tribut√°ria, Societ√°ria, Falimentar, Precat√≥rios)",
            # Mapeado de a-2, c-1, c-2, c-3, b-1: Setores de alto capital e tecnologia
            "Intelig√™ncia Artificial (IA - apenas grandes movimentos de mercado, M&A no setor ou regula√ß√£o pesada)",
            "Semicondutores (geopol√≠tica da cadeia de suprimentos, grandes investimentos/f√°bricas)",
            "Energia Nuclear (grandes projetos, concess√µes, marco regulat√≥rio)",
            "Aeroespacial e Defesa (grandes contratos governamentais, privatiza√ß√µes)",
            # Pol√≠tica e regula√ß√£o com impacto direto
            "Pol√≠tica Econ√¥mica (Decis√µes de juros e pol√≠tica fiscal que afetem o cr√©dito e a sa√∫de financeira das empresas)",
            "Decis√µes do CADE (bloqueio de fus√µes, imposi√ß√£o de rem√©dios)",
            "Ativismo Acion√°rio (grandes investidores tentando influenciar a gest√£o)"
        ],
        "empresas": [ # Empresas para monitoramento cont√≠nuo de resultados e movimentos estrat√©gicos
            # Mapeado de h (Big Techs)
            "Alphabet", "AMD", "Apple", "Google", "Intel", "Intuitive Machines", "Meta",
            "Micron Technology", "Microsoft", "Netflix", "Tesla", "Nvidia",
            # Mapeado de i (Energia)
            "Constellation Energy Group", "Siemens Energy AG",
            # Mapeado de j, k (Bancos e Seguros)
            "Banco Master", "Banco Pan", "Caixa Econ√¥mica Federal", "PREVIC"
        ]
    },
    "P3_MONITORAMENTO": {
        "descricao": "CONTEXTO DE MERCADO: Informa√ß√µes gerais para entendimento do cen√°rio macro, sem a√ß√£o direta.",
        "assuntos": [
            # Mapeado de d (Cripto)
            "Criptomoedas (apenas vis√£o macro de mercado, ado√ß√£o institucional ou regula√ß√£o. Sem an√°lise t√©cnica de moedas espec√≠ficas).",
            # Cen√°rio internacional
            "Geoeconomia", "Acordos Comerciais (Mercosul-UE, etc.)", "Decis√µes do FED e BCE",
            # Mapeado de e-1: Games (apenas M&A no setor)
            "Games (apenas not√≠cias sobre grandes fus√µes e aquisi√ß√µes, ex: Microsoft comprando Activision)"
        ],
        "empresas": [],
        "regras_especiais": {
            "balancos_financeiros": "Not√≠cias sobre balan√ßos s√≥ s√£o relevantes se (A) forem de empresas listadas em P2, ou (B) indicarem um estresse financeiro severo (risco de RJ/Fal√™ncia), tornando-as P1.",
            "classificados_e_leiloes": "An√∫ncios de classificados s√£o 99% irrelevantes. A √∫nica exce√ß√£o √© um leil√£o judicial ou venda de um ativo √∫nico de alt√≠ssimo valor (>R$10M), que deve ser tratado como P1."
        }
    }
}

# Criar uma vers√£o em string para ser usada nos prompts
def gerar_lista_relevancia_para_prompt():
    texto_prompt = ""
    for prioridade, data in LISTA_RELEVANCIA_HIERARQUICA.items():
        texto_prompt += f"**{prioridade} ({data['descricao']})**\n"
        if data['assuntos']:
            texto_prompt += "- Assuntos: " + ", ".join(data['assuntos']) + "\n"
        if data['empresas']:
            texto_prompt += "- Empresas: " + ", ".join(data['empresas']) + "\n"
        texto_prompt += "\n"
    return texto_prompt

LISTA_RELEVANCIA_FORMATADA = gerar_lista_relevancia_para_prompt()

# ==============================================================================
# 5. PROMPTS DETALHADOS PARA O PIPELINE DE IA (v5.0)
# ==============================================================================

# # Prompt original mais restritivo para uso em caso de necessidade
# PROMPT_EXTRACAO_TOLERANCIA_ZERO_V7 = f"""
# Sua identidade: Voc√™ √© um analista s√™nior da mesa de 'Special Situations' do banco BTG Pactual. Seu b√¥nus depende da qualidade do seu filtro.

# **A REGRA DE OURO (SEU FILTRO MENTAL):**
# Ao ler uma not√≠cia, fa√ßa a si mesmo esta √∫nica pergunta: **"Isto aponta para um ativo estressado, uma assimetria de informa√ß√£o ou uma inefici√™ncia de mercado que pode ser monetizada HOJE?"** Se a resposta n√£o for um "SIM" √≥bvio, **REJEITE IMEDIATAMENTE.**

# --------------------------------------------------------------------------------
# **ETAPA 1: AN√ÅLISE E CLASSIFICA√á√ÉO**
# --------------------------------------------------------------------------------
# 1.  Avalie a not√≠cia contra a **LISTA DE REJEI√á√ÉO IMEDIATA**. Se houver correspond√™ncia, descarte.
# 2.  Avalie contra a **LISTA DE INTERESSES**. Se n√£o houver correspond√™ncia clara, descarte.
# 3.  Se a not√≠cia passar, voc√™ DEVE atribuir um **Score de Relev√¢ncia** e uma **Justificativa**.

# **LISTA DE INTERESSES (FOCO EXCLUSIVO):**
# {LISTA_RELEVANCIA_FORMATADA}

# **LISTA DE REJEI√á√ÉO IMEDIATA (SE A NOT√çCIA FOR SOBRE ISTO, √â LIXO):**
# - **Fofoca Pol√≠tica e Partid√°ria:** Disputas no congresso, popularidade de pol√≠ticos, elei√ß√µes, vida pessoal de figuras p√∫blicas.
# - **Legisla√ß√£o Social e Ambiental Gen√©rica:** Leis de cotas, regras de EAD, Lei Rouanet, ESG, cr√©ditos de carbono, licenciamento ambiental. (Exce√ß√£o: se levar diretamente √† fal√™ncia de uma empresa relevante).
# - **Opera√ß√µes do Dia a Dia do Governo:** Restitui√ß√£o de IR, filas do INSS, dados do IBGE, protocolos de blitz.
# - **Crimes, Golpes e Seguran√ßa P√∫blica:** Golpes virtuais, fraudes, disputas criminais de baixo impacto.
# - **Eventos, Semin√°rios, Cultura e Esportes:** Cobertura de f√≥runs, not√≠cias sobre games (exceto M&A), parques tem√°ticos, crises em clubes de futebol.
# - **An√∫ncios Publicit√°rios e Classificados Gen√©ricos.**

# --------------------------------------------------------------------------------
# **ETAPA 2: EXTRA√á√ÉO (APENAS SE APROVADO)**
# --------------------------------------------------------------------------------
# Se a not√≠cia for relevante, extraia os dados no formato JSON abaixo.

# - **`relevance_score`**: Score de 0-100. P1 (RJ, Fal√™ncia, M&A direto) deve ser > 85. P2 (Regulat√≥rio, Tend√™ncias) entre 50-85. P3 (Contexto Macro) < 50.
# - **`relevance_reason`**: Justifique sua decis√£o em uma frase. Ex: "Encaixa-se na regra P1-CRITICO: Recupera√ß√£o Judicial".
# - **`tag`**: Use uma das 4 tags: 'Governo e Politica', 'Economia e Tecnologia', 'Judicionario', 'Empresas Privadas'.

# **FORMATO DE SA√çDA (JSON PURO):**
# ```json
# [
#   {{
#     "titulo": "...",
#     "texto_completo": "Resumo focado na tese de investimento...",
#     "jornal": "...",
#     "autor": "...",
#     "pagina": "...",
#     "data": "...",
#     "categoria": "O item mais espec√≠fico da LISTA DE INTERESSES (ex: Recupera√ß√£o Judicial)",
#     "tag": "Uma das 4 tags v√°lidas",
#     "prioridade": "A prioridade correta (P1_CRITICO, P2_ESTRATEGICO, ou P3_MONITORAMENTO)",
#     "relevance_score": 95.0,
#     "relevance_reason": "Encaixa-se na regra P1-CRITICO: Pedido de Fal√™ncia de empresa relevante do setor a√©reo."
#   }}
# ]
# ```
# Seja brutal no filtro. Se nenhuma not√≠cia passar, retorne [].
# """

# Prompt mais permissivo para extra√ß√£o inicial (melhor captura)
PROMPT_EXTRACAO_PERMISSIVO_V8 = f"""
Sua identidade: Voc√™ √© um analista junior da mesa de 'Special Situations' do banco BTG Pactual. Sua fun√ß√£o √© fazer uma primeira triagem ampla de not√≠cias.

**OBJETIVO:** Capturar TODAS as not√≠cias que possam ter alguma relev√¢ncia para decis√µes de investimento, mesmo que remotamente. 
√â melhor incluir uma not√≠cia desnecess√°ria do que perder uma oportunidade importante.

--------------------------------------------------------------------------------
**LISTA DE REJEI√á√ÉO IMEDIATA (SE FOR SOBRE ISSO, √â RU√çDO):**
--------------------------------------------------------------------------------
- **Crimes e Seguran√ßa P√∫blica Cotidiana:** Pris√µes individuais (como a do homem em Roraima), estat√≠sticas gen√©ricas de crimes (feminic√≠dios, latroc√≠nios do Anu√°rio de Seguran√ßa), opera√ß√µes policiais de rotina, fraudes comuns. **Exce√ß√£o:** Apenas se envolver diretamente uma empresa P1 ou P2 em um esquema de corrup√ß√£o de grande escala.
- **Esportes:** Finan√ßas de clubes (como a falta de verba do Corinthians), resultados de jogos, contrata√ß√µes. **Exce√ß√£o:** Apenas se houver um processo de Recupera√ß√£o Judicial ou M&A de uma SAF (Sociedade An√¥nima do Futebol) relevante.
- **Cultura, Fofoca e Entretenimento:** Vidas de celebridades (morte de Hulk Hogan), festivais (Dan√ßa de Joinville), lan√ßamentos de musicais, moda (Flavia Aranha), disputas judiciais de natureza pessoal (Juliana Oliveira vs. SBT).
- **Assuntos Locais sem Impacto Sist√™mico:** Disputas de bairro (pr√©dio em Higien√≥polis), classificados e leil√µes de baixo valor (Rico Leil√£o de ve√≠culos).
- **Pol√≠tica Partid√°ria Pura:** Disputas internas de partidos, fofocas de bastidores, popularidade de pol√≠ticos. **Exce√ß√£o:** Decis√µes de pol√≠tica econ√¥mica com impacto direto.

**FOCO PRINCIPAL - CAPTURE SE A NOT√çCIA FOR SOBRE:**
{LISTA_RELEVANCIA_FORMATADA}

**REJEITE APENAS SE FOR CLARAMENTE IRRELEVANTE:**
- Esportes (exceto aspectos financeiros de clubes)
- Entretenimento e cultura (exceto aspectos de neg√≥cio)
- Crimes cotidianos sem impacto empresarial
- Pol√≠tica partid√°ria pura (exceto pol√≠ticas econ√¥micas)
- Eventos sociais e celebridades

**INSTRU√á√ïES DE EXTRA√á√ÉO:**
1. **Seja GENEROSO** na classifica√ß√£o inicial - prefira incluir do que excluir
2. Para not√≠cias de fronteira, classifique como P3_MONITORAMENTO 
3. Use scores mais baixos (30-50) para not√≠cias incertas, mas INCLUA elas
4. O filtro rigoroso ser√° aplicado later no pipeline

**INSTRU√á√ïES DE EXTRA√á√ÉO E SCORING:**
- **`prioridade`**: Atribua P1, P2 ou P3 RIGOROSAMENTE com base na lista. Uma not√≠cia sobre "Recupera√ß√£o Judicial" √© SEMPRE `P1_CRITICO`.
- **`relevance_score`**: Seja conservador.
    - **P1_CRITICO (Score 85-100):** Apenas para eventos acion√°veis (RJ, Fal√™ncia, M&A anunciado).
    - **P2_ESTRATEGICO (Score 50-84):** Apenas para tend√™ncias setoriais e mudan√ßas regulat√≥rias claras.
    - **P3_MONITORAMENTO (Score 20-49):** Para contexto macroecon√¥mico e not√≠cias de empresas monitoradas que n√£o se encaixam em P1/P2.
- **`tag`**: Classifique com precis√£o


**FORMATO DE SA√çDA (JSON PURO):**
```json
[
  {{
    "titulo": "...",
    "texto_completo": "Resumo da not√≠cia...",
    "jornal": "...",
    "autor": "...",
    "pagina": "...",
    "data": "...",
    "categoria": "Categoria da LISTA DE INTERESSES ou 'Geral'",
    "tag": "Uma das 4 tags: 'Governo e Politica', 'Economia e Tecnologia', 'Judicionario', 'Empresas Privadas'",
    "prioridade": "P1_CRITICO, P2_ESTRATEGICO, ou P3_MONITORAMENTO",
    "relevance_score": 45.0,
    "relevance_reason": "Not√≠cia sobre setor relevante para acompanhamento"
  }}
]
```

**LEMBRE-SE:** √â melhor capturar 100 not√≠cias e depois filtrar para 20, do que capturar apenas 5 e perder oportunidades importantes.
"""

# Prompt para os resumos de 1 par√°grafo do TOP 12
PROMPT_RESUMO_CRITICO_V1 = """
Voc√™ √© um analista de investimentos escrevendo um briefing para o comit√™ executivo.
Sua tarefa √© resumir o seguinte evento em um **√∫nico par√°grafo conciso de, no m√°ximo, 5 linhas.**
Foque nos fatos essenciais: Quem, O qu√™, Quando, Onde e Qual a implica√ß√£o financeira ou oportunidade de neg√≥cio.
Ignore detalhes secund√°rios. Seja direto e informativo.

DADOS DO EVENTO PARA RESUMIR:
{DADOS_DO_GRUPO}

FORMATO DE SA√çDA OBRIGAT√ìRIO (JSON PURO):
```json
{{
  "resumo_final": "Seu resumo executivo de um par√°grafo aqui."
}}
```
"""

# Prompt para os resumos de 1 linha do RADAR
PROMPT_RADAR_MONITORAMENTO_V1 = """
Voc√™ est√° criando um "Radar de Monitoramento" para executivos. Sua tarefa √© transformar as not√≠cias do cluster abaixo em UM √öNICO bullet point de UMA LINHA, come√ßando com a entidade principal.

IMPORTANTE: Crie APENAS um bullet point por cluster, consolidando todas as not√≠cias relacionadas em uma √∫nica linha informativa.

Exemplos:
- Cluster sobre iFood: "iFood: em negocia√ß√µes para adquirir a Alelo por R$ 5 bilh√µes"
- Cluster sobre governo: "Mercado de Carbono: governo adia a cria√ß√£o de ag√™ncia reguladora para o setor"

DADOS DO CLUSTER PARA TRANSFORMAR EM BULLET POINT:
{DADOS_DO_GRUPO}

FORMATO DE SA√çDA OBRIGAT√ìRIO (JSON PURO):
```json
{{
  "bullet_point": "Entidade: resumo consolidado das not√≠cias do cluster em uma linha"
}}
```
"""


PROMPT_AGRUPAMENTO_CONSOLIDADO_V2 = """
Voc√™ √© um especialista em an√°lise de conte√∫do focado em granularidade. Sua tarefa √© agrupar not√≠cias de uma lista JSON que se referem exatamente ao mesmo fato gerador. Diferentes jornais cobrir√£o o mesmo fato com t√≠tulos distintos, e sua miss√£o √© identific√°-los com precis√£o.

**DIRETRIZES DE AGRUPAMENTO:**

1.  **INTEGRIDADE TOTAL:** TODAS as not√≠cias da entrada DEVEM ser alocadas a um grupo. Not√≠cias sem par formam um grupo de um √∫nico item. NENHUMA not√≠cia pode ser descartada.

2.  **FOCO NO N√öCLEO SEM√ÇNTICO:** O que realmente aconteceu? Qual foi a decis√£o, o an√∫ncio ou o evento?
    - **EXEMPLO DE AGRUPAR (MESMO FATO):**
      - Not√≠cia 1: "Alexandre de Moraes decide n√£o prender Jair Bolsonaro, mas confirma descumprimento de restri√ß√µes."
      - Not√≠cia 2: "Ministro do STF n√£o decreta pris√£o preventiva de Jair Bolsonaro, mas mant√©m e esclarece cautelares."
      - **An√°lise:** O n√∫cleo sem√¢ntico √© o mesmo: a decis√£o de Moraes de n√£o prender Bolsonaro, mantendo as restri√ß√µes. **Devem estar no mesmo grupo.**

3.  **DESDOBRAMENTOS PODEM SER AGRUPADOS NA MESMA NOTICIA DEIXANDO:** Uma a√ß√£o e a rea√ß√£o a ela s√£o O MESMO FATO EM MOMENTOS DIFERENTES.
      - Grupo A: "Governo anuncia nova pol√≠tica de pre√ßos."
      - Grupo B: "Setor industrial critica nova pol√≠tica de pre√ßos."

4.  **TEMA PRINCIPAL PRECISO:** O `tema_principal` deve descrever o fato gerador de forma neutra e espec√≠fica. Deve ser o t√≠tulo da a√ß√£o consolidada.

5.  **MAPEAMENTO POR ID:** Use os `ids_originais` fornecidos para garantir a correspond√™ncia.

**FORMATO DE ENTRADA (EXEMPLO):**
[
 {"id": 0, "titulo": "Apple lan√ßa iPhone 20", "jornal": "Jornal Tech", "trecho": "..."},
 {"id": 1, "titulo": "Novo iPhone 20 da Apple chega ao mercado", "jornal": "Jornal Varejo", "trecho": "..."},
 {"id": 2, "titulo": "Analistas reagem ao lan√ßamento do iPhone 20", "jornal": "Jornal Mercado", "trecho": "..."},
 {"id": 3, "titulo": "Tesla anuncia novo carro el√©trico", "jornal": "Jornal Auto", "trecho": "..."}
]

**FORMATO DE SA√çDA OBRIGAT√ìRIO (JSON PURO - USANDO 'ids_originais'):**
```json
[
 {
   "tema_principal": "Apple lan√ßa o novo iPhone 20",
   "ids_originais": [0, 1]
 },
 {
   "tema_principal": "Alexandre de Moraes manda prender Jair Bolsonaro",
   "ids_originais": [2]
 },
 {
   "tema_principal": "Tesla anuncia novo modelo de carro el√©trico",
   "ids_originais": [3]
 }
]
```
"""

PROMPT_RESUMO_FINAL_V3 = """
Voc√™ √© um analista de intelig√™ncia criando um resumo sobre um evento espec√≠fico, baseado em um CLUSTER de not√≠cias relacionadas. A profundidade do seu resumo deve variar conforme o **N√≠vel de Detalhe** solicitado.

**IMPORTANTE:** Voc√™ est√° resumindo um CLUSTER DE NOT√çCIAS sobre o mesmo fato gerador, n√£o sess√µes separadas. Combine todas as informa√ß√µes das not√≠cias do cluster em um resumo coerente e abrangente.

**N√çVEIS DE DETALHE:**
-   **Executivo (P1_CRITICO):** Um resumo aprofundado de 2 a 3 par√°grafos. Detalhe o contexto, os principais dados (valores, percentuais), os players envolvidos e as implica√ß√µes estrat√©gicas (riscos/oportunidades). Seja completo.
-   **Padr√£o (P2_ESTRATEGICO):** Um √∫nico par√°grafo denso e informativo que sintetiza os fatos mais importantes do evento, combinando informa√ß√µes de todas as not√≠cias do cluster.
-   **Conciso (P3_MONITORAMENTO):** Uma √∫nica frase (m√°ximo duas) que captura a ess√™ncia do evento consolidado.

**MISS√ÉO:**
Baseado no CLUSTER de not√≠cias fornecido e no **N√≠vel de Detalhe** `{NIVEL_DE_DETALHE}` solicitado, produza um resumo consolidado que integre todas as informa√ß√µes do cluster.

**FORMATO DE SA√çDA OBRIGAT√ìRIO (JSON PURO):**
```json
{{
  "titulo_final": "Use exatamente o tema_principal fornecido no cluster.",
  "resumo_final": "O resumo consolidado de todas as not√≠cias do cluster conforme o N√≠vel de Detalhe especificado."
}}
```

**DADOS DO CLUSTER PARA AN√ÅLISE:**
{DADOS_DO_GRUPO}
"""

PROMPT_EXTRACAO_JSON_V1 = f"""
Voc√™ √© um classificador e sumarizador de not√≠cias. Sua fun√ß√£o √© analisar UM √öNICO item de not√≠cia (t√≠tulo e texto completo) que j√° foi pr√©-extra√≠do de um site.

--------------------------------------------------------------------------------
**ETAPA 1: AN√ÅLISE DE RELEV√ÇNCIA E SUMARIZA√á√ÉO**
--------------------------------------------------------------------------------
1.  **Relev√¢ncia:** Compare o conte√∫do da not√≠cia com a `LISTA DE RELEV√ÇNCIA` abaixo. A not√≠cia DEVE ser sobre um dos t√≥picos de interesse e NENHUM dos t√≥picos proibidos.
2.  **Sumariza√ß√£o:** Se a not√≠cia for relevante, crie um resumo informativo e bem estruturado do texto original, com no m√°ximo 5 par√°grafos, focando em fatos, dados e consequ√™ncias. Se o texto original j√° for curto e objetivo, pode transcrev√™-lo.

**LISTA DE RELEV√ÇNCIA (ALLOW LIST):**
{LISTA_RELEVANCIA_FORMATADA}

**LISTA DE ASSUNTOS PROIBIDOS (REJEI√á√ÉO IMEDIATA):**
- Cultura geral (exceto games), esportes, sa√∫de local, pol√≠tica partid√°ria gen√©rica, crimes comuns.
- Entretenimento puro sem conex√£o tecnol√≥gica ou comercial
- Crimes comuns, acidentes locais, fofocas pol√≠ticas sem impacto econ√¥mico
- Not√≠cias puramente sociais ou culturais

--------------------------------------------------------------------------------
**ETAPA 2: A√á√ÉO**
--------------------------------------------------------------------------------
-   **SE a not√≠cia for IRRELEVANTE:** Retorne um JSON com uma lista vazia: `[]`.
-   **SE a not√≠cia for RELEVANTE:** Extraia as informa√ß√µes e as retorne como um JSON contendo uma lista com UM √öNICO objeto, conforme o formato abaixo.

--------------------------------------------------------------------------------
**ETAPA 3: AUTO-VERIFICA√á√ÉO DA TAG (OBRIGAT√ìRIO)**
--------------------------------------------------------------------------------
Sua tarefa mais cr√≠tica √© atribuir a tag correta. A `tag` DEVE ser UMA e APENAS UMA das 5 op√ß√µes a seguir. Analise os exemplos positivos e negativos para cada uma antes de decidir.

**1. TAG: 'Internacional'**
- **Defini√ß√£o:** Eventos que ocorrem fora do Brasil ou envolvem a rela√ß√£o do Brasil com atores externos (pa√≠ses, empresas estrangeiras, bancos centrais globais).
- **Exemplos de ONDE USAR (Positivos):**
    - `Fran√ßa: Macron anuncia reconhecimento oficial do Estado da Palestina...`
    - `Estados Unidos e Israel: abandonam negocia√ß√µes de cessar-fogo em Gaza.`
    - `Donald Trump e Jerome Powell: confrontam-se publicamente na sede do Federal Reserve...`
    - `EUA: Fecham acordo comercial com Jap√£o e avan√ßam em negocia√ß√µes com a Uni√£o Europeia...`
    - `LVMH: registra queda de 3% na receita... impactada por menor demanda na China e Jap√£o.`
    - `Brasil: exporta√ß√µes de soja para a China seguem em alta...`
    - `Christine Lagarde (BCE): adota tom duro, levando mercado a rever apostas de cortes de juros...`
    - `Coreia do Sul: expande 'soft power' no Brasil via gastronomia...`
    - `Antonov An-24: queda na R√∫ssia exp√µe os desafios da avia√ß√£o russa com san√ß√µes...`
- **Exemplos de ONDE N√ÉO USAR (Negativos):**
    - Not√≠cia: `IBGE: Acesso √† internet e uso de celular no Brasil atingiram novos recordes...` -> **Tag Correta: 'Economia e Politica'**. (√â sobre dados internos do Brasil).
    - Not√≠cia: `Trigo: plantio no Sul do Brasil chega ao fim com proje√ß√µes de queda...` -> **Tag Correta: 'Economia e Politica'**. (√â sobre agricultura nacional).


**2. TAG: 'Economia e Politica' (Foco Brasil)**
- **Defini√ß√£o:** Acontecimentos da pol√≠tica e economia DOM√âSTICA do Brasil. Envolve governo federal, minist√©rios, pol√≠ticas p√∫blicas e dados macroecon√¥micos nacionais.
- **Exemplos de ONDE USAR (Positivos):**
    - `IBGE: divulga dados do IPCA-15 de julho, importante indicador da infla√ß√£o oficial do pa√≠s.`
    - `Governo Brasileiro: finaliza plano de conting√™ncia e negocia com EUA para evitar tarifa√ßo...`
    - `Arrecada√ß√£o Federal: atinge recorde de R$ 234,59 bilh√µes...`
    - `Sal√°rios: estudo da LCA 4intelligence aponta que s√£o o principal fator da infla√ß√£o de servi√ßos...`
- **Exemplos de ONDE N√ÉO USAR (Negativos):**
    - Not√≠cia: `Alexandre de Moraes: decide n√£o prender Jair Bolsonaro...` -> **Tag Correta: 'Legislativo e Judiciario'**. (Decis√£o do poder judici√°rio).
    - Not√≠cia: `Donald Trump e Jerome Powell: confrontam-se...` -> **Tag Correta: 'Internacional'**. (Pol√≠tica e economia dos EUA).
    - Not√≠cia: `Anu√°rio Brasileiro de Seguran√ßa P√∫blica: revela queda das mortes violentas...` -> **Rejeitar Not√≠cia**. (Tema indesejado de seguran√ßa p√∫blica).


**3. TAG: 'Legislativo e Judiciario' (Foco Brasil)**
- **Defini√ß√£o:** Decis√µes, julgamentos e processos dos poderes Judici√°rio e Legislativo do Brasil. Envolve STF, STJ, TJ, processos de fal√™ncia, recupera√ß√£o judicial e vota√ß√µes no Congresso.
- **Exemplos de ONDE USAR (Positivos):**
    - `Alexandre de Moraes: decide n√£o prender Jair Bolsonaro, mas confirma descumprimento de restri√ß√µes...`
    - `Jair Bolsonaro: condenado pelo TJDF a pagar R$ 150 mil por danos morais...`
    - `STJ: decide que pagamento de legado de renda vital√≠cia n√£o depende da conclus√£o do invent√°rio.`
    - `Partido Verde (PV): aciona STF para contestar artigo da Lei Anticorrup√ß√£o...`
    - `STF: suspende, por pedido de vista do ministro Fl√°vio Dino, o julgamento...`
    - `W3 Camisetas Ltda.: fal√™ncia decretada pela 2¬™ Vara Empresarial de Belo Horizonte/MG.`
    - `Belo Monte: obteve vit√≥ria judicial contra o ONS...`
- **Exemplos de ONDE N√ÉO USAR (Negativos):**
    - Not√≠cia: `Governo Brasileiro: finaliza plano de conting√™ncia...` -> **Tag Correta: 'Economia e Politica'**. (A√ß√£o do poder Executivo).
    - Not√≠cia: `Homem preso em Roraima ao sacar dinheiro roubado...` -> **Rejeitar Not√≠cia**. (Crime comum, n√£o √© uma decis√£o judicial de relev√¢ncia sist√™mica).

**4. TAG: 'Tecnologia'**
- **Defini√ß√£o:** Not√≠cias sobre inova√ß√£o, intelig√™ncia artificial, semicondutores, data centers, ciberseguran√ßa e o modelo de neg√≥cios de empresas de tecnologia.
- **Exemplos de ONDE USAR (Positivos):**
    - `Conselho Nacional de Educa√ß√£o: elabora o primeiro regramento para o uso de Intelig√™ncia Artificial...`
    - `TikTok: investir√° R$ 50 bilh√µes no Brasil, com Ministro Silveira prevendo mais aportes em tecnologia e IA...`
    - `Demanda por IA: eleva o custo de energia a recorde hist√≥rico...`
    - `Nvidia: US$ 1 bilh√£o em chips contrabandeados para a China...`
    - `A√ß√µes meme: impulsionam 'gamma squeezes' no mercado de op√ß√µes...` (Fen√¥meno de mercado com base tecnol√≥gica).
- **Exemplos de ONDE N√ÉO USAR (Negativos):**
    - Not√≠cia: `IBGE: Acesso √† internet e uso de celular no Brasil...` -> **Tag Correta: 'Economia e Politica'**. (√â um dado socioecon√¥mico, n√£o sobre a tecnologia em si).
    - Not√≠cia: `S√£o Paulo: concentra 18,5% dos roubos e furtos de celular do Brasil...` -> **Rejeitar Not√≠cia**. (√â sobre crime, n√£o tecnologia).


**5. TAG: 'Empresas Privadas'**
- **Defini√ß√£o:** A√ß√µes e movimentos espec√≠ficos de empresas, como investimentos, emiss√£o de d√≠vida, fus√µes e aquisi√ß√µes (M&A), disputas corporativas e projetos.
- **Exemplos de ONDE USAR (Positivos):**
    - `Empresas: Sabesp, Randon e Localiza est√£o entre as 26 companhias com ofertas de deb√™ntures...`
    - `Cade: arquiva inqu√©rito que investigava Globo, Disney e Warner...`
    - `Grupo Kalunga: assina acordo de sublicenciamento com a ESPN...`
    - `Helbor: Grupo de moradores de Higien√≥polis tenta barrar constru√ß√£o de pr√©dio residencial...`
- **Exemplos de ONDE N√ÉO USAR (Negativos):**
    - Not√≠cia: `LVMH: registra queda de 3% na receita... impactada por menor demanda na China e Jap√£o.` -> **Tag Correta: 'Internacional'**. (A not√≠cia √© sobre o impacto de fatores geopol√≠ticos/macroecon√¥micos na empresa).
    - Not√≠cia: `W3 Camisetas Ltda.: fal√™ncia decretada...` -> **Tag Correta: 'Legislativo e Judiciario'**. (O fato gerador √© a decis√£o judicial, n√£o uma a√ß√£o da empresa).
**ATRIBUI√á√ÉO DE PRIORIDADE:** Ao extrair, voc√™ DEVE analisar a LISTA HIER√ÅRQUICA e atribuir a prioridade correta ('P1_CRITICO', 'P2_ESTRATEGICO', ou 'P3_MONITORAMENTO') ao campo "prioridade". Not√≠cias que se encaixam em P1 devem ser sempre P1, mesmo que tamb√©m toquem em temas P2 ou P3.

**FORMATO DE SA√çDA (JSON PURO, LISTA COM UM √öNICO ITEM SE RELEVANTE):**
```json
[
  {{
    "titulo": "T√≠tulo da not√≠cia, conforme recebido",
    "autor": "N/A",
    "texto_completo": "O resumo completo e estruturado da not√≠cia, com no m√°ximo 5 par√°grafos.",
    "pagina": "N/A",
    "jornal": "Ser√° preenchido posteriormente",
    "data": "Ser√° preenchido posteriormente",
    "categoria": "Categoria espec√≠fica da LISTA DE RELEV√ÇNCIA (ex: Bitcoin, Apple, etc.)",
    "tag": "Uma das 4 tags v√°lidas",
    "prioridade": "A prioridade correta (P1_CRITICO, P2_ESTRATEGICO, ou P3_MONITORAMENTO) baseada na LISTA HIER√ÅRQUICA"
  }}
]
```
INSTRU√á√ÉO FINAL: Analise o item de not√≠cia fornecido. Se for relevante, retorne a lista com o resumo JSON. Se n√£o, retorne [].
"""

PROMPT_CORRECAO_JSON = """
A seguinte string deveria ser um JSON v√°lido contendo uma lista de objetos, mas cont√©m um erro de sintaxe que impede sua decodifica√ß√£o.
Sua √∫nica tarefa √© corrigir a sintaxe e retornar APENAS o c√≥digo JSON v√°lido e completo. N√£o adicione nenhum coment√°rio ou texto explicativo.

JSON Quebrado:
{json_quebrado}
"""

# ==============================================================================
# 6. FUN√á√ïES AUXILIARES E DE PROCESSAMENTO
# ==============================================================================

def corrigir_tag_invalida(tag_original: str) -> str:
    """
    Mapeia tags inv√°lidas ou similares para uma das 4 tags v√°lidas.
    
    Args:
        tag_original: Tag original que pode estar inv√°lida
        
    Returns:
        Tag v√°lida mapeada
    """
    if not tag_original or not isinstance(tag_original, str):
        return 'Empresas Privadas'  # Tag padr√£o
    
    tag_limpa = tag_original.strip().lower()
    
    # Mapeamento de tags similares para as corretas
    MAPEAMENTO_TAGS = {
        # Governo e Pol√≠tica
        'governo e politica': 'Governo e Politica',
        'governo e pol√≠tica': 'Governo e Politica', 
        'pol√≠tica': 'Governo e Politica',
        'politica': 'Governo e Politica',
        'governo': 'Governo e Politica',
        'pol√≠tica econ√¥mica': 'Governo e Politica',
        'politica economica': 'Governo e Politica',
        'pol√≠tica p√∫blica': 'Governo e Politica',
        'pol√≠tica publica': 'Governo e Politica',
        
        # Economia e Tecnologia  
        'economia e tecnologia': 'Economia e Tecnologia',
        'economia': 'Economia e Tecnologia',
        'tecnologia': 'Economia e Tecnologia',
        'tech': 'Economia e Tecnologia',
        'ia': 'Economia e Tecnologia',
        'intelig√™ncia artificial': 'Economia e Tecnologia',
        'inteligencia artificial': 'Economia e Tecnologia',
        'cripto': 'Economia e Tecnologia',
        'criptomoedas': 'Economia e Tecnologia',
        'bitcoin': 'Economia e Tecnologia',
        
        # Judici√°rio
        'judicionario': 'Judicionario',
        'judici√°rio': 'Judicionario',
        'judicial': 'Judicionario',
        'justi√ßa': 'Judicionario',
        'justica': 'Judicionario',
        'tribunal': 'Judicionario',
        'stf': 'Judicionario',
        'stj': 'Judicionario',
        'fal√™ncia': 'Judicionario',
        'falencia': 'Judicionario',
        'recupera√ß√£o judicial': 'Judicionario',
        'recuperacao judicial': 'Judicionario',
        
        # Empresas Privadas
        'empresas privadas': 'Empresas Privadas',
        'empresa': 'Empresas Privadas',
        'empresas': 'Empresas Privadas',
        'corporativo': 'Empresas Privadas',
        'neg√≥cios': 'Empresas Privadas',
        'negocios': 'Empresas Privadas',
        'setor privado': 'Empresas Privadas',
        'm&a': 'Empresas Privadas',
        'fus√µes e aquisi√ß√µes': 'Empresas Privadas',
        'fusoes e aquisicoes': 'Empresas Privadas',
    }
    
    # Tenta mapeamento direto
    if tag_limpa in MAPEAMENTO_TAGS:
        return MAPEAMENTO_TAGS[tag_limpa]
    
    # Tenta mapeamento por palavras-chave
    if any(palavra in tag_limpa for palavra in ['governo', 'pol√≠tico', 'politico', 'minist√©rio', 'ministerio']):
        return 'Governo e Politica'
    elif any(palavra in tag_limpa for palavra in ['tecnologia', 'economia', 'mercado', 'financeiro', 'cripto']):
        return 'Economia e Tecnologia'
    elif any(palavra in tag_limpa for palavra in ['judicial', 'tribunal', 'justi√ßa', 'justica', 'fal√™ncia', 'falencia']):
        return 'Judicionario'
    elif any(palavra in tag_limpa for palavra in ['empresa', 'corporativo', 'neg√≥cio', 'negocio', 'setor']):
        return 'Empresas Privadas'
    
    # Se nada funcionou, retorna tag padr√£o
    print(f"   ‚ö†Ô∏è Tag desconhecida '{tag_original}' mapeada para 'Empresas Privadas'")
    return 'Empresas Privadas'

def verificar_dependencias():
    """Verifica se todas as depend√™ncias necess√°rias est√£o instaladas."""
    dependencias_faltantes = []
    
    try:
        import google.generativeai
    except ImportError:
        dependencias_faltantes.append("google-generativeai")
    
    try:
        from docx import Document
    except ImportError:
        dependencias_faltantes.append("python-docx")
    
    try:
        from pydantic import BaseModel
    except ImportError:
        dependencias_faltantes.append("pydantic")
    
    try:
        import fitz  # PyMuPDF
    except ImportError:
        dependencias_faltantes.append("PyMuPDF")
    
    if dependencias_faltantes:
        print("‚ùå ERRO: Depend√™ncias n√£o encontradas!")
        print(f"   Instale com: pip install {' '.join(dependencias_faltantes)}")
        return False
    
    return True

def contar_paginas_pdf(caminho_pdf: str) -> int:
    """
    Conta o n√∫mero de p√°ginas de um PDF.
    
    Args:
        caminho_pdf: Caminho para o arquivo PDF
        
    Returns:
        N√∫mero de p√°ginas do PDF
    """
    try:
        with fitz.open(caminho_pdf) as doc:
            return doc.page_count
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao contar p√°ginas de {caminho_pdf}: {e}")
        return 0

def limpar_nome_arquivo(nome: str) -> str:
    """Limpa uma string para ser usada como um nome de arquivo seguro."""
    nome = re.sub(r'[<>:"/\\|?*]', '_', nome)
    nome = re.sub(r'\s+', '_', nome)
    nome = re.sub(r'[^\w\-_.]', '', nome)
    return nome[:150]

def extrair_json_da_resposta(resposta: str) -> Any:
    """
    Tenta extrair e decodificar um objeto JSON de uma string de resposta do LLM,
    que pode estar envolto em markdown, texto solto ou ser truncado.
    Inclui depura√ß√£o detalhada em caso de falha.
    """
    # ETAPA 0: Valida√ß√£o inicial da resposta
    # Garante que a resposta n√£o √© vazia ou nula antes de qualquer processamento.
    if not isinstance(resposta, str) or not resposta.strip():
        print("‚ùå Erro ao extrair JSON: A resposta recebida da API est√° vazia ou n√£o √© uma string.")
        return None

    json_str = ""
    # ETAPA 1: Tenta encontrar um bloco de c√≥digo JSON expl√≠cito (```json ... ```)
    # Esta √© a forma mais confi√°vel de extrair o conte√∫do.
    match = re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', resposta, re.DOTALL)
    if match:
        json_str = match.group(1).strip()
    else:
        # ETAPA 2 (Fallback): Se n√£o houver bloco de c√≥digo, busca o primeiro '[' ou '{'
        # Isso ajuda a ignorar qualquer texto introdut√≥rio que a API possa ter enviado.
        start_bracket = resposta.find('[')
        start_brace = resposta.find('{')
        
        start = -1
        if start_bracket != -1 and (start_bracket < start_brace or start_brace == -1):
            start = start_bracket
        elif start_brace != -1:
            start = start_brace

        if start != -1:
            # Se encontrou um marcador, assume que o resto da string √© a tentativa de JSON.
            json_str = resposta[start:].strip()
        else:
            # SE NENHUM MARCADOR FOI ENCONTRADO, a resposta n√£o se parece com JSON.
            print("‚ùå Erro ao extrair JSON: Nenhum marcador de in√≠cio ('[' ou '{') foi encontrado.")
            print("üìã RESPOSTA COMPLETA DA API (para depura√ß√£o):")
            print("-" * 50)
            print(resposta)
            print("-" * 50)
            return None

    # ETAPA 3: Tenta decodificar o JSON extra√≠do
    # √â aqui que erros de truncamento (JSON incompleto) ser√£o pegos.
    try:
        return json.loads(json_str)
    except json.JSONDecodeError as e:
        # Ocorreu um erro de decodifica√ß√£o. O JSON est√° malformado ou incompleto.
        print(f"‚ùå Erro ao decodificar JSON (malformado ou incompleto): {e}")
        print("üìã CONTE√öDO QUE FALHOU NA AN√ÅLISE (para depura√ß√£o):")
        print("-" * 50)
        print(json_str)
        print("-" * 50)
        return None

def processar_pdf_com_chunking(caminho_pdf: str, client, generation_config_decision) -> List[Dict[str, Any]]:
    """
    Processa um PDF dividindo-o em chunks menores para evitar truncamento.
    Usa arquivos tempor√°rios √∫nicos para evitar conflitos em processamento paralelo.
    """
    nome_arquivo = os.path.basename(caminho_pdf)
    print(f"üìñ Processando PDF com chunking: `{nome_arquivo}`")
    noticias_consolidadas = []

    try:
        doc = fitz.open(caminho_pdf)
        total_paginas = doc.page_count
        print(f"   üìÑ Total de p√°ginas: {total_paginas}")

        for i_chunk in range(0, total_paginas, PAGINAS_POR_CHUNK):
            start_page = i_chunk
            end_page = min(i_chunk + PAGINAS_POR_CHUNK, total_paginas)
            
            print(f"   ‚ö° Processando chunk: p√°ginas {start_page + 1} a {end_page}")
            
            # Criar um arquivo tempor√°rio √∫nico para este chunk
            with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_file:
                temp_chunk_path = temp_file.name
            
            try:
                # Criar um PDF tempor√°rio para o chunk atual
                chunk_doc = fitz.open()
                chunk_doc.insert_pdf(doc, from_page=start_page, to_page=end_page - 1)
                chunk_doc.save(temp_chunk_path)
                chunk_doc.close()
                
                # Upload e processamento do chunk
                uploaded_file = client.files.upload(file=temp_chunk_path)
                
                # Aguarda processamento
                while uploaded_file.state.name == "PROCESSING":
                    time.sleep(5)
                    uploaded_file = client.files.get(name=uploaded_file.name)
                
                if uploaded_file.state.name != "ACTIVE":
                    print(f"     ‚ùå Erro no processamento do chunk: {uploaded_file.state.name}")
                    client.files.delete(name=uploaded_file.name)
                    continue
                
                response = client.models.generate_content(
                    model='gemini-2.0-flash-lite',
                    contents=[uploaded_file, PROMPT_EXTRACAO_PERMISSIVO_V8],
                    config=generation_config_decision
                )
                client.files.delete(name=uploaded_file.name)
                
                # ======================================================================
                # BLOCO DE CORRE√á√ÉO (Tratamento de Resposta da API)
                # ======================================================================
                # O objetivo deste bloco √© garantir que a resposta da API n√£o foi bloqueada
                # ou retornou vazia antes de tentar processar o JSON.
                response_text = None
                try:
                    response_text = response.text
                except ValueError:
                    # Este erro √© levantado pela biblioteca do Gemini quando o conte√∫do √© bloqueado.
                    print(f"     ‚ö†Ô∏è A resposta da API para o chunk foi bloqueada (provavelmente por filtros de seguran√ßa).")
                    if response.prompt_feedback and response.prompt_feedback.block_reason:
                         print(f"        -> Motivo do Bloqueio: {response.prompt_feedback.block_reason.name}")

                # S√≥ prossegue se response_text for uma string v√°lida
                if response_text:
                    noticias_chunk = extrair_json_da_resposta(response_text)
                    if noticias_chunk and isinstance(noticias_chunk, list):
                        noticias_validadas_chunk = []
                        for j, noticia_data in enumerate(noticias_chunk):
                            try:
                                if noticia_data.get('jornal'):
                                    noticia_data['jornal'] = nome_arquivo
                                
                                # CORRE√á√ÉO DE TAG ANTES DA VALIDA√á√ÉO
                                if 'tag' in noticia_data:
                                    noticia_data['tag'] = corrigir_tag_invalida(noticia_data['tag'])
                                
                                # APLICA MIGRA√á√ÉO PARA GARANTIR CAMPOS NECESS√ÅRIOS
                                noticia_data = migrar_noticia_cache_legado(noticia_data)
                                
                                noticia_obj = Noticia(**noticia_data)
                                noticias_validadas_chunk.append(noticia_obj.model_dump())
                            except ValidationError as e:
                                print(f"     ‚ö†Ô∏è Not√≠cia {j+1} do chunk inv√°lida e DESCARTADA: {[error['loc'][0] for error in e.errors()]}")
                            except Exception as e:
                                print(f"     ‚ö†Ô∏è Erro ao validar not√≠cia {j+1} do chunk: {e}")
                        
                        noticias_consolidadas.extend(noticias_validadas_chunk)
                        print(f"     ‚úÖ {len(noticias_validadas_chunk)} not√≠cias v√°lidas extra√≠das do chunk")
                    else:
                        print(f"     ‚úÖ 0 not√≠cias relevantes encontradas no chunk (conforme esperado).")
                else:
                    print(f"     INFO: A API n√£o retornou conte√∫do para este chunk.")
                # ======================================================================
                # FIM DO BLOCO DE CORRE√á√ÉO
                # ======================================================================
                        
            except Exception as e:
                print(f"     ‚ùå Erro ao processar chunk {start_page + 1}-{end_page}: {e}")
            finally:
                # Limpa arquivo tempor√°rio √∫nico
                try:
                    if os.path.exists(temp_chunk_path):
                        os.remove(temp_chunk_path)
                except Exception as cleanup_error:
                    print(f"     ‚ö†Ô∏è Aviso: N√£o foi poss√≠vel limpar arquivo tempor√°rio: {cleanup_error}")
        
        doc.close()
        print(f"   üéØ Total consolidado: {len(noticias_consolidadas)} not√≠cias de `{nome_arquivo}`")
        return noticias_consolidadas
        
    except Exception as e:
        print(f"‚ùå ERRO CR√çTICO no chunking de `{nome_arquivo}`: {e}")
        return []

def processar_uma_noticia(args: tuple) -> Optional[Dict[str, Any]]:
    """
    Fun√ß√£o 'worker' para processar um √∫nico item de not√≠cia.
    Projetada para ser usada em um processo paralelo.
    Inclui l√≥gica de auto-corre√ß√£o de JSON.
    """
    noticia_original, client, generation_config_decision, nome_arquivo_origem = args
    i, noticia_data = noticia_original
    
    titulo = noticia_data.get("titulo")
    texto_completo = noticia_data.get("texto_completo")
    
    if not all([titulo, texto_completo]):
        print(f"   ‚ö†Ô∏è Not√≠cia {i} pulada: 'titulo' ou 'texto_completo' ausente.")
        return None

    print(f"   -> Processando not√≠cia {i}: '{titulo[:60]}...'")

    try:
        # Prepara o conte√∫do para enviar √† API
        conteudo_para_api = json.dumps({"titulo": titulo, "texto_completo": texto_completo}, ensure_ascii=False)
        
        # 1¬™ Tentativa: Chamada principal √† API
        response = client.models.generate_content(
            model='gemini-2.0-flash-lite',
            contents=[PROMPT_EXTRACAO_JSON_V1, conteudo_para_api],
            config=generation_config_decision
        )
        
        response_text = response.text
        noticias_extraidas = extrair_json_da_resposta(response_text)

    except json.JSONDecodeError as e:
        print(f"   üêõ JSON malformado detectado na not√≠cia {i}. Tentando auto-corre√ß√£o... Erro: {e}")
        try:
            # 2¬™ Tentativa: Chamada para o prompt de corre√ß√£o
            response_corrigida = client.models.generate_content(
                model='gemini-2.0-flash-lite',
                contents=[PROMPT_CORRECAO_JSON.format(json_quebrado=response_text)],
                config=generation_config_decision
            )
            noticias_extraidas = extrair_json_da_resposta(response_corrigida.text)
            print(f"   ‚úÖ Auto-corre√ß√£o bem-sucedida para a not√≠cia {i}.")
        except Exception as e_corr:
            print(f"   ‚ùå Falha na auto-corre√ß√£o para a not√≠cia {i}. Descartando. Erro: {e_corr}")
            return None
    except Exception as e_main:
        print(f"   ‚ùå Erro inesperado ao processar a not√≠cia {i}: {e_main}")
        return None

    # Valida√ß√£o e enriquecimento do resultado
    if noticias_extraidas and isinstance(noticias_extraidas, list) and len(noticias_extraidas) > 0:
        try:
            noticia_processada = noticias_extraidas[0]
            
            # CORRE√á√ÉO: Tratar fonte corretamente para crawlers JSON
            if nome_arquivo_origem.endswith('.json'):
                # Para arquivos JSON, usar a fonte real do dicion√°rio
                fonte_real = noticia_data.get('fonte', 'N/A')
                noticia_processada['jornal'] = fonte_real
                # Preservar o arquivo de origem para refer√™ncia se necess√°rio
                noticia_processada['arquivo_origem'] = nome_arquivo_origem
            else:
                # Para PDFs, usar o nome do arquivo
                noticia_processada['jornal'] = nome_arquivo_origem
            
            noticia_processada['data'] = noticia_data.get('data_publicacao')
            noticia_processada['autor'] = noticia_data.get('autor', 'N/A')
            
            # +++ IN√çCIO DA ALTERA√á√ÉO +++
            # CAPTURAR A URL DO OBJETO ORIGINAL DA NOT√çCIA (DO ARQUIVO JSON)
            # Tenta capturar de 'url' ou 'link', que s√£o os nomes mais comuns.
            url_original = noticia_data.get('url') or noticia_data.get('link')
            noticia_processada['url'] = url_original
            # +++ FIM DA ALTERA√á√ÉO +++
            
            # CORRE√á√ÉO DE TAG ANTES DA VALIDA√á√ÉO
            if 'tag' in noticia_processada:
                noticia_processada['tag'] = corrigir_tag_invalida(noticia_processada['tag'])
            
            # APLICA MIGRA√á√ÉO PARA GARANTIR CAMPOS NECESS√ÅRIOS
            noticia_processada = migrar_noticia_cache_legado(noticia_processada)
            
            # Valida com Pydantic para garantir a estrutura final
            noticia_obj = Noticia(**noticia_processada)
            print(f"   ‚úÖ Not√≠cia {i} validada com sucesso.")
            return noticia_obj.model_dump()
        except ValidationError as e_val:
            print(f"   ‚ö†Ô∏è Not√≠cia {i} DESCARTADA (erro de valida√ß√£o Pydantic): {[error['loc'][0] for error in e_val.errors()]}")
            return None
    
    return None

def processar_arquivo_json(caminho_json: str, client, generation_config_decision) -> List[Dict[str, Any]]:
    """
    Processa um arquivo JSON contendo uma lista de not√≠cias de forma PARALELA e ROBUSTA.
    """
    import concurrent.futures

    nome_arquivo = os.path.basename(caminho_json)
    print(f"üìñ Processando arquivo JSON com performance otimizada: `{nome_arquivo}`")
    
    try:
        with open(caminho_json, 'r', encoding='utf-8') as f:
            dados_json = json.load(f)
    except (json.JSONDecodeError, FileNotFoundError) as e:
        print(f"‚ùå Erro cr√≠tico ao ler o arquivo JSON `{nome_arquivo}`: {e}")
        return []

    if not isinstance(dados_json, list):
        print(f"‚ùå Erro: O arquivo JSON `{nome_arquivo}` n√£o cont√©m uma lista na raiz.")
        return []

    # N√∫mero de chamadas paralelas. Um bom ponto de partida √© entre 10 e 20.
    # N√£o aumente demais para n√£o sobrecarregar a API (rate limits).
    MAX_WORKERS = 15
    noticias_consolidadas = []
    
    # Prepara os argumentos para cada worker
    tarefas = [( (i, noticia), client, generation_config_decision, nome_arquivo) for i, noticia in enumerate(dados_json, 1)]

    print(f" üöÄ Iniciando processamento paralelo de {len(dados_json)} not√≠cias com at√© {MAX_WORKERS} workers...")
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # map executa as tarefas em paralelo e retorna os resultados na ordem
        resultados = executor.map(processar_uma_noticia, tarefas)
        
        # Coleta os resultados que n√£o s√£o nulos
        for resultado in resultados:
            if resultado:
                noticias_consolidadas.append(resultado)

    print(f"\n üéØ Total consolidado do JSON: {len(noticias_consolidadas)} not√≠cias v√°lidas de `{nome_arquivo}`")
    return noticias_consolidadas

def gerar_relatorio_docx(relatorios_finais: List[Dict[str, Any]], pasta_saida: str, stats_funil: Dict[str, int] = None):
    """Gera um documento Word (.docx) interativo com t√≠tulos recolh√≠veis."""
    try:
        timestamp = datetime.now().strftime("%Y-%m-%d_%Hh%Mm")
        nome_arquivo = os.path.join(pasta_saida, f"Relatorio_Consolidado_{timestamp}.docx")
        print(f"\n‚úçÔ∏è  Gerando Relat√≥rio DOCX: `{nome_arquivo}`")

        # NOVO: Ordenar relat√≥rios por prioridade antes de gerar o DOCX
        ordem_prioridade = {'P1_CRITICO': 1, 'P2_ESTRATEGICO': 2, 'P3_MONITORAMENTO': 3}
        relatorios_finais_ordenados = sorted(relatorios_finais, key=lambda x: ordem_prioridade.get(x.get('prioridade', 'P3_MONITORAMENTO'), 3))

        doc = Document()
        doc.add_heading('Relat√≥rio Consolidado de Not√≠cias', level=0).alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_data = doc.add_paragraph()
        p_data.add_run(f'Gerado em: {datetime.now().strftime("%d/%m/%Y √†s %H:%M")}').italic = True
        p_data.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # Adicionar linha do funil de processamento
        if stats_funil:
            p_funil = doc.add_paragraph()
            p_funil.add_run('Funil de Processamento: ').bold = True
            p_funil.add_run(f"{stats_funil.get('noticias_extraidas', 0)} not√≠cias extra√≠das ‚Üí "
                           f"{stats_funil.get('grupos_faticos', 0)} eventos consolidados ‚Üí "
                           f"{stats_funil.get('resumos_finais', 0)} resumos finais")
            p_funil.alignment = WD_ALIGN_PARAGRAPH.CENTER

        doc.add_paragraph()  # Espa√ßamento

        # NOVO: Adiciona par√°grafo instrutivo
        p_instrucao = doc.add_paragraph()
        p_instrucao.add_run('Guia de Leitura: ').bold = True
        p_instrucao.add_run('Clique na seta ao lado de cada t√≠tulo para expandir ou recolher o resumo. '
                          'Use o "Painel de Navega√ß√£o" (Exibir ‚Üí Painel de Navega√ß√£o) para navegar pelo documento.').italic = True
        
        doc.add_page_break()

        doc.add_paragraph()

        # Agrupamento dos resumos por TAG (usando relat√≥rios ordenados)
        TAG_ORDER = [
            'Governo e Politica',
            'Economia e Tecnologia',
            'Judicionario',
            'Empresas Privadas'
        ]
        
        grupos_por_tag: Dict[str, List[Dict[str, Any]]] = {tag: [] for tag in TAG_ORDER}
        for rel in relatorios_finais_ordenados:
            tag_val = rel.get('tag')
            if tag_val in grupos_por_tag:
                grupos_por_tag[tag_val].append(rel)

        doc.add_heading('√çndice de Not√≠cias', level=1)
        for tag in TAG_ORDER:
            itens_tag = grupos_por_tag.get(tag, [])
            if not itens_tag:
                continue
            doc.add_paragraph(f'{tag}:', style='Heading 3')
            for relatorio in itens_tag:
                doc.add_paragraph(relatorio["titulo_final"], style='List Number')

        doc.add_page_break()

        doc.add_heading('An√°lises Consolidadas', level=1)
        for tag in TAG_ORDER:
            itens_tag = grupos_por_tag.get(tag, [])
            if not itens_tag:
                continue
            doc.add_heading(tag, level=2)
            for i, relatorio in enumerate(itens_tag, 1):
                # ALTERA√á√ÉO PRINCIPAL: Troca 'level=3' por 'style='Heading 3''
                titulo_p = doc.add_paragraph(style='Heading 3')
                titulo_p.add_run(f'{i}. {relatorio["titulo_final"]}')
                
                # O resumo continua como par√°grafo normal
                doc.add_paragraph(relatorio['resumo_final'], style='Body Text')

                p_fontes = doc.add_paragraph()
                p_fontes.add_run('Fontes: ').bold = True
                
                fontes_formatadas_unicas = set() # Usar um set para garantir fontes √∫nicas
                
                for fonte in relatorio.get("fontes", []):
                    jornal = fonte.get('jornal', 'N/A').replace('.pdf', '')
                    pagina_str = ""
                    autor_str = ""
                    
                    # Adicionar p√°gina apenas se for v√°lida
                    pagina = fonte.get('pagina')
                    if pagina and str(pagina).lower() != 'n/a':
                        pagina_str = f"p√°g. {pagina}"
                        
                    # Adicionar autor apenas se for v√°lido
                    autor = fonte.get('autor')
                    if autor and str(autor).lower() != 'n/a':
                        autor_str = f"por {autor}"
                        
                    # Montar a string da fonte de forma limpa
                    partes_fonte = [item for item in [pagina_str, autor_str] if item]
                    if partes_fonte:
                        fonte_str = f"{jornal} ({', '.join(partes_fonte)})"
                    else:
                        fonte_str = jornal
                        
                    fontes_formatadas_unicas.add(fonte_str)
                    
                p_fontes.add_run(" | ".join(sorted(list(fontes_formatadas_unicas)))).font.size = Pt(9)
                p_fontes.runs[-1].italic = True
                doc.add_paragraph()

        doc.save(nome_arquivo)
        print(f"‚úÖ Relat√≥rio Final Gerado com sucesso em `{nome_arquivo}`")

    except Exception as e:
        print(f"‚ùå Erro Cr√≠tico ao gerar o arquivo DOCX: {e}")


# ==============================================================================
# 7. FUN√á√ÉO AUXILIAR PARA PROCESSAMENTO PARALELO DE ARQUIVOS
# ==============================================================================

def processar_um_arquivo(args):
    """
    Fun√ß√£o worker para processar um √∫nico arquivo (PDF ou JSON).
    Projetada para ser chamada em paralelo.
    """
    caminho_arquivo, client, generation_config_decision = args
    nome_arquivo = os.path.basename(caminho_arquivo)
    print(f"\n--- \nüì∞ Processando arquivo: `{nome_arquivo}`")
    
    # L√≥gica para JSON (com cache implementado)
    if caminho_arquivo.endswith('.json'):
        nome_cache = f"{limpar_nome_arquivo(nome_arquivo)}_processado.json"
        caminho_cache = os.path.join(CACHE_DIRECTORY, nome_cache)

        if os.path.exists(caminho_cache):
            try:
                print(f"‚ôªÔ∏è  Tentando carregar do cache: `{caminho_cache}`")
                with open(caminho_cache, 'r', encoding='utf-8') as f:
                    noticias_cache = json.load(f)
                
                # Usar fun√ß√£o de migra√ß√£o para validar e migrar dados antigos
                noticias_validadas_cache = validar_e_migrar_cache(noticias_cache, nome_arquivo)
                
                # Se a valida√ß√£o passou sem erros e o cache n√£o est√° vazio:
                if noticias_validadas_cache:
                    print(f"‚úÖ Cache HIT: {len(noticias_validadas_cache)} not√≠cias carregadas com sucesso.")
                    
                    # Se houve migra√ß√£o, salva o cache atualizado
                    if len(noticias_cache) != len(noticias_validadas_cache) or any('relevance_score' not in n for n in noticias_cache):
                        with open(caminho_cache, 'w', encoding='utf-8') as f:
                            json.dump(noticias_validadas_cache, f, ensure_ascii=False, indent=2)
                        print(f"üíæ Cache migrado e atualizado para `{nome_arquivo}`.")
                    
                    return noticias_validadas_cache
                else:
                    # O cache estava vazio ou todos os dados eram inv√°lidos.
                    print("‚ö†Ô∏è Aviso: Cache vazio ou inv√°lido. Reprocessando.")

            except Exception as e:
                print(f"‚ö†Ô∏è Aviso: Falha cr√≠tica ao ler cache `{nome_arquivo}`. Reprocessando. Erro: {e}")

        # Se o cache falhou ou n√£o existe, processa o JSON
        noticias_validadas_json = processar_arquivo_json(caminho_arquivo, client, generation_config_decision)
        
        # Salva no cache e retorna
        if noticias_validadas_json:
            with open(caminho_cache, 'w', encoding='utf-8') as f:
                json.dump(noticias_validadas_json, f, ensure_ascii=False, indent=2)
            print(f"üíæ Cache SAVE: Resultados de `{nome_arquivo}` salvos.")
            return noticias_validadas_json
        else:
            print(f"‚úÖ Nenhuma not√≠cia relevante encontrada em `{nome_arquivo}`.")
            return []

    # L√≥gica para PDF (incluindo o novo bloco de cache corrigido)
    elif caminho_arquivo.endswith('.pdf'):
        nome_cache = f"{limpar_nome_arquivo(nome_arquivo)}.json"
        caminho_cache = os.path.join(CACHE_DIRECTORY, nome_cache)

        if os.path.exists(caminho_cache):
            try:
                print(f"‚ôªÔ∏è  Tentando carregar do cache: `{caminho_cache}`")
                with open(caminho_cache, 'r', encoding='utf-8') as f:
                    noticias_cache = json.load(f)
                
                # Usar fun√ß√£o de migra√ß√£o para validar e migrar dados antigos
                noticias_validadas_cache = validar_e_migrar_cache(noticias_cache, nome_arquivo)
                
                # Se a valida√ß√£o passou sem erros e o cache n√£o est√° vazio:
                if noticias_validadas_cache:
                    print(f"‚úÖ Cache HIT: {len(noticias_validadas_cache)} not√≠cias carregadas com sucesso.")
                    
                    # Se houve migra√ß√£o, salva o cache atualizado
                    if len(noticias_cache) != len(noticias_validadas_cache) or any('relevance_score' not in n for n in noticias_cache):
                        with open(caminho_cache, 'w', encoding='utf-8') as f:
                            json.dump(noticias_validadas_cache, f, ensure_ascii=False, indent=2)
                        print(f"üíæ Cache migrado e atualizado para `{nome_arquivo}`.")
                    
                    return noticias_validadas_cache
                else:
                    # O cache estava vazio ou todos os dados eram inv√°lidos.
                    print("‚ö†Ô∏è Aviso: Cache vazio ou inv√°lido. Reprocessando.")

            except Exception as e:
                print(f"‚ö†Ô∏è Aviso: Falha cr√≠tica ao ler cache `{nome_arquivo}`. Reprocessando. Erro: {e}")

        # Se o cache falhou ou n√£o existe, processa o PDF
        num_paginas = contar_paginas_pdf(caminho_arquivo)
        usar_chunking = num_paginas > LIMITE_PAGINAS_CHUNKING
        
        noticias_validadas_pdf = []
        if usar_chunking:
            print(f"üìä PDF grande detectado ({num_paginas} p√°ginas) - usando chunking.")
            noticias_validadas_pdf = processar_pdf_com_chunking(caminho_arquivo, client, generation_config_decision)
        else:
            print(f"üìÑ PDF normal ({num_paginas} p√°ginas) - processamento direto.")
            try:
                uploaded_file = client.files.upload(file=caminho_arquivo)
                while uploaded_file.state.name == "PROCESSING":
                    time.sleep(5)
                    uploaded_file = client.files.get(name=uploaded_file.name)
                
                if uploaded_file.state.name == "ACTIVE":
                    response = client.models.generate_content(
                        model='gemini-2.0-flash-lite',
                        contents=[uploaded_file, PROMPT_EXTRACAO_PERMISSIVO_V8],
                        config=generation_config_decision
                    )
                    response_text = response.text
                    noticias_brutas = extrair_json_da_resposta(response_text)
                    
                    if noticias_brutas and isinstance(noticias_brutas, list):
                        for noticia_data in noticias_brutas:
                            try:
                                noticia_data['jornal'] = nome_arquivo
                                
                                # CORRE√á√ÉO DE TAG ANTES DA VALIDA√á√ÉO
                                if 'tag' in noticia_data:
                                    noticia_data['tag'] = corrigir_tag_invalida(noticia_data['tag'])
                                
                                noticias_validadas_pdf.append(Noticia(**noticia_data).model_dump())
                            except ValidationError as e:
                                print(f"   ‚ö†Ô∏è Not√≠cia do PDF DESCARTADA: {[error['loc'][0] for error in e.errors()]}")
                    client.files.delete(name=uploaded_file.name)
                else:
                     print(f"‚ùå Erro no upload do PDF: {uploaded_file.state.name}")
            except Exception as e:
                print(f"‚ùå ERRO CR√çTICO ao processar PDF `{nome_arquivo}`: {e}")

        # Salva no cache e retorna
        if noticias_validadas_pdf:
            with open(caminho_cache, 'w', encoding='utf-8') as f:
                json.dump(noticias_validadas_pdf, f, ensure_ascii=False, indent=2)
            print(f"üíæ Cache SAVE: Resultados de `{nome_arquivo}` salvos.")
            return noticias_validadas_pdf
        else:
            print(f"‚úÖ Nenhuma not√≠cia relevante encontrada em `{nome_arquivo}`.")
            return []
    return []

def gerar_resumo_para_grupo(args):
    """
    Fun√ß√£o worker para gerar o resumo final de um √∫nico evento com profundidade vari√°vel.
    Inclui retry autom√°tico e melhor tratamento de erros.
    """
    grupo, i, total, client, generation_config_text = args
    tema = grupo.get('tema_principal', f'Grupo {i}')
    prioridade = grupo.get('prioridade')
    print(f"  -> Resumindo evento {i}/{total} (Prioridade: {prioridade}): {tema}")
    
    # NOVO: Sistema de retry com m√∫ltiplas tentativas
    MAX_TENTATIVAS = 3
    
    for tentativa in range(1, MAX_TENTATIVAS + 1):
        try:
            # NOVO: Determinar o n√≠vel de detalhe com base na prioridade
            mapa_detalhe = {
                'P1_CRITICO': 'Executivo (P1_CRITICO)',
                'P2_ESTRATEGICO': 'Padr√£o (P2_ESTRATEGICO)',
                'P3_MONITORAMENTO': 'Conciso (P3_MONITORAMENTO)'
            }
            nivel_de_detalhe = mapa_detalhe.get(prioridade, 'Padr√£o (P2_ESTRATEGICO)')

            # Passar o 'tema_principal' no JSON para o prompt
            dados_para_resumir = {
                "tema_principal": tema,
                "noticias": grupo.get("noticias", [])
            }
            dados_json_str = json.dumps(dados_para_resumir, ensure_ascii=False, indent=2)

            # --- CORRE√á√ÉO PRINCIPAL AQUI ---
            # Unificar instru√ß√£o e dados em um √∫nico prompt.
            prompt_completo = PROMPT_RESUMO_FINAL_V3.format(
                NIVEL_DE_DETALHE=nivel_de_detalhe,
                DADOS_DO_GRUPO=dados_json_str  # Injeta o JSON como string
            )
            # --- FIM DA CORRE√á√ÉO ---
            
            # NOVO: Configura√ß√£o mais conservadora para resumos
            config_resumo = types.GenerateContentConfig(
                temperature=0.3,  # Ainda mais baixa para consist√™ncia
                top_p=0.9,
                top_k=20,
                max_output_tokens=8192,  # Limite menor para evitar truncamento
            )
            
            response = client.models.generate_content(
                model='gemini-2.5-flash',
                # Envia apenas UMA string com tudo dentro
                contents=[prompt_completo],
                config=config_resumo  # Usa config espec√≠fica
            )
            
            # NOVO: Verifica√ß√£o de resposta v√°lida antes de extrair JSON
            if not hasattr(response, 'text') or not response.text:
                if tentativa < MAX_TENTATIVAS:
                    print(f"     ‚ö†Ô∏è Tentativa {tentativa}: Resposta vazia da API. Tentando novamente...")
                    time.sleep(1)
                    continue
                else:
                    print(f"     ‚ùå Falha na tentativa {tentativa}: API retornou resposta vazia.")
                    return None
            
            # NOVO: Tratamento robusto de JSON com debug
            resumo_bruto = extrair_json_da_resposta(response.text)
            
            if not resumo_bruto:
                if tentativa < MAX_TENTATIVAS:
                    print(f"     ‚ö†Ô∏è Tentativa {tentativa}: JSON inv√°lido. Tentando novamente...")
                    time.sleep(1)
                    continue
                else:
                    print(f"     ‚ùå Falha na tentativa {tentativa}: N√£o foi poss√≠vel extrair JSON v√°lido.")
                    print(f"     üìã RESPOSTA DA API (debug): {response.text[:200]}...")
                    return None

            # Valida√ß√£o da estrutura do resumo
            if not isinstance(resumo_bruto, dict) or 'titulo_final' not in resumo_bruto or 'resumo_final' not in resumo_bruto:
                if tentativa < MAX_TENTATIVAS:
                    print(f"     ‚ö†Ô∏è Tentativa {tentativa}: Estrutura JSON incompleta. Tentando novamente...")
                    time.sleep(1)
                    continue
                else:
                    print(f"     ‚ùå Falha na tentativa {tentativa}: JSON sem campos obrigat√≥rios.")
                    return None

            # Processamento das tags e fontes (j√° validado)
            tag_counts = {}
            for n in grupo.get('noticias', []):
                tag_val = n.get('tag')
                if tag_val:
                    tag_counts[tag_val] = tag_counts.get(tag_val, 0) + 1
            
            if tag_counts:
                tag_predominante = max(tag_counts, key=tag_counts.get)
                
                fontes_validadas = []
                for noticia in grupo.get('noticias', []):
                    try:
                        fonte = FonteResumo(
                            jornal=noticia.get("jornal"),
                            pagina=noticia.get("pagina"),
                            autor=noticia.get("autor")
                        )
                        fontes_validadas.append(fonte.model_dump())
                    except ValidationError:
                        continue
                
                resumo_bruto["fontes"] = fontes_validadas
                resumo_bruto["tag"] = tag_predominante
                resumo_bruto["prioridade"] = grupo.get("prioridade")
                
                # Valida√ß√£o final com Pydantic
                resumo_obj = ResumoFinal(**resumo_bruto)
                print(f"     ‚úÖ Resumo para '{tema}' gerado e validado com sucesso (tentativa {tentativa}).")
                return resumo_obj.model_dump()
            else:
                print(f"     ‚ùå Falha: Nenhuma tag v√°lida encontrada no grupo '{tema}'.")
                return None
                
        except json.JSONDecodeError as json_err:
            if tentativa < MAX_TENTATIVAS:
                print(f"     ‚ö†Ô∏è Tentativa {tentativa}: Erro de JSON. Tentando novamente... ({json_err})")
                time.sleep(1)
                continue
            else:
                print(f"     ‚ùå Falha final na tentativa {tentativa}: Erro JSON persistente para '{tema}': {json_err}")
                return None
                
        except ValidationError as val_err:
            if tentativa < MAX_TENTATIVAS:
                print(f"     ‚ö†Ô∏è Tentativa {tentativa}: Erro de valida√ß√£o. Tentando novamente...")
                time.sleep(1)
                continue
            else:
                print(f"     ‚ùå Falha final na tentativa {tentativa}: Erro de valida√ß√£o para '{tema}': {val_err}")
                return None
                
        except Exception as e:
            if tentativa < MAX_TENTATIVAS:
                print(f"     ‚ö†Ô∏è Tentativa {tentativa}: Erro geral. Tentando novamente... ({e})")
                time.sleep(1)
                continue
            else:
                print(f"     ‚ùå Falha final na tentativa {tentativa}: Erro geral para '{tema}': {e}")
                return None
    
    print(f"     ‚ùå Todas as {MAX_TENTATIVAS} tentativas falharam para o grupo '{tema}'.")
    return None

# ==============================================================================
# 8. FUN√á√ÉO PRINCIPAL DO PIPELINE (v7.0)
# ==============================================================================

def main(client, generation_config_decision, generation_config_text):
    """
    Executa o pipeline v7.0 com foco em um briefing executivo de duas camadas.
    1. Extrai not√≠cias com score de relev√¢ncia.
    2. Agrupa por eventos.
    3. RANKING: Separa os eventos, ranqueia os P1 e seleciona o TOP 12.
    4. SUMARIZA√á√ÉO EM DUAS CAMADAS:
        - TOP 12 Cr√≠ticos: Resumos de 1 par√°grafo.
        - Radar de Monitoramento: Resumos de 1 linha (bullet points).
    5. MONTAGEM DO RELAT√ìRIO: Gera o DOCX no novo formato.
    """
    print("\n" + "="*80)
    print("üöÄ INICIANDO PIPELINE DE BRIEFING EXECUTIVO (v7.0) üöÄ")
    print("="*80)

    todas_noticias_extraidas = []
    grupos_de_eventos = []

    # --------------------------------------------------------------------------
    # ETAPA 1: EXTRA√á√ÉO DE NOT√çCIAS EM PARALELO
    # --------------------------------------------------------------------------
    print("\n" + "-"*22 + " ETAPA 1: Extra√ß√£o em Paralelo " + "-"*23)
    caminhos_pdfs = glob.glob(os.path.join(PDF_DIRECTORY, '*.pdf'))
    caminhos_jsons = glob.glob(os.path.join(PDF_DIRECTORY, '*.json'))
    todos_os_arquivos = caminhos_pdfs + caminhos_jsons

    if not todos_os_arquivos:
        print(f"‚ùå ERRO: Nenhum arquivo PDF ou JSON encontrado na pasta `{PDF_DIRECTORY}`. Encerrando.")
        return

    MAX_WORKERS_EXTRACAO = 10
    print(f"üöÄ Iniciando extra√ß√£o paralela de {len(todos_os_arquivos)} arquivos com at√© {MAX_WORKERS_EXTRACAO} workers...")
    print(f"üìÅ Distribui√ß√£o: {len(caminhos_pdfs)} PDFs e {len(caminhos_jsons)} JSONs")

    tarefas_extr = [(path, client, generation_config_decision) for path in todos_os_arquivos]

    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS_EXTRACAO) as executor:
        resultados_por_arquivo = executor.map(processar_um_arquivo, tarefas_extr)
        
        for lista_de_noticias in resultados_por_arquivo:
            if lista_de_noticias:
                todas_noticias_extraidas.extend(lista_de_noticias)

    print(f"\nüéØ EXTRA√á√ÉO PARALELA CONCLU√çDA: {len(todas_noticias_extraidas)} not√≠cias totais de {len(todos_os_arquivos)} arquivos.")

    # --------------------------------------------------------------------------
    # ETAPA 2: AGRUPAMENTO CONSOLIDADO DE EVENTOS
    # --------------------------------------------------------------------------
    print("\n" + "-"*21 + " ETAPA 2: Agrupamento Consolidado " + "-"*22)
    if todas_noticias_extraidas:
        noticias_para_agrupar_com_id = [
            {
                "id": i,
                "titulo": n.get("titulo", ""),
                "jornal": n.get("jornal", ""),
                "trecho": (n.get("texto_completo", "")[:300] + "...") if len(n.get("texto_completo", "")) > 300 else n.get("texto_completo", "")
            }
            for i, n in enumerate(todas_noticias_extraidas)
        ]
        mapa_id_para_noticia = {i: n for i, n in enumerate(todas_noticias_extraidas)}

        print(f"üîÑ Consolidando {len(noticias_para_agrupar_com_id)} not√≠cias em eventos √∫nicos (em lotes)...")

        # Novo: Agrupamento em lotes com retry e fallback
        tamanho_lote = 120
        max_tentativas = 3
        grupos_por_lote: list[dict] = []

        for inicio in range(0, len(noticias_para_agrupar_com_id), tamanho_lote):
            fim = min(inicio + tamanho_lote, len(noticias_para_agrupar_com_id))
            lote = noticias_para_agrupar_com_id[inicio:fim]
            print(f"   üì¶ Lote {inicio//tamanho_lote + 1}: itens {inicio}‚Äì{fim-1} ({len(lote)} not√≠cias)")

            grupos_brutos = None
            for tentativa in range(1, max_tentativas + 1):
                try:
                    prompt_completo = PROMPT_AGRUPAMENTO_CONSOLIDADO_V2 + "\n\nNOT√çCIAS PARA AGRUPAR:\n" + json.dumps(lote, indent=2, ensure_ascii=False)
                    response = client.models.generate_content(
                        model='gemini-2.5-flash',
                        contents=[prompt_completo],
                        config=generation_config_text
                    )

                    if not hasattr(response, 'text') or not isinstance(response.text, str) or not response.text.strip():
                        print(f"     ‚ö†Ô∏è Tentativa {tentativa}: Resposta vazia do LLM para o lote. Retry...")
                        time.sleep(1)
                        continue

                    grupos_brutos = extrair_json_da_resposta(response.text)
                    if not grupos_brutos or not isinstance(grupos_brutos, list):
                        print(f"     ‚ö†Ô∏è Tentativa {tentativa}: JSON inv√°lido no lote. Retry...")
                        time.sleep(1)
                        grupos_brutos = None
                        continue

                    # Sucesso
                    break
                except Exception as e:
                    print(f"     ‚ö†Ô∏è Tentativa {tentativa}: Erro ao agrupar lote: {e}")
                    time.sleep(1)

            if not grupos_brutos:
                # Fallback: cada item do lote vira seu pr√≥prio grupo
                print("     üîÅ Fallback ativado: criando grupos unit√°rios para o lote.")
                for item in lote:
                    grupos_por_lote.append({
                        "tema_principal": item.get("titulo") or "Evento",
                        "ids_originais": [item.get("id")]
                    })
            else:
                grupos_por_lote.extend(grupos_brutos)

        # Consolida√ß√£o dos grupos de todos os lotes em 'grupos_de_eventos'
        # Mescla por 'tema_principal' id√™ntico para reduzir duplicatas simples
        grupos_por_tema: dict[str, set] = {}
        for grupo in grupos_por_lote:
            tema = (grupo.get("tema_principal") or "Evento").strip()
            ids = grupo.get("ids_originais", []) or []
            if tema not in grupos_por_tema:
                grupos_por_tema[tema] = set()
            grupos_por_tema[tema].update(ids)

        for tema, ids_set in grupos_por_tema.items():
            noticias_consolidadas_grupo = []
            for id_original in ids_set:
                noticia_original = mapa_id_para_noticia.get(id_original)
                if noticia_original:
                    noticias_consolidadas_grupo.append(noticia_original)
            if noticias_consolidadas_grupo:
                grupos_de_eventos.append({
                    "tema_principal": tema,
                    "noticias": noticias_consolidadas_grupo
                })

        print(f"‚úÖ {len(grupos_de_eventos)} eventos √∫nicos criados ap√≥s consolida√ß√£o em lotes.")
    else:
        print("‚ö†Ô∏è Aviso: Nenhuma not√≠cia extra√≠da, pulando agrupamentos.")

    # --------------------------------------------------------------------------
    # ETAPA 3: SEPARA√á√ÉO E RANKING DOS GRUPOS
    # --------------------------------------------------------------------------
    print("\n" + "-"*23 + " ETAPA 3: Ranking e Sele√ß√£o de Grupos " + "-"*24)
    
    grupos_p1_candidatos = []
    grupos_monitoramento = []

    if not grupos_de_eventos:
        print("‚ùå Nenhum grupo de eventos foi criado. Encerrando.")
        return

    for grupo in grupos_de_eventos:
        prioridades = [n.get('prioridade') for n in grupo['noticias']]
        # Calcula o score m√©dio do grupo
        scores = [n.get('relevance_score', 0) for n in grupo['noticias']]
        grupo['score_medio'] = sum(scores) / len(scores) if scores else 0
        
        # Atribui a maior prioridade ao grupo
        if 'P1_CRITICO' in prioridades:
            grupo['prioridade'] = 'P1_CRITICO'
            grupos_p1_candidatos.append(grupo)
        else:
            grupo['prioridade'] = 'P2_ESTRATEGICO' if 'P2_ESTRATEGICO' in prioridades else 'P3_MONITORAMENTO'
            grupos_monitoramento.append(grupo)

    # Ranqueia os grupos P1 pelo score e seleciona o TOP 12
    grupos_p1_candidatos.sort(key=lambda g: g['score_medio'], reverse=True)
    top_12_criticos = grupos_p1_candidatos[:12]
    
    # O resto dos P1 vai para o monitoramento tamb√©m
    grupos_monitoramento.extend(grupos_p1_candidatos[12:])
    
    print(f"üéØ Sele√ß√£o Cr√≠tica: {len(top_12_criticos)} eventos P1 selecionados para an√°lise detalhada.")
    print(f"üì° Radar de Monitoramento: {len(grupos_monitoramento)} eventos para resumos de 1 linha.")
    
    # --------------------------------------------------------------------------
    # ETAPA 4: GERA√á√ÉO DE RESUMOS (DUAS CAMADAS)
    # --------------------------------------------------------------------------
    print("\n" + "-"*21 + " ETAPA 4: Gera√ß√£o de Resumos (2 Camadas) " + "-"*21)
    
    # Camada 1: Resumos detalhados para o TOP 12
    relatorios_criticos = []
    if top_12_criticos:
        print(f"üìù Gerando resumos detalhados para {len(top_12_criticos)} eventos cr√≠ticos...")
        
        # Gerar resumos cr√≠ticos em paralelo
        MAX_WORKERS_RESUMO = 15
        tarefas_criticos = [(grupo, client, generation_config_text) for grupo in top_12_criticos]
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS_RESUMO) as executor:
            resultados_criticos = executor.map(gerar_resumo_critico, tarefas_criticos)
            relatorios_criticos = [res for res in resultados_criticos if res is not None]

    # Camada 2: Bullet points para o Radar
    relatorios_radar = {}  # Dicion√°rio para agrupar por se√ß√£o
    if grupos_monitoramento:
        print(f"‚ö° Gerando bullet points para {len(grupos_monitoramento)} eventos do radar...")
        
        # Gerar bullet points em paralelo
        tarefas_radar = [(grupo, client, generation_config_text) for grupo in grupos_monitoramento]
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS_RESUMO) as executor:
            resultados_radar = executor.map(gerar_resumo_radar, tarefas_radar)
            
            # Agrupar por se√ß√µes
            relatorios_radar = {
                'Empresas Privadas': [],
                'Economia e Politica': [],
                'Legislativo e Judiciario': [],
                'Internacional': [],
                'Tecnologia': []
            }
            
            for resultado in resultados_radar:
                if resultado and 'bullet_point' in resultado and 'tag' in resultado:
                    tag = resultado['tag']
                    bullet = resultado['bullet_point']
                    
                    # Mapear tags para se√ß√µes
                    if tag == 'Empresas Privadas':
                        relatorios_radar['Empresas Privadas'].append(bullet)
                    elif tag == 'Governo e Politica':
                        relatorios_radar['Economia e Politica'].append(bullet)
                    elif tag == 'Judicionario':
                        relatorios_radar['Legislativo e Judiciario'].append(bullet)
                    elif tag == 'Economia e Tecnologia':
                        # Dividir entre tecnologia e internacional baseado no conte√∫do
                        if any(tech_word in bullet.lower() for tech_word in ['ia', 'intelig√™ncia artificial', 'tecnologia', 'software', 'app', 'digital', 'crypto', 'bitcoin']):
                            relatorios_radar['Tecnologia'].append(bullet)
                        else:
                            relatorios_radar['Internacional'].append(bullet)

    # --------------------------------------------------------------------------
    # ETAPA 5: GERA√á√ÉO DO RELAT√ìRIO FINAL EM DOCX
    # --------------------------------------------------------------------------
    print("\n" + "-"*25 + " ETAPA 5: Gera√ß√£o do Relat√≥rio " + "-"*26)
    
    # Coletar nomes dos jornais analisados (agora o campo jornal j√° tem a fonte correta)
    nomes_jornais = set()
    for n in todas_noticias_extraidas:
        jornal_name = n.get('jornal', '')
        # Remove extens√µes de arquivo se existirem
        jornal_limpo = jornal_name.replace('.pdf', '').replace('.json', '')
        if jornal_limpo and jornal_limpo != 'N/A':
            nomes_jornais.add(jornal_limpo)

    stats_funil = {
        'jornais_analisados': ", ".join(sorted(list(nomes_jornais))),
        'noticias_analisadas': len(todas_noticias_extraidas),
        'eventos_agrupados': len(grupos_de_eventos),
        'resumos_criticos': len(relatorios_criticos)
    }

    gerar_relatorio_docx_v2(
        relatorios_criticos,
        relatorios_radar,
        OUTPUT_DIRECTORY,
        stats_funil
    )
    
    print("\n‚úÖ PIPELINE DE BRIEFING EXECUTIVO v7.0 CONCLU√çDO.\n")

# ==============================================================================
# 9. FUN√á√ïES AUXILIARES PARA O NOVO PIPELINE v7.0
# ==============================================================================

def gerar_resumo_critico(tarefa):
    """
    Gera um resumo cr√≠tico de 1 par√°grafo para eventos do TOP 12.
    
    Args:
        tarefa: Tupla (grupo, client, generation_config_text)
    
    Returns:
        Dict com resumo cr√≠tico ou None em caso de erro
    """
    grupo, client, generation_config_text = tarefa
    
    try:
        # Preparar dados do grupo
        dados_grupo = {
            "tema_principal": grupo.get("tema_principal", ""),
            "noticias": grupo.get("noticias", [])
        }
        
        prompt_completo = PROMPT_RESUMO_CRITICO_V1.format(
            DADOS_DO_GRUPO=json.dumps(dados_grupo, indent=2, ensure_ascii=False)
        )
        
        response = client.models.generate_content(
            model='gemini-2.5-flash',
            contents=[prompt_completo],
            config=generation_config_text
        )
        
        resultado = extrair_json_da_resposta(response.text)
        
        if resultado and 'resumo_final' in resultado:
            # Preparar fontes
            fontes = []
            for noticia in grupo.get("noticias", []):
                # --- ALTERA√á√ÉO AQUI ---
                # Adicione a URL ao dicion√°rio da fonte
                fontes.append({
                    'jornal': noticia.get('jornal', 'N/A'),
                    'pagina': noticia.get('pagina', 'N/A'),
                    'autor': noticia.get('autor', 'N/A'),
                    'url': noticia.get('url') # Adiciona a URL capturada
                })
            
            return {
                'titulo_final': grupo.get("tema_principal", ""),
                'resumo_final': resultado['resumo_final'],
                'fontes': fontes,
                'tag': grupo.get("noticias", [{}])[0].get('tag', 'Empresas Privadas'),
                'prioridade': 'P1_CRITICO'
            }
        
    except Exception as e:
        print(f"‚ùå Erro ao gerar resumo cr√≠tico para '{grupo.get('tema_principal', '')}': {e}")
    
    return None

def gerar_resumo_radar(tarefa):
    """
    Gera um bullet point de 1 linha para eventos do radar de monitoramento.
    
    Args:
        tarefa: Tupla (grupo, client, generation_config_text)
    
    Returns:
        Dict com bullet point e tag ou None em caso de erro
    """
    grupo, client, generation_config_text = tarefa
    
    try:
        # Preparar dados do grupo
        dados_grupo = {
            "tema_principal": grupo.get("tema_principal", ""),
            "noticias": grupo.get("noticias", [])
        }
        
        prompt_completo = PROMPT_RADAR_MONITORAMENTO_V1.format(
            DADOS_DO_GRUPO=json.dumps(dados_grupo, indent=2, ensure_ascii=False)
        )
        
        response = client.models.generate_content(
            model='gemini-2.5-flash',
            contents=[prompt_completo],
            config=generation_config_text
        )
        
        resultado = extrair_json_da_resposta(response.text)
        
        if resultado and 'bullet_point' in resultado:
            return {
                'bullet_point': resultado['bullet_point'],
                'tag': grupo.get("noticias", [{}])[0].get('tag', 'Empresas Privadas')
            }
        
    except Exception as e:
        print(f"‚ùå Erro ao gerar bullet point para '{grupo.get('tema_principal', '')}': {e}")
    
    return None

def gerar_relatorio_docx_v2(
    relatorios_criticos: List[Dict[str, Any]],
    relatorios_radar: Dict[str, List[str]],
    pasta_saida: str,
    stats_funil: Dict[str, any]
):
    """
    Gera o Briefing Executivo no formato de duas camadas, com formata√ß√£o detalhada das fontes.
    1. An√°lises Cr√≠ticas (TOP 12) com resumos e fontes detalhadas.
    2. Radar de Monitoramento com bullet points.
    """
    try:
        timestamp = datetime.now().strftime("%Y-%m-%d_%Hh%Mm")
        nome_arquivo = os.path.join(pasta_saida, f"Briefing_Executivo_{timestamp}.docx")
        print(f"\n‚úçÔ∏è Montando Briefing Executivo com fontes detalhadas: `{nome_arquivo}`")
        doc = Document()
        
        # --- CABE√áALHO (sem altera√ß√µes) ---
        doc.add_heading('Relat√≥rio Consolidado de Not√≠cias', level=0).alignment = WD_ALIGN_PARAGRAPH.CENTER
        p_data = doc.add_paragraph()
        p_data.add_run(f'Gerado em: {datetime.now().strftime("%d/%m/%Y √†s %H:%M")}').italic = True
        p_data.alignment = WD_ALIGN_PARAGRAPH.CENTER
        doc.add_paragraph()
        
        p_funil = doc.add_paragraph()
        p_funil.add_run('Jornais analisados: ').bold = True
        p_funil.add_run(f"{stats_funil.get('jornais_analisados', 'N/A')}\n")
        p_funil.add_run('Not√≠cias Analisadas: ').bold = True
        p_funil.add_run(f"{stats_funil.get('noticias_analisadas', 0)} ‚Üí ")
        p_funil.add_run('Agrupadas em: ').bold = True
        p_funil.add_run(f"{stats_funil.get('eventos_agrupados', 0)} ‚Üí ")
        p_funil.add_run('Resumidas: ').bold = True
        p_funil.add_run(str(stats_funil.get('resumos_criticos', 0)))
        p_funil.alignment = WD_ALIGN_PARAGRAPH.CENTER

        # --- CAMADA 1: AN√ÅLISES CR√çTICAS (TOP 12) ---
        if relatorios_criticos:
            doc.add_heading('Principais Noticias', level=1)
            for i, relatorio in enumerate(relatorios_criticos, 1):
                doc.add_heading(f"{i}. {relatorio['titulo_final']}", level=2)
                p_resumo = doc.add_paragraph(relatorio.get('resumo_final', 'N/A'))
                p_resumo.paragraph_format.space_after = Pt(12)
                
                # ======================================================================
                # IN√çCIO DA CORRE√á√ÉO DEFINITIVA NO TRATAMENTO DAS FONTES
                # ======================================================================
                p_fontes = doc.add_paragraph()
                p_fontes.add_run('Fontes: ').bold = True
                
                # N√£o vamos mais usar um 'set' para n√£o perder detalhes.
                # Processamos cada fonte individualmente da lista.
                lista_de_fontes = relatorio.get('fontes', [])
                for idx, fonte in enumerate(lista_de_fontes):
                    # Limpa o nome do jornal para exibi√ß√£o
                    jornal_limpo = str(fonte.get('jornal', 'N/A')).replace('.pdf','').replace('.json','')
                    url = fonte.get('url')
                    
                    # L√≥gica para fontes de SITES/JSON (que possuem URL)
                    if url:
                        # Cria um hiperlink com o nome do jornal como texto clic√°vel
                        add_hyperlink(p_fontes, jornal_limpo, url)
                    
                    # L√≥gica para fontes de PDFS (que N√ÉO possuem URL)
                    else:
                        pagina = fonte.get('pagina')
                        autor = fonte.get('autor')
                        
                        # Constr√≥i a string de detalhes (p√°gina, autor) de forma din√¢mica
                        partes_detalhe = []
                        if pagina and str(pagina).strip().lower() not in ['n/a', '']:
                            partes_detalhe.append(f"p√°g. {str(pagina).strip()}")
                        if autor and str(autor).strip().lower() not in ['n/a', '']:
                            partes_detalhe.append(f"por {str(autor).strip()}")
                        
                        # Formata a string final
                        if partes_detalhe:
                            detalhes_str = ", ".join(partes_detalhe)
                            texto_fonte = f"{jornal_limpo.upper()} ({detalhes_str})"
                        else:
                            texto_fonte = jornal_limpo.upper()
                        
                        p_fontes.add_run(texto_fonte).italic = True
                    
                    # Adiciona um separador entre as fontes, mas n√£o ap√≥s a √∫ltima
                    if idx < len(lista_de_fontes) - 1:
                        p_fontes.add_run(' | ').italic = True
                
                doc.add_paragraph() # Adiciona um espa√ßo ap√≥s a linha de fontes
                # ======================================================================
                # FIM DA CORRE√á√ÉO DEFINITIVA
                # ======================================================================
        
        # --- CAMADA 2: RADAR DE MONITORAMENTO (sem altera√ß√µes) ---
        if relatorios_radar:
            doc.add_heading('Radar de Monitoramento', level=1)
            SECTIONS_ORDER = ['Empresas Privadas', 'Economia e Politica', 'Legislativo e Judiciario', 'Internacional', 'Tecnologia']
            for section in SECTIONS_ORDER:
                if section in relatorios_radar and relatorios_radar[section]:
                    doc.add_heading(section, level=2)
                    for bullet_point in relatorios_radar[section][:12]:
                        if bullet_point.startswith('‚Ä¢'):
                            bullet_point = bullet_point[1:].strip()
                        if ':' in bullet_point:
                            partes = bullet_point.split(':', 1)
                            p = doc.add_paragraph(style='List Bullet')
                            p.add_run(partes[0] + ':').bold = True
                            p.add_run(partes[1])
                        else:
                            p = doc.add_paragraph(bullet_point, style='List Bullet')
                        p.paragraph_format.space_after = Pt(6)
                    doc.add_paragraph()

        doc.save(nome_arquivo)
        print(f"‚úÖ Briefing Executivo Gerado com sucesso em `{nome_arquivo}`")

    except Exception as e:
        print(f"‚ùå Erro Cr√≠tico ao gerar o arquivo DOCX: {e}")

# ==============================================================================
# 10. PONTO DE ENTRADA DO SCRIPT
# ==============================================================================

if __name__ == "__main__":
    # Verifica√ß√£o de depend√™ncias
    if not verificar_dependencias():
        exit(1)
    
    for diretorio in [PDF_DIRECTORY, OUTPUT_DIRECTORY, CACHE_DIRECTORY]:
        os.makedirs(diretorio, exist_ok=True)

    # Configura√ß√£o da API Key
    api_key = "AIzaSyAB7hZ1C9t_Tb-q-sXLzLvhqbJUmB3noUE"
    print("üîë Usando chave de API definida diretamente no c√≥digo.")
    
    try:
        # Criar cliente com a nova API
        client = genai.Client(api_key=api_key)
        
        # Configura√ß√£o de IA otimizada para precis√£o em decis√µes
        generation_config_decision = types.GenerateContentConfig(
            temperature=0.1,  # Baixa temperatura para decis√µes precisas
            top_p=0.95,
            top_k=40,
            max_output_tokens=65536,
        )

        # Configura√ß√£o para tarefas de processamento de texto (agrupar, resumir)
        generation_config_text = types.GenerateContentConfig(
            temperature=0.5,  # Temperatura moderada para criatividade controlada
            top_p=0.95,
            top_k=40,
            max_output_tokens=65536,
        )

        print("‚ö°Ô∏è Inicializando cliente de IA com configura√ß√µes espec√≠ficas...")

        model_name1 = 'gemini-2.5-flash-lite'
        model_name2 = 'gemini-2.5-flash'


        print(f"   - Modelo para Decis√µes (Extrair/Filtrar): {model_name1} (temp={generation_config_decision.temperature})")
        print(f"   - Modelo para Texto (Agrupar/Resumir): {model_name2} (temp={generation_config_text.temperature})")

        # Passa o cliente e as configura√ß√µes para a fun√ß√£o main
        main(client=client, generation_config_decision=generation_config_decision, generation_config_text=generation_config_text)

    except Exception as e:
        print(f"\n‚ùå Ocorreu um erro fatal durante a inicializa√ß√£o ou execu√ß√£o: {e}")
        print("   Verifique sua chave de API, conex√£o com a internet e permiss√µes de pasta.")
        exit(1) 