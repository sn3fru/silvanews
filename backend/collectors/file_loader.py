"""
Coletor de arquivos para o BTG AlphaFeed.
Carrega not√≠cias a partir de arquivos PDFs (com extra√ß√£o via IA) e JSONs.
"""

import os
import json
import hashlib
import time
import tempfile
import re
from typing import Dict, Any, List, Optional
from pathlib import Path
import requests
import concurrent.futures

try:
    import fitz  # PyMuPDF
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False

# Importa√ß√µes do projeto com suporte a execu√ß√£o como pacote ou script
try:
    from ..database import SessionLocal
    from ..models import ArtigoBrutoCreate
    from ..crud import create_artigo_bruto, get_artigo_by_hash, create_log
    from ..prompts import PROMPT_EXTRACAO_PDF_RAW_V1
    from ..utils import get_datetime_brasil_str, get_date_brasil_str, gerar_titulo_fallback_curto, titulo_e_generico, inferir_tipo_fonte_por_jornal
except ImportError:
    # Fallback para execu√ß√£o direta
    try:
        from database import SessionLocal
        from models import ArtigoBrutoCreate
        from crud import create_artigo_bruto, get_artigo_by_hash, create_log
        from prompts import PROMPT_EXTRACAO_PDF_RAW_V1
        from utils import get_datetime_brasil_str, get_date_brasil_str, gerar_titulo_fallback_curto, titulo_e_generico, inferir_tipo_fonte_por_jornal
    except ImportError as e:
        print(f"‚ùå ERRO: N√£o foi poss√≠vel importar m√≥dulos do backend: {e}")
        print(f"   Verifique se est√° executando a partir do diret√≥rio correto")
        raise

# Constantes para o processamento de PDF, para garantir robustez.
PAGINAS_POR_CHUNK = 5
LIMITE_PAGINAS_CHUNKING = 10  # Um limite mais baixo √© mais seguro para PDFs densos

class FileLoader:
    """
    Coletor refatorado para processar arquivos de forma robusta.
    - Utiliza a File API do Gemini para OCR e extra√ß√£o de not√≠cias de PDFs.
    - Implementa 'chunking' para PDFs grandes, evitando timeouts da API.
    - Processa JSONs de forma eficiente.
    - Interage com a API do backend ou diretamente com o banco de dados.
    """

    def __init__(self, api_base_url: str = "http://localhost:8000",
                 files_directory: str = "../pdfs", client: Any = None):
        self.api_base_url = api_base_url
        self.files_directory = Path(files_directory)
        self.session = requests.Session()
        
        # Inje√ß√£o de depend√™ncia: o cliente Gemini √© recebido aqui.
        # Isso centraliza a configura√ß√£o e torna a classe mais test√°vel.
        self.client = client
        self.extraction_prompt = PROMPT_EXTRACAO_PDF_RAW_V1
        # Config padr√£o para tarefas de decis√£o (alinhado ao poc_silva)
        self.generation_config_decision = {
            "temperature": 0.1,
            "top_p": 0.95,
            "top_k": 40,
            "max_output_tokens": 65536,
        }

        if not self.files_directory.exists():
            raise FileNotFoundError(f"ERRO: Diret√≥rio de origem n√£o encontrado: {self.files_directory}")
        if PDF_AVAILABLE and not self.client:
            print("‚ö†Ô∏è AVISO: Cliente Gemini n√£o foi fornecido. O processamento de PDFs usar√° extra√ß√£o de texto simples, sem IA.")

    def gerar_hash_artigo(self, texto: str, url: str = "") -> str:
        """Gera hash √∫nico para o artigo."""
        conteudo = f"{texto}{url if url else ''}"
        return hashlib.sha256(conteudo.encode('utf-8')).hexdigest()
    
    def detectar_tipo_fonte(self, jornal: str) -> str:
        """
        Detecta se um jornal √© nacional ou internacional baseado no nome.
        """
        jornais_nacionais = {
            'folha', 'folha de s. paulo', 'folha de s√£o paulo', 'folha de s.paulo',
            'estadao', 'estad√£o', 'estado de s. paulo', 'estado de s√£o paulo', 'o estado de s. paulo',
            'globo', 'o globo', 'g1', 'valor', 'valor economico', 'valor econ√¥mico',
            'uol', 'r7', 'veja', 'exame', 'isto√©', 'istoe', '√©poca', 'epoca',
            'correio braziliense', 'correio', 'zero hora', 'gazeta do povo',
            'metr√≥poles', 'metropoles', 'poder360', 'infomoney', 'investing.com brasil',
            'brasil 247', 'carta capital', 'cartacapital', 'ag√™ncia brasil', 'agencia brasil',
            'bbc brasil', 'cnn brasil', 'di√°rio do com√©rcio', 'diario do comercio',
            'dci', 'monitor mercantil', 'brazil journal', 'jota', 'conjur',
            'consultor jur√≠dico', 'migalhas', 'capital reset', 'neo feed', 'neofeed',
            'brazil journal', 'the brazilian report'
        }
        
        jornais_internacionais = {
            'new york times', 'nyt', 'wall street journal', 'wsj', 'financial times', 'ft',
            'bloomberg', 'reuters', 'associated press', 'ap', 'washington post',
            'the guardian', 'bbc', 'cnn', 'cnbc', 'the economist', 'forbes',
            'business insider', 'marketwatch', 'barrons', 'barron\'s', 'the telegraph',
            'the times', 'daily mail', 'usa today', 'los angeles times', 'chicago tribune',
            'boston globe', 'miami herald', 'axios', 'politico', 'the hill',
            'foreign policy', 'foreign affairs', 'the atlantic', 'vox', 'vice',
            'buzzfeed', 'huffpost', 'huffington post', 'daily beast', 'slate',
            'salon', 'mother jones', 'the intercept', 'propublica', 'npr',
            'pbs', 'abc news', 'nbc news', 'cbs news', 'fox news', 'msnbc',
            'sky news', 'al jazeera', 'russia today', 'rt', 'sputnik',
            'china daily', 'xinhua', 'nikkei', 'japan times', 'korea herald',
            'south china morning post', 'scmp', 'the hindu', 'times of india',
            'dawn', 'the nation', 'jerusalem post', 'haaretz', 'al arabiya',
            'gulf news', 'arab news', 'the national', 'daily star', 'the star'
        }
        
        if not jornal:
            return 'nacional'  # Default para nacional
        
        jornal_lower = jornal.lower().strip()
        
        # Verifica se √© nacional
        for nome in jornais_nacionais:
            if nome in jornal_lower:
                return 'nacional'
        
        # Verifica se √© internacional
        for nome in jornais_internacionais:
            if nome in jornal_lower:
                return 'internacional'
        
        # Se tem .br no nome, provavelmente √© nacional
        if '.br' in jornal_lower or 'brasil' in jornal_lower or 'brazil' in jornal_lower:
            return 'nacional'
        
        # Default para nacional para manter compatibilidade
        return 'nacional'

    # --- L√ìGICA DE PROCESSAMENTO DE PDF (REFATORADA) ---

    def _extrair_json_da_resposta(self, resposta: str, contexto: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        Extrai, higieniza e decodifica JSON retornado por LLMs de forma robusta.
        - Remove blocos markdown
        - Tenta parse direto
        - Fallback para extra√ß√£o campo-a-campo via regex
        """
        if not isinstance(resposta, str) or not resposta.strip():
            print("  ‚ùå Resposta da API vazia.")
            return []

        # Remove blocos markdown primeiro (mais agressivo)
        json_str = resposta
        # Remove m√∫ltiplas varia√ß√µes de blocos markdown
        json_str = re.sub(r'```(?:json|JSON|Json)?\s*\n?', '', json_str)
        json_str = re.sub(r'```\s*$', '', json_str)
        
        # Se ainda tem marcadores, tenta extrair conte√∫do entre eles
        match = re.search(r'```(?:json|JSON|Json)?\s*([\s\S]*?)\s*```', resposta, re.DOTALL)
        if match:
            json_str = match.group(1).strip()
        
        # Tenta parse direto primeiro (para respostas bem formadas)
        try:
            dados = json.loads(json_str)
            if isinstance(dados, list):
                return dados
            if isinstance(dados, dict):
                return [dados]
        except:
            pass  # Continua com sanitiza√ß√£o

        # Se falhou, tenta extra√ß√£o campo-a-campo via regex
        artigos_extraidos = []
        
        # Pattern para encontrar not√≠cias individuais
        # Busca por objetos que tenham pelo menos titulo e texto_completo
        pattern = re.compile(
            r'\{[^{}]*?"titulo"\s*:\s*"((?:[^"]|(?<=\\)")*?)"[^{}]*?"texto_completo"\s*:\s*"((?:[^"]|(?<=\\)")*?)"[^{}]*?\}',
            re.DOTALL
        )
        
        for match in pattern.finditer(json_str):
            titulo_raw = match.group(1)
            texto_raw = match.group(2)
            
            # Limpa os campos extra√≠dos
            titulo = titulo_raw.replace('\\"', '"').replace('\\n', ' ').replace('\n', ' ').strip()
            texto = texto_raw.replace('\\"', '"').replace('\\n', ' ').replace('\n', ' ').strip()
            
            if titulo and texto:
                # Tenta extrair outros campos do objeto
                obj_str = match.group(0)
                
                # Extrai campos opcionais - mais tolerante com valores num√©ricos
                def extract_field(field_name: str, obj: str) -> Optional[str]:
                    # Tenta string entre aspas
                    pat = re.compile(rf'"{field_name}"\s*:\s*"((?:[^"]|(?<=\\)")*?)"')
                    m = pat.search(obj)
                    if m:
                        return m.group(1).replace('\\"', '"').replace('\\n', ' ').replace('\n', ' ').strip()
                    
                    # Tenta valor num√©rico (para pagina)
                    pat_num = re.compile(rf'"{field_name}"\s*:\s*(\d+)')
                    m_num = pat_num.search(obj)
                    if m_num:
                        return m_num.group(1)
                    
                    # Tenta null/None
                    pat_null = re.compile(rf'"{field_name}"\s*:\s*(?:null|None)')
                    if pat_null.search(obj):
                        return None
                    return None
                
                artigos_extraidos.append({
                    'titulo': titulo,
                    'texto_completo': texto,
                    'jornal': extract_field('jornal', obj_str),
                    'autor': extract_field('autor', obj_str),
                    'pagina': extract_field('pagina', obj_str),
                    'data': extract_field('data', obj_str),
                    'categoria': extract_field('categoria', obj_str),
                    'tag': extract_field('tag', obj_str),
                    'prioridade': extract_field('prioridade', obj_str),
                    'relevance_score': extract_field('relevance_score', obj_str),
                    'relevance_reason': extract_field('relevance_reason', obj_str)
                })
        
        if artigos_extraidos:
            return artigos_extraidos
        
        # Se ainda n√£o conseguiu, tenta sanitiza√ß√£o mais agressiva
        print("  üîß Tentando sanitiza√ß√£o avan√ßada do JSON...")
        
        # Pr√©-corre√ß√µes comuns antes do parse
        try:
            # Fun√ß√£o auxiliar para sanitizar campos string
            def _sanitize_string_fields(s: str, fields: List[str]) -> str:
                out_parts: List[str] = []
                pos = 0
                pattern = re.compile(r'("(?:' + '|'.join(re.escape(f) for f in fields) + r')"\s*:\s*")')
                while True:
                    m = pattern.search(s, pos)
                    if not m:
                        out_parts.append(s[pos:])
                        break
                    # adiciona trecho antes do campo
                    out_parts.append(s[pos:m.end()])
                    i = m.end()  # in√≠cio do conte√∫do dentro das aspas
                    buf_chars: List[str] = []
                    while i < len(s):
                        ch = s[i]
                        if ch == '\\':
                            # mant√©m escapes j√° existentes, mas normaliza \n, \r, \t para espa√ßo
                            if i + 1 < len(s) and s[i+1] in ('n', 'r', 't'):
                                buf_chars.append(' ')
                                i += 2
                                continue
                            # trata sequ√™ncia barra + quebra real (\\\n ou \\\r)
                            if i + 1 < len(s) and s[i+1] in ('\n', '\r'):
                                buf_chars.append(' ')
                                i += 2
                                continue
                            # mant√©m escape para outros casos
                            if i + 1 < len(s):
                                buf_chars.append('\\')
                                buf_chars.append(s[i+1])
                                i += 2
                                continue
                            buf_chars.append('\\')
                            i += 1
                            continue
                        if ch == '"':
                            # olha adiante para decidir se √© fechamento (seguido de , ou })
                            j = i + 1
                            while j < len(s) and s[j] in (' ', '\n', '\r', '\t'):
                                j += 1
                            if j < len(s) and s[j] in (',', '}'):
                                # fecha string
                                out_parts.append(''.join(buf_chars))
                                out_parts.append('"')
                                pos = i + 1
                                break
                            # aspa interna n√£o escapada ‚Üí escapa
                            buf_chars.append('\\"')
                            i += 1
                            continue
                        if ch in ('\n', '\r'):
                            buf_chars.append(' ')
                            i += 1
                            continue
                        buf_chars.append(ch)
                        i += 1
                    else:
                        # EOF sem fechamento; adiciona buffer e encerra
                        out_parts.append(''.join(buf_chars))
                        pos = i
                        break
                # Colapsa espa√ßos excessivos dentro dos campos j√° sanitizados
                result = ''.join(out_parts)
                return result

            json_str = _sanitize_string_fields(json_str, ['titulo', 'texto_completo'])

            # Sanitiza campos problem√°ticos com aspas internas n√£o escapadas (ex.: texto_completo, titulo)
            def _escape_inner_quotes_for_field(s: str, field: str) -> str:
                # casa: "field": "...CONTEUDO...",  at√© a pr√≥xima chave conhecida
                proxima_chave = r'("texto_completo"|"jornal"|"autor"|"pagina"|"data"|"categoria"|"tag"|"prioridade"|"relevance_score"|"relevance_reason"|\})'
                pattern = rf'("{field}"\s*:\s*")(.*?)(")\s*,\s*{proxima_chave}'
                def repl(m: re.Match) -> str:
                    inicio = m.group(1)
                    conteudo = m.group(2)
                    fim = m.group(3)
                    resto = m.group(4)
                    # Normaliza quebras de linha para \n e escapa aspas n√£o escapadas
                    conteudo = conteudo.replace('\r\n', '\n').replace('\r', '\n')
                    conteudo = conteudo.replace('\n', '\\n')
                    conteudo = re.sub(r'(?<!\\)"', r'\\"', conteudo)
                    return f"{inicio}{conteudo}{fim}, {resto}"
                return re.sub(pattern, repl, s, flags=re.DOTALL)

            # J√° sanitizado por _sanitize_string_fields; evita reintroduzir \n ou quebrar aspas com regex

            # Remove sequ√™ncias de escape fora de strings (ex.: \\n entre campos)
            def _remove_escapes_outside_strings(s: str) -> str:
                result_chars: List[str] = []
                in_string = False
                escape_next = False
                i = 0
                while i < len(s):
                    ch = s[i]
                    if in_string:
                        if escape_next:
                            result_chars.append(ch)
                            escape_next = False
                        else:
                            if ch == '\\':
                                result_chars.append(ch)
                                escape_next = True
                            elif ch == '"':
                                result_chars.append(ch)
                                in_string = False
                            else:
                                result_chars.append(ch)
                        i += 1
                        continue
                    # Fora de string
                    if ch == '"':
                        result_chars.append(ch)
                        in_string = True
                        i += 1
                        continue
                    # Remove sequ√™ncias de barras invertidas seguidas de n/r/t (qualquer quantidade de barras)
                    if ch == '\\':
                        j = i
                        # conta barras consecutivas
                        while j < len(s) and s[j] == '\\':
                            j += 1
                        if j < len(s) and s[j] in ('n', 'r', 't'):
                            # descarta todas as barras e o caractere de controle
                            i = j + 1
                            continue
                    # Normaliza: ap√≥s v√≠rgula, remova espa√ßamentos e escapes fora de string at√© o pr√≥ximo token
                    if ch == ',':
                        result_chars.append(ch)
                        i += 1
                        # pular espa√ßos, quebras e escapes estilo \n fora de string
                        while i < len(s):
                            if s[i] in (' ', '\n', '\r', '\t'):
                                i += 1
                                continue
                            if s[i] == '\\':
                                k = i
                                while k < len(s) and s[k] == '\\':
                                    k += 1
                                if k < len(s) and s[k] in ('n', 'r', 't'):
                                    i = k + 1
                                    continue
                            break
                        continue
                    # mant√©m demais caracteres
                    result_chars.append(ch)
                    i += 1
                return ''.join(result_chars)

            json_str = _remove_escapes_outside_strings(json_str)

            # Remove coment√°rios estilo // e /* */
            json_str = re.sub(r"//.*?$", "", json_str, flags=re.MULTILINE)
            json_str = re.sub(r"/\*[\s\S]*?\*/", "", json_str)

            # Normaliza aspas tipogr√°ficas para ASCII
            json_str = (json_str
                        .replace("\u201c", '"').replace("\u201d", '"')
                        .replace("\u2018", "'").replace("\u2019", "'")
                        .replace("‚Äú", '"').replace("‚Äù", '"')
                        .replace("‚Äò", "'").replace("‚Äô", "'"))

            # Corrige chaves com aspas simples: {'key': ...} -> {"key": ...}
            json_str = re.sub(r"\'(\w+)\':", r'"\1":', json_str)

            # Aspas em chaves n√£o citadas simples: {key: ...} -> {"key": ...}
            json_str = re.sub(r'([\{,]\s*)([A-Za-z_][A-Za-z0-9_]*)\s*:', r'\1"\2":', json_str)

            # Converte valores entre aspas simples para aspas duplas em contextos comuns (: 'valor', ['a','b'])
            json_str = re.sub(r'(:\s*)\'([^\'\\]*(?:\\.[^\'\\]*)*)\'', r'\1"\2"', json_str)
            json_str = re.sub(r'(\[\s*)\'([^\'\\]*(?:\\.[^\'\\]*)*)\'', r'\1"\2"', json_str)
            json_str = re.sub(r"\'([^\'\\]*(?:\\.[^\'\\]*)*)\'(\s*[\],])", r'"\1"\2', json_str)

            # Converte booleanos/None estilo Python para JSON
            json_str = re.sub(r"\bTrue\b", "true", json_str)
            json_str = re.sub(r"\bFalse\b", "false", json_str)
            json_str = re.sub(r"\bNone\b", "null", json_str)

            # Remove quebras de linha internas em strings e normaliza espa√ßos
            def _compactar_strings(m: re.Match) -> str:
                txt = m.group(0)
                # preserva aspas externas
                if len(txt) >= 2 and txt[0] == '"' and txt[-1] == '"':
                    inner = txt[1:-1]
                    # normaliza quebras reais e sequ√™ncias escapadas para espa√ßo
                    inner = inner.replace('\r', ' ').replace('\n', ' ')
                    inner = inner.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')
                    inner = re.sub(r'\s+', ' ', inner)
                    return '"' + inner + '"'
                return txt
            json_str = re.sub(r'"[^"\\]*(?:\\.[^"\\])*"', _compactar_strings, json_str)

            # Escapa barras invertidas que n√£o fazem parte de sequ√™ncia JSON v√°lida
            json_str = re.sub(r'\\(?!["\\/bfnrtu])', r'\\\\', json_str)

            # Remove v√≠rgulas √† direita antes de } ou ]
            json_str = re.sub(r",\s*(\}|\])", r"\1", json_str)
        except Exception as e:
            print(f"  ‚ö†Ô∏è Erro durante corre√ß√£o de string antes do parse: {e}")

        # Helper: imprime contexto do erro com posi√ß√£o (apenas uma vez por resposta)
        printed_error_context = {"done": False}
        def _debug_print_json_error(tag: str, err: json.JSONDecodeError, s: str) -> None:
            try:
                ctx_info = ""
                if contexto:
                    ctx_info = f" [arquivo={contexto.get('arquivo')}, pagina={contexto.get('pagina')}, temp={contexto.get('temp_pdf')}]"
                print(f"  üß© Detalhe do erro{ctx_info}: {tag} @char {err.pos} (linha {err.lineno}, col {err.colno})")
                if not printed_error_context["done"]:
                    start = max(0, err.pos - 120)
                    end = min(len(s), err.pos + 120)
                    snippet = s[start:end]
                    pointer = ' ' * (err.pos - start) + '^'
                    print("  --- trecho pr√≥ximo ao erro ---")
                    print(snippet[:500])
                    print(pointer)
                    print("  --- fim do trecho ---")
                    printed_error_context["done"] = True
            except Exception:
                pass

        # Tentativa com JSON sanitizado
        try:
            dados = json.loads(json_str)
            if isinstance(dados, list):
                return dados
            if isinstance(dados, dict):
                return [dados]
            return []
        except json.JSONDecodeError as e:
            print(f"  ‚ùå Erro ao decodificar JSON ap√≥s sanitiza√ß√£o: {e}")
            _debug_print_json_error("sanitizado", e, json_str)

        # Se ainda falhou, √∫ltima tentativa com extra√ß√£o mais tolerante
        print("  üîç Tentando extra√ß√£o alternativa de campos...")
        
        # Busca por padr√µes mais flex√≠veis
        # Aceita aspas n√£o escapadas no meio do texto
        alt_pattern = re.compile(
            r'"titulo"\s*:\s*"([^"]*(?:"[^:,}]*)*?)"[^{}]*?"texto_completo"\s*:\s*"([^"]*(?:"[^:,}]*)*?)"',
            re.DOTALL
        )
        
        for match in alt_pattern.finditer(resposta):
            titulo = match.group(1).replace('\n', ' ').strip()
            texto = match.group(2).replace('\n', ' ').strip()
            
            if titulo and texto:
                artigos_extraidos.append({
                    'titulo': titulo,
                    'texto_completo': texto,
                    'jornal': contexto.get('arquivo', '').replace('.pdf', '') if contexto else None,
                    'pagina': contexto.get('pagina') if contexto else None
                })

        if not artigos_extraidos:
            print("  ‚ö†Ô∏è Nenhum JSON v√°lido foi extra√≠do desta resposta.")
        return artigos_extraidos

    def _processar_chunk_pdf_com_ia(self, pdf_path: Path, nome_arquivo_original: str, numero_pagina: int | None = None) -> List[Dict[str, Any]]:
        """
        M√©todo central que envia um arquivo PDF (completo ou um chunk)
        para a API do Gemini e processa a resposta.
        """
        artigos_formatados = []
        try:
            print(f"  üß† Enviando '{pdf_path.name}' para extra√ß√£o via Gemini File API...")
            # Compatibilidade com diferentes clientes (google.genai vs google.generativeai wrappers)
            uploaded_file = None
            try:
                # API mais recente (google.genai)
                uploaded_file = self.client.files.upload(file=str(pdf_path))
            except TypeError:
                # Fallback: algumas vers√µes aceitam 'path='
                uploaded_file = self.client.files.upload(path=str(pdf_path))

            # Aguarda processamento do arquivo na File API
            while getattr(uploaded_file, "state", None) and getattr(uploaded_file.state, "name", None) == "PROCESSING":
                time.sleep(0.2)
                uploaded_file = self.client.files.get(name=uploaded_file.name)

            if uploaded_file.state.name != "ACTIVE":
                raise Exception(f"Falha no processamento do arquivo na API: {uploaded_file.state.name}")
            
            # Gera√ß√£o de conte√∫do: tenta a interface nova .models.generate_content, com fallback seguro
            response = None
            if hasattr(self.client, 'models') and hasattr(self.client.models, 'generate_content'):
                response = self.client.models.generate_content(
                    model='gemini-2.0-flash',
                    contents=[uploaded_file, self.extraction_prompt],
                    config=self.generation_config_decision
                )
            elif hasattr(self.client, 'generate_content'):
                response = self.client.generate_content(
                    model='models/gemini-2.0-flash',
                    contents=[uploaded_file, self.extraction_prompt],
                    generation_config=self.generation_config_decision
                )
            else:
                raise AttributeError("Cliente Gemini n√£o possui m√©todo generate_content compat√≠vel")
            self.client.files.delete(name=uploaded_file.name)  # Limpeza

            # Tratamento de resposta
            def _get_response_text(resp: Any) -> Optional[str]:
                # 1) Atributo direto
                try:
                    t = getattr(resp, 'text', None)
                    if isinstance(t, str) and t.strip():
                        return t
                except Exception:
                    pass
                # 2) Novo SDK: candidates -> content.parts[].text
                try:
                    candidates = getattr(resp, 'candidates', None)
                    if candidates:
                        parts_text: List[str] = []
                        for cand in candidates:
                            content = getattr(cand, 'content', None) or {}
                            parts = getattr(content, 'parts', None) or []
                            for p in parts:
                                text_val = getattr(p, 'text', None)
                                if isinstance(text_val, str):
                                    parts_text.append(text_val)
                        if parts_text:
                            return "\n".join(parts_text)
                except Exception:
                    pass
                # 3) Algumas libs exp√µem output_text
                try:
                    ot = getattr(resp, 'output_text', None)
                    if isinstance(ot, str) and ot.strip():
                        return ot
                except Exception:
                    pass
                # 4) Fallback via dict
                try:
                    to_dict = getattr(resp, 'to_dict', None)
                    d = to_dict() if callable(to_dict) else None
                    if d and isinstance(d, dict):
                        cands = d.get('candidates') or []
                        parts_text: List[str] = []
                        for cand in cands:
                            content = (cand or {}).get('content') or {}
                            parts = content.get('parts') or []
                            for p in parts:
                                tv = (p or {}).get('text')
                                if isinstance(tv, str):
                                    parts_text.append(tv)
                        if parts_text:
                            return "\n".join(parts_text)
                except Exception:
                    pass
                return None

            response_text = _get_response_text(response)
            if not response_text:
                print("  ‚ö†Ô∏è API n√£o retornou conte√∫do utiliz√°vel para este trecho/p√°gina.")
                return artigos_formatados

            noticias_extraidas = self._extrair_json_da_resposta(
                response_text,
                {"arquivo": nome_arquivo_original, "pagina": numero_pagina, "temp_pdf": pdf_path.name}
            )
            print(f"  ‚ú® {len(noticias_extraidas)} not√≠cias candidatas extra√≠das.")
            if not noticias_extraidas:
                # Amostra limitada da resposta para debugging (apenas em caso de falha)
                preview = response_text[:400].replace('\n', ' ') if isinstance(response_text, str) else ''
                print(f"  üõ†Ô∏è Amostra da resposta (truncada): {preview}")

            # Converte a sa√≠da do LLM para o formato esperado pelo banco de dados
            for noticia in noticias_extraidas:
                if isinstance(noticia, dict) and noticia.get('texto_completo'):
                    # Determina jornal (prioriza extra√≠do pela IA; fallback: nome do arquivo)
                    jornal_extraido = noticia.get('jornal') or nome_arquivo_original.replace('.pdf', '')
                    # Determina p√°gina (prioriza extra√≠do; fallback: n√∫mero da p√°gina processada)
                    pagina_extraida = noticia.get('pagina') if noticia.get('pagina') not in [None, '', 'N/A'] else numero_pagina
                    # Determina URL quando dispon√≠vel
                    url_detectada = noticia.get('url') or noticia.get('link')
                    # Gera t√≠tulo robusto quando ausente/gen√©rico
                    titulo_extraido = (noticia.get('titulo') or '').strip()
                    if titulo_e_generico(titulo_extraido):
                        titulo_extraido = gerar_titulo_fallback_curto(noticia.get('texto_completo'))
                    artigos_formatados.append({
                        'texto_bruto': noticia['texto_completo'],
                        'url_original': url_detectada,
                        'metadados': {
                            'titulo': titulo_extraido or gerar_titulo_fallback_curto(noticia.get('texto_completo')),
                            'subtitulo': '',
                            # Fonte original deve refletir o jornal para alinhar com o fluxo dos JSONs
                            'fonte_original': jornal_extraido,
                            'arquivo_origem': nome_arquivo_original,
                            'data_processamento': get_datetime_brasil_str(),
                            'tipo_arquivo': 'pdf',
                            # Campos extra√≠dos pela IA
                            'jornal': jornal_extraido,
                            'autor': noticia.get('autor') or 'N/A',
                            'pagina': pagina_extraida,
                            'data_publicacao': noticia.get('data') or None,
                            'data_ultima_modificacao': None,
                            'categoria': noticia.get('categoria') or None,
                            'tags_originais': [],
                            'id_hash_original': '',
                            # Alinhamento de compatibilidade com JSONs
                            'link': url_detectada,
                            # Mant√©m campos de IA apenas como metadados informativos
                            'tag_ia': noticia.get('tag'),
                            'prioridade_ia': noticia.get('prioridade'),
                            'relevance_score_ia': noticia.get('relevance_score'),
                            'relevance_reason_ia': noticia.get('relevance_reason')
                        }
                    })

            # Fallback: se nada v√°lido foi extra√≠do, usa texto simples da p√°gina
            if not artigos_formatados and PDF_AVAILABLE:
                try:
                    with fitz.open(pdf_path) as temp_doc:
                        texto_pagina = ''
                        if temp_doc.page_count > 0:
                            texto_pagina = (temp_doc.load_page(0).get_text() or '').strip()
                    if texto_pagina:
                        primeira_linha = texto_pagina.split('\n', 1)[0].strip()
                        if titulo_e_generico(primeira_linha):
                            primeira_linha = gerar_titulo_fallback_curto(texto_pagina)
                        jornal_fallback = nome_arquivo_original.replace('.pdf', '')
                        artigos_formatados.append({
                            'texto_bruto': texto_pagina,
                            'url_original': None,
                            'metadados': {
                                'titulo': (primeira_linha or gerar_titulo_fallback_curto(texto_pagina)) or f"{jornal_fallback} - P√°gina {numero_pagina or ''}",
                                'subtitulo': '',
                                'fonte_original': jornal_fallback,
                                'arquivo_origem': nome_arquivo_original,
                                'data_processamento': get_datetime_brasil_str(),
                                'tipo_arquivo': 'pdf',
                                'jornal': jornal_fallback,
                                'pagina': numero_pagina,
                                'data_publicacao': None,
                                'data_ultima_modificacao': None,
                                'categoria': None,
                                'tags_originais': [],
                                'id_hash_original': '',
                                'link': None
                            }
                        })
                        print("  üîÅ Fallback: extra√ß√£o simples de texto aplicada para esta p√°gina.")
                except Exception as fe:
                    print(f"  ‚ö†Ô∏è Fallback de texto falhou: {fe}")
        except Exception as e:
            print(f"  ‚ùå Erro durante a chamada √† API Gemini para '{pdf_path.name}': {e}")
        
        return artigos_formatados

    def processar_pdf(self, file_path: Path) -> List[Dict[str, Any]]:
        """
        Orquestra a extra√ß√£o de not√≠cias de um arquivo PDF.
        - Se o cliente Gemini n√£o estiver dispon√≠vel, faz uma extra√ß√£o de texto simples.
        - Se estiver dispon√≠vel, usa a File API, com chunking para arquivos grandes.
        """
        if not PDF_AVAILABLE: 
            return []

        if not self.client:
            print(f"  ‚ö†Ô∏è Extra√ß√£o de texto simples (sem IA) para '{file_path.name}'...")
            artigos_simples: List[Dict[str, Any]] = []
            with fitz.open(file_path) as doc:
                num_paginas = len(doc)
                jornal_fallback = file_path.stem
                for idx, page in enumerate(doc, start=1):
                    texto_pagina = (page.get_text() or '').strip()
                    if not texto_pagina:
                        continue
                    # T√≠tulo como primeira linha da p√°gina
                    primeira_linha = texto_pagina.split('\n', 1)[0].strip()
                    artigos_simples.append({
                        'texto_bruto': texto_pagina,
                        'url_original': None,
                        'metadados': {
                            'titulo': primeira_linha or f"{jornal_fallback} - P√°gina {idx}",
                            'fonte_original': jornal_fallback,
                            'arquivo_origem': file_path.name,
                            'data_processamento': get_datetime_brasil_str(),
                            'tipo_arquivo': 'pdf',
                            'jornal': jornal_fallback,
                            'pagina': idx,
                            'total_paginas_pdf': num_paginas
                        }
                    })
            if not artigos_simples:
                return []
            return artigos_simples

        # Fluxo principal com IA: p√°gina a p√°gina EM PARALELO para performance
        print(f"üöÄ Iniciando extra√ß√£o com IA (p√°gina a p√°gina) para: {file_path.name}")
        artigos_finais: List[Dict[str, Any]] = []
        try:
            with fitz.open(file_path) as doc:
                num_paginas = len(doc)
                print(f"  üìÑ Total de p√°ginas: {num_paginas}")

                def processar_pagina(idx: int) -> List[Dict[str, Any]]:
                    numero_pagina_local = idx + 1
                    print(f"  üîé Processando p√°gina {numero_pagina_local}/{num_paginas}...")
                    with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as temp_file:
                        temp_page_path = Path(temp_file.name)
                    try:
                        with fitz.open() as page_doc:
                            page_doc.insert_pdf(doc, from_page=idx, to_page=idx)
                            page_doc.save(str(temp_page_path))
                        return self._processar_chunk_pdf_com_ia(
                            temp_page_path, file_path.name, numero_pagina=numero_pagina_local
                        )
                    except Exception as e:
                        print(f"  ‚ùå Erro ao processar p√°gina {numero_pagina_local}: {e}")
                        return []
                    finally:
                        if temp_page_path.exists():
                            try:
                                os.remove(temp_page_path)
                            except Exception:
                                pass

                # Executa p√°ginas em paralelo com limite para n√£o saturar a API
                max_workers = min(8, num_paginas)  # limite conservador
                try:
                    import concurrent.futures
                    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                        resultados = list(executor.map(processar_pagina, range(num_paginas)))
                    for lista in resultados:
                        if lista:
                            artigos_finais.extend(lista)
                except Exception as e:
                    # Fallback sequencial em caso de ambientes sem suporte a threads
                    print(f"  ‚ö†Ô∏è Falha no paralelismo, executando sequencialmente: {e}")
                    for idx in range(num_paginas):
                        artigos_finais.extend(processar_pagina(idx))
        except Exception as e:
            print(f"‚ùå Erro cr√≠tico ao orquestrar processamento do PDF '{file_path.name}': {e}")

        return artigos_finais

    def _criar_noticia_basica(self, texto_completo: str, file_path: Path) -> Dict[str, Any]:
        """
        Cria uma not√≠cia b√°sica quando n√£o h√° LLM dispon√≠vel.
        """
        # Extrai primeira linha como t√≠tulo
        linhas = texto_completo.split('\n')
        titulo = linhas[0].strip() if linhas else "Sem t√≠tulo"
        
        return {
            'texto_bruto': texto_completo.strip(),
            'url_original': None,
            'metadados': {
                'titulo': titulo,
                'fonte_original': 'PDF',
                'categoria': 'Geral',
                'data_publicacao': get_date_brasil_str(),
                'data_ultima_modificacao': get_date_brasil_str(),
                'tags_originais': [],
                'id_hash_original': '',
                'arquivo_origem': file_path.name,
                'data_processamento': get_datetime_brasil_str(),
                'tipo_arquivo': 'pdf'
            }
        }

    # --- M√âTODOS EXISTENTES (AJUSTADOS) ---

    def processar_json_dump(self, file_path: Path) -> List[Dict[str, Any]]:
        """Processa arquivo JSON no formato dump_crawlers."""
        try:
            print(f"üìÑ Processando JSON: {file_path.name}")
            
            with open(file_path, 'r', encoding='utf-8') as f:
                dados = json.load(f)
            
            print(f"  üìä Total de itens no JSON: {len(dados)}")
            
            artigos = []
            for i, item in enumerate(dados, 1):
                print(f"  üîç Processando item {i}/{len(dados)}...")
                
                texto_bruto = item.get('texto_completo', item.get('titulo', ''))
                if texto_bruto:
                    artigos.append({
                        'texto_bruto': texto_bruto,
                        'url_original': item.get('link', ''),
                        'metadados': {
                            'titulo': item.get('titulo', ''),
                            'subtitulo': item.get('subtitulo', ''),
                            'fonte_original': item.get('fonte', 'JSON_Dump'),
                            'categoria': item.get('categoria', ''),
                            'data_publicacao': item.get('data_publicacao', ''),
                            'data_ultima_modificacao': item.get('data_ultima_modificacao', ''),
                            'tags_originais': item.get('tags', []),
                            'id_hash_original': item.get('id_hash', ''),
                            'arquivo_origem': file_path.name,
                            'data_processamento': get_datetime_brasil_str(),
                            'tipo_arquivo': 'json',
                            **item  # Adiciona todos os outros campos do JSON original aos metadados
                        }
                    })
                    print(f"    ‚úÖ Item {i} processado: {item.get('titulo', '')[:50]}...")
                else:
                    print(f"    ‚ö†Ô∏è Item {i} sem texto: {item.get('titulo', '')[:50]}...")
            
            print(f"üìä Total de artigos extra√≠dos: {len(artigos)}")
            return artigos
            
        except Exception as e:
            print(f"‚ùå ERRO ao processar JSON {file_path}: {e}")
            import traceback
            traceback.print_exc()
            return []
            
    def processar_arquivo(self, file_path: Path, usar_api: bool) -> int:
        """Processa um √∫nico arquivo (PDF ou JSON) e salva os artigos resultantes."""
        print(f"\nüìÑ Processando arquivo: {file_path.name}")
        
        artigos_brutos = []
        if file_path.suffix.lower() == '.json':
            print(f"üìÑ Arquivo JSON detectado")
            artigos_brutos = self.processar_json_dump(file_path)
        elif file_path.suffix.lower() == '.pdf':
            print(f"üì∞ Arquivo PDF detectado")
            artigos_brutos = self.processar_pdf(file_path)
        else:
            print(f"‚ö†Ô∏è Formato de arquivo n√£o suportado: {file_path.name}")
            return 0
        
        if not artigos_brutos:
            print(f"‚ö†Ô∏è AVISO: Nenhum artigo extra√≠do de {file_path.name}")
            return 0
        
        print(f"üìä Encontrados {len(artigos_brutos)} artigos em {file_path.name}")
        
        # Envia artigos
        sucessos = 0
        for i, artigo in enumerate(artigos_brutos, 1):
            print(f"  üì§ Enviando artigo {i}/{len(artigos_brutos)}...")
            
            if usar_api:
                if self.enviar_artigo_via_api(artigo):
                    sucessos += 1
                    print(f"    ‚úÖ Artigo {i} enviado via API")
                else:
                    print(f"    ‚ùå Falha ao enviar artigo {i} via API")
            else:
                if self.enviar_artigo_direto_db(artigo):
                    sucessos += 1
                    print(f"    ‚úÖ Artigo {i} salvo no banco")
                else:
                    print(f"    ‚ùå Falha ao salvar artigo {i} no banco")
            
            # Aguarda um pouco entre envios
            time.sleep(0.05)
        
        print(f"üéâ SUCESSO: {sucessos}/{len(artigos_brutos)} artigos processados de {file_path.name}")
        return sucessos
        
    def processar_diretorio(self, usar_api: bool) -> Dict[str, int]:
        """Processa um diret√≥rio completo em paralelo para otimizar o tempo."""
        print(f"üöÄ Iniciando processamento do diret√≥rio: {self.files_directory}")
        
        files = list(self.files_directory.glob('*.json')) + list(self.files_directory.glob('*.pdf'))
        
        if not files:
            print("‚ö†Ô∏è Nenhum arquivo .json ou .pdf encontrado para processar.")
            return {"arquivos_processados": 0, "artigos_criados": 0}
            
        print(f"üìä Encontrados {len([f for f in files if f.suffix.lower() == '.json'])} JSONs e {len([f for f in files if f.suffix.lower() == '.pdf'])} PDFs")
        
        # Para processamento sequencial (mais seguro para APIs com rate limits)
        stats = {"arquivos_processados": 0, "artigos_criados": 0}
        
        for i, file_path in enumerate(files, 1):
            print(f"\nüìÑ [{i}/{len(files)}] Processando: {file_path.name}")
            try:
                num_artigos = self.processar_arquivo(file_path, usar_api)
                stats["artigos_criados"] += num_artigos
                if num_artigos > 0:
                    stats["arquivos_processados"] += 1
                    print(f"‚úÖ Conclu√≠do '{file_path.name}': {num_artigos} artigos carregados.")
                else:
                    print(f"‚ö†Ô∏è Nenhum artigo extra√≠do de '{file_path.name}'")
            except Exception as exc:
                print(f"‚ùå Erro ao processar o arquivo {file_path.name}: {exc}")

        print(f"\nüéâ SUCESSO: Processamento finalizado:")
        print(f"   üìÅ Arquivos processados: {stats['arquivos_processados']}")
        print(f"   üì∞ Artigos criados: {stats['artigos_criados']}")
        
        return stats

    # --- M√âTODOS DE ENVIO (MANTIDOS) ---

    def enviar_artigo_via_api(self, artigo_data: Dict[str, Any]) -> bool:
        """
        Envia artigo para a API via HTTP.
        """
        try:
            # Gera hash √∫nico
            hash_unico = self.gerar_hash_artigo(
                artigo_data['texto_bruto'], 
                artigo_data.get('url_original', '')
            )
            
            # Prepara dados
            dados_artigo = {
                "hash_unico": hash_unico,
                "texto_bruto": artigo_data['texto_bruto'],
                "url_original": artigo_data.get('url_original'),
                "fonte_coleta": "file_loader",
                "metadados": artigo_data.get('metadados', {})
            }
            
            # Envia para API
            response = self.session.post(
                f"{self.api_base_url}/internal/novo-artigo",
                json=dados_artigo,
                timeout=30
            )
            
            if response.status_code == 200:
                resultado = response.json()
                print(f"SUCESSO: Artigo enviado via API: {resultado['message']}")
                return True
            else:
                print(f"ERRO: Erro ao enviar artigo via API: {response.status_code} - {response.text}")
                return False
                
        except Exception as e:
            print(f"ERRO: Erro de conex√£o com API: {e}")
            return False

    def enviar_artigo_direto_db(self, artigo_data: Dict[str, Any]) -> bool:
        """
        Envia artigo diretamente para o banco de dados.
        """
        try:
            db = SessionLocal()
            
            # Gera hash √∫nico
            hash_unico = self.gerar_hash_artigo(
                artigo_data['texto_bruto'], 
                artigo_data.get('url_original', '')
            )
            
            # Verifica se j√° existe
            artigo_existente = get_artigo_by_hash(db, hash_unico)
            if artigo_existente:
                print(f"‚ö†Ô∏è AVISO: Artigo j√° existe no banco: {artigo_existente.id}")
                return True
            
            # Detecta tipo de fonte baseado no jornal
            jornal = artigo_data.get('metadados', {}).get('jornal') or artigo_data.get('metadados', {}).get('fonte_original', '')
            # Heur√≠stica ampliada: tenta inferir por nome quando lista interna n√£o captar
            tipo_fonte = self.detectar_tipo_fonte(jornal)
            if not tipo_fonte or tipo_fonte not in ('nacional', 'internacional'):
                tipo_fonte = inferir_tipo_fonte_por_jornal(jornal)
            
            # Cria novo artigo
            dados_artigo = ArtigoBrutoCreate(
                hash_unico=hash_unico,
                texto_bruto=artigo_data['texto_bruto'],
                url_original=artigo_data.get('url_original'),
                fonte_coleta="file_loader",
                metadados=artigo_data.get('metadados', {})
            )
            
            novo_artigo = create_artigo_bruto(db, dados_artigo)
            
            # Salva dados originais nos novos campos
            metadados = artigo_data.get('metadados', {})
            
            # Atualiza campos originais (com verifica√ß√£o de exist√™ncia)
            try:
                if hasattr(novo_artigo, 'subtitulo'):
                    novo_artigo.subtitulo = metadados.get('subtitulo')
                if hasattr(novo_artigo, 'fonte_original'):
                    novo_artigo.fonte_original = metadados.get('fonte_original')
                if hasattr(novo_artigo, 'tags_originais'):
                    novo_artigo.tags_originais = metadados.get('tags_originais')
                if hasattr(novo_artigo, 'id_hash_original'):
                    novo_artigo.id_hash_original = metadados.get('id_hash_original')
                # Se 'jornal' processado vier vazio, preenche com 'fonte_original'
                if hasattr(novo_artigo, 'jornal') and not novo_artigo.jornal:
                    novo_artigo.jornal = metadados.get('jornal') or metadados.get('fonte_original')
                
                # Converte datas se presentes
                if metadados.get('data_publicacao'):
                    try:
                        from datetime import datetime
                        novo_artigo.data_publicacao = datetime.fromisoformat(metadados['data_publicacao'].replace('Z', '+00:00'))
                    except:
                        pass
                
                if metadados.get('data_ultima_modificacao'):
                    try:
                        if hasattr(novo_artigo, 'data_ultima_modificacao'):
                            novo_artigo.data_ultima_modificacao = datetime.fromisoformat(metadados['data_ultima_modificacao'].replace('Z', '+00:00'))
                    except:
                        pass
                
                # Salva categoria original
                if hasattr(novo_artigo, 'categoria'):
                    novo_artigo.categoria = metadados.get('categoria')
                
                # Salva tipo de fonte
                if hasattr(novo_artigo, 'tipo_fonte'):
                    novo_artigo.tipo_fonte = tipo_fonte
                    
            except Exception as e:
                print(f"‚ö†Ô∏è AVISO: Alguns campos novos n√£o est√£o dispon√≠veis: {e}")
            
            db.commit()
            db.refresh(novo_artigo)
            
            create_log(db, "INFO", "file_loader", 
                      f"Artigo criado: {novo_artigo.id}",
                      {"arquivo": artigo_data.get('metadados', {}).get('arquivo_origem', 'desconhecido')})
            
            print(f"‚úÖ SUCESSO: Artigo criado no banco: {novo_artigo.id}")
            return True
            
        except Exception as e:
            print(f"‚ùå ERRO: Erro ao criar artigo no banco: {e}")
            import traceback
            traceback.print_exc()
            return False
        finally:
            if 'db' in locals():
                db.close()

    def verificar_api_status(self) -> bool:
        """Verifica se a API est√° funcionando."""
        try:
            print(f"üîç Verificando status da API: {self.api_base_url}")
            response = self.session.get(f"{self.api_base_url}/health", timeout=10)
            if response.status_code == 200:
                status = response.json()
                print(f"‚úÖ SUCESSO: API Status: {status['status']}")
                return True
            else:
                print(f"‚ùå ERRO: API n√£o est√° saud√°vel: {response.status_code}")
                return False
        except Exception as e:
            print(f"‚ùå ERRO: Erro ao verificar API: {e}")
            return False


def main():
    """Fun√ß√£o principal para carregamento de arquivos."""
    print("=" * 60)
    print("BTG AlphaFeed - Carregador de Arquivos")
    print("=" * 60)
    
    # Configura√ß√µes
    files_dir = input("Diret√≥rio dos arquivos (../pdfs): ").strip()
    if not files_dir:
        files_dir = "../pdfs"
    
    # Cria inst√¢ncia do carregador
    try:
        loader = FileLoader(files_directory=files_dir)
    except FileNotFoundError as e:
        print(f"ERRO: {e}")
        return
    
    # Pergunta sobre m√©todo de envio
    print("\nM√©todo de envio:")
    print("1. Via API HTTP (recomendado)")
    print("2. Direto no banco de dados")
    
    metodo = input("Escolha o m√©todo (1/2): ").strip()
    usar_api = metodo != "2"
    
    if usar_api:
        # Verifica status da API
        if not loader.verificar_api_status():
            print("\nERRO: API n√£o est√° dispon√≠vel. Verifique se o backend est√° rodando.")
            return
    
    # Pergunta se deve executar
    executar = input("\nExecutar carregamento? (s/N): ").lower().strip()
    if executar in ['s', 'sim', 'yes', 'y']:
        stats = loader.processar_diretorio(usar_api=usar_api)
        print(f"\nResumo: {stats['artigos_criados']} artigos criados de {stats['arquivos_processados']} arquivos")
    else:
        print("Carregamento cancelado pelo usu√°rio.")
    
    print("\n" + "=" * 60)
    print("Dicas:")
    print("   - Coloque arquivos JSON e PDF na pasta especificada")
    print("   - JSONs devem seguir o formato dump_crawlers")
    print("   - PDFs ser√£o extra√≠dos usando IA (se cliente Gemini dispon√≠vel)")
    print("   - Artigos duplicados s√£o automaticamente ignorados")
    print("=" * 60)


if __name__ == "__main__":
    main() 