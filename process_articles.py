#!/usr/bin/env python3
"""
Orquestrador para processar artigos pendentes seguindo a l√≥gica do AlphaFeed.
Implementa o fluxo de neg√≥cio correto: 
1. Processar todas as not√≠cias (extrair dados raw)
2. Criar clusters/agrupamentos por fato gerador
3. Classificar prioridade e gerar resumos seletivos
"""

import os
import sys
import json
import re
import time
from datetime import datetime, date
from pathlib import Path
from typing import Dict, Any, List, Optional

# Adiciona o diret√≥rio backend ao path
backend_dir = Path(__file__).parent / "backend"
sys.path.insert(0, str(backend_dir))

# Configura√ß√£o SSL para desenvolvimento
import ssl
ssl._create_default_https_context = ssl._create_unverified_context

# Imports do backend
try:
    from dotenv import load_dotenv
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
    print("‚úÖ Google Gemini dispon√≠vel")
except ImportError:
    GEMINI_AVAILABLE = False
    print("‚ùå AVISO: Google Gemini n√£o est√° dispon√≠vel.")
from sqlalchemy.orm import Session
from sqlalchemy import func

from backend.database import SessionLocal, ArtigoBruto, ClusterEvento
from backend.crud import (
    get_artigos_pendentes, create_log, update_artigo_status, 
    update_artigo_processado, update_artigo_dados_sem_status, associate_artigo_to_cluster,
    create_cluster, get_active_clusters_today, get_artigos_by_cluster,
    get_cluster_by_id
)
from backend.models import Noticia, NoticiaResumida, ResumoFinal
from backend.processing import (
    gerar_embedding, bytes_to_embedding, calcular_similaridade_cosseno,
    processar_artigo_pipeline, gerar_resumo_cluster, find_or_create_cluster
)
from backend.prompts import PROMPT_AGRUPAMENTO_V1, PROMPT_RESUMO_FINAL_V3
from backend.utils import get_date_brasil_str, get_datetime_brasil_str

# Carrega vari√°veis de ambiente
env_file = backend_dir / ".env"
load_dotenv(env_file)
print(f"SUCESSO: Arquivo .env carregado: {env_file}")

# Configura√ß√£o de lotes para evitar truncamento
BATCH_SIZE_AGRUPAMENTO = 150  # Lote maior para aumentar consolida√ß√£o por fato

# Configura√ß√£o do Gemini
api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    print("ERRO: GEMINI_API_KEY n√£o configurada")
    sys.exit(1)

genai.configure(api_key=api_key)
client = genai.GenerativeModel('gemini-2.0-flash')
print("SUCESSO: Gemini configurado com sucesso!")

def extrair_json_da_resposta(resposta: str) -> Any:
    """
    Extrai e decodifica um objeto JSON de uma string de resposta do LLM,
    com um fluxo de tentativas robusto e simplificado.
    """
    import json
    import re

    if not isinstance(resposta, str) or not resposta.strip():
        print("‚ùå ERRO: Resposta da API est√° vazia.")
        return None

    print(f"üîç Processando resposta de {len(resposta)} caracteres...")
    
    json_str = ""
    # Tenta extrair de um bloco de c√≥digo markdown primeiro, que √© o formato mais confi√°vel.
    match = re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', resposta, re.DOTALL)
    if match:
        json_str = match.group(1).strip()
        print(f"üìã JSON extra√≠do de bloco de c√≥digo: {len(json_str)} caracteres")
    else:
        # Se n√£o houver bloco de c√≥digo, busca pelo in√≠cio de um objeto ou array.
        start_pos = resposta.find('[')
        if start_pos == -1:
            start_pos = resposta.find('{')
        
        if start_pos != -1:
            json_str = resposta[start_pos:].strip()
            print(f"üìã JSON extra√≠do por marcador de in√≠cio: {len(json_str)} caracteres")
        else:
            print("‚ùå ERRO: Nenhum marcador de in√≠cio de JSON ('[' ou '{') encontrado na resposta.")
            return None

    # --- FLUXO DE TENTATIVAS DE PARSING ---

    # TENTATIVA 1: Parse Direto
    try:
        return json.loads(json_str)
    except json.JSONDecodeError as e:
        print(f"‚ö†Ô∏è Falha na Tentativa 1 (Parse Direto): {e}")

    # TENTATIVA 2: Extrair a Parte V√°lida (para JSONs truncados)
    json_parcial = extrair_json_valido(json_str)
    if json_parcial:
        print(f"üîß Tentativa 2 (Extra√ß√£o Parcial) obteve um JSON de {len(json_parcial)} caracteres.")
        try:
            return json.loads(json_parcial)
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è Falha na Tentativa 2 mesmo ap√≥s extra√ß√£o parcial: {e}")

    # TENTATIVA 3: Corre√ß√£o de Strings (como √∫ltimo recurso)
    json_corrigido = corrigir_json_strings(json_str)
    if json_corrigido != json_str:
        print(f"üîß Tentativa 3 (Corre√ß√£o de Strings) alterou o JSON para {len(json_corrigido)} caracteres.")
        try:
            return json.loads(json_corrigido)
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è Falha na Tentativa 3 mesmo ap√≥s corre√ß√£o: {e}")

    print("‚ùå ERRO FINAL: Todas as tentativas de extrair um JSON v√°lido falharam.")
    print(f"üìã Primeiros 500 caracteres da resposta problem√°tica: {resposta[:500]}...")
    return None

def corrigir_json_strings(json_str: str) -> str:
    """
    Realiza corre√ß√µes b√°sicas e seguras em uma string JSON.
    O foco √© remover caracteres inv√°lidos e garantir que o escape de aspas
    seja consistente, sem tentar adivinhar o conte√∫do de strings truncadas.
    """
    import re
    
    # 1. Remove caracteres de controle ASCII, exceto os comuns como \n, \r, \t.
    # Isso limpa "lixo" invis√≠vel que quebra o parser.
    json_corrigido = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f]', '', json_str)
    
    # 2. Garante que barras invertidas que n√£o s√£o parte de uma sequ√™ncia de escape v√°lida
    # sejam escapadas. Ex: "caminho\no_arquivo" -> "caminho\\no_arquivo"
    # Isso evita erros de "invalid escape sequence".
    # A express√£o (?!["\\/bfnrtu]) √© um "negative lookahead", garantindo que n√£o vamos
    # escapar barras que j√° fazem parte de uma sequ√™ncia v√°lida.
    json_corrigido = re.sub(r'\\(?!["\\/bfnrtu])', r'\\\\', json_corrigido)
    
    return json_corrigido

def extrair_json_valido(json_str: str) -> str:
    """
    Tenta extrair a maior por√ß√£o inicial de uma string JSON que seja v√°lida.
    Funciona encontrando o √∫ltimo ponto de termina√ß√£o de um objeto JSON completo
    antes do ponto de truncamento.
    """
    # Procura pelo final do √∫ltimo objeto completo em uma lista.
    # O padr√£o √©: uma chave fechando, seguida opcionalmente por espa√ßos, e uma v√≠rgula.
    # Exemplo: ... {"id": 123}, {"id": 456} <-- queremos parar aqui
    last_good_char_pos = json_str.rfind('},')
    
    if last_good_char_pos != -1:
        # Encontramos um objeto intermedi√°rio. Pegamos tudo at√© ele e fechamos o array.
        # Adicionamos 1 para incluir a chave '}' na fatia.
        partial_json = json_str[:last_good_char_pos + 1]
        
        # Se o JSON original come√ßava com '[', n√≥s fechamos o array com ']'
        if partial_json.strip().startswith('['):
             return partial_json.strip() + ']'
        # Se era um objeto √∫nico, apenas retornamos a parte v√°lida
        return partial_json

    # Se n√£o encontrou '},', pode ser um JSON com um √∫nico objeto ou j√° v√°lido.
    # Como fallback, retorna None, pois a extra√ß√£o falhou em encontrar um ponto de corte seguro.
    return None

def processar_artigos_pendentes(limite: int = 10) -> bool:
    """
    Processa artigos pendentes seguindo o fluxo de neg√≥cio correto:
    1. Processa TODAS as not√≠cias pendentes (extra√ß√£o de dados)
    2. Cria clusters/agrupamentos por fato gerador usando prompt de agrupamento
    3. Classifica prioridade e gera resumos seletivos
    """
    db = SessionLocal()
    try:
        # Busca artigos pendentes
        artigos_pendentes = get_artigos_pendentes(db, limite=limite)
        
        if not artigos_pendentes:
            print("SUCESSO: Nenhum artigo pendente encontrado")
            return True
        
        print(f"ARTIGOS: Encontrados {len(artigos_pendentes)} artigos pendentes")
        
        # Estat√≠sticas do banco
        total_artigos = db.query(ArtigoBruto).count()
        artigos_processados = db.query(ArtigoBruto).filter(ArtigoBruto.status == "processado").count()
        artigos_erro = db.query(ArtigoBruto).filter(ArtigoBruto.status == "erro").count()
        clusters_existentes = db.query(ClusterEvento).count()
        
        print(f"ESTATISTICAS: Total artigos: {total_artigos}, Processados: {artigos_processados}, Erros: {artigos_erro}, Clusters: {clusters_existentes}")
        
        # ETAPA 1: Processar TODAS as not√≠cias pendentes
        print(f"\nETAPA 1: Processando {len(artigos_pendentes)} artigos pendentes...")
        
        sucessos = 0
        erros = 0
        
        for i, artigo in enumerate(artigos_pendentes, 1):
            print(f"  PROCESSANDO: Artigo {i}/{len(artigos_pendentes)} (ID: {artigo.id})...")
            
            # Usa a fun√ß√£o do backend para processar cada artigo (SEM clusteriza√ß√£o)
            if processar_artigo_sem_cluster(db, artigo.id, client):
                sucessos += 1
                print(f"    SUCESSO: Artigo {artigo.id} processado")
            else:
                erros += 1
                print(f"    ERRO: Falha no processamento do artigo {artigo.id}")
            
            time.sleep(0.1)  # Pausa leve entre processamentos (throttle)
        
        print(f"ETAPA 1 CONCLUIDA: Sucessos: {sucessos}, Erros: {erros}")
        
        # ETAPA 2: Agrupamento inteligente com pivot autom√°tico
        print(f"\nETAPA 2: Agrupamento inteligente com pivot autom√°tico...")
        
        # Verifica se h√° clusters existentes hoje para decidir o modo
        hoje = get_date_brasil_str()
        clusters_existentes = db.query(ClusterEvento).filter(
            func.date(ClusterEvento.created_at) == hoje,
            ClusterEvento.status == 'ativo'
        ).count()
        
        if clusters_existentes > 0:
            # Modo incremental: anexa a clusters existentes
            print(f"üéØ MODO INCREMENTAL: {clusters_existentes} clusters existentes encontrados")
            sucesso_agrupamento = agrupar_noticias_incremental(db, client)
        else:
            # Modo em lote: cria clusters do zero
            print("üéØ MODO EM LOTE: Nenhum cluster existente, criando do zero")
            sucesso_agrupamento = agrupar_noticias_com_prompt(db, client)
        
        if sucesso_agrupamento:
            print("ETAPA 2 CONCLUIDA: Agrupamento realizado com sucesso")
        else:
            print("ETAPA 2 FALHOU: Erro no agrupamento")
            return False
        
        # ETAPA 3: Classificar e gerar resumos usando prompts
        print(f"\nETAPA 3: Classificando clusters e gerando resumos...")
        
        # Busca apenas clusters ativos hoje que N√ÉO t√™m resumo (novos)
        hoje = get_date_brasil_str()
        clusters_hoje = db.query(ClusterEvento).filter(
            ClusterEvento.status == 'ativo',
            ClusterEvento.created_at >= hoje,
            ClusterEvento.resumo_cluster.is_(None)  # Apenas clusters sem resumo
        ).all()
        
        resumos_gerados = 0
        
        for cluster in clusters_hoje:
            print(f"  üìù Processando cluster {cluster.id}: '{cluster.titulo_cluster}'")
            
            # Classifica o cluster usando prompts do prompts.py
            if classificar_e_resumir_cluster(db, cluster.id, client):
                resumos_gerados += 1
                print(f"    ‚úÖ Cluster {cluster.id} - Classificado e resumido")
            else:
                print(f"    ‚ùå Cluster {cluster.id} - Falha na classifica√ß√£o")
        
        print(f"ETAPA 3 CONCLUIDA: Resumos gerados: {resumos_gerados}")
        
        # Resumo final
        print(f"\nPROCESSAMENTO CONCLUIDO:")
        print(f"  Artigos processados: {sucessos}")
        print(f"  Resumos gerados: {resumos_gerados}")
        
        return True
        
    except Exception as e:
        print(f"ERRO: Erro geral no processamento: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        db.close()

def mapear_tag_prompt_para_modelo(tag_prompt: str) -> str:
    """
    Mapeia as tags do prompt para as tags v√°lidas do modelo.
    Agora as tags s√£o as mesmas, ent√£o s√≥ retorna a tag original.
    """
    # As tags do prompt j√° s√£o as mesmas do modelo
    return tag_prompt

def migrar_tag_antiga_para_nova(tag_antiga: str) -> str:
    """
    Migra tags antigas para as novas tags do TAGS_SPECIAL_SITUATIONS.
    """
    mapeamento_antigo = {
        'Economia e Tecnologia': 'Internacional (Economia e Pol√≠tica)',
        'Governo e Politica': 'Pol√≠tica Econ√¥mica (Brasil)',
        'Judicionario': 'Jur√≠dico, Fal√™ncias e Regulat√≥rio',
        'Empresas Privadas': 'Mercado de Capitais e Finan√ßas Corporativas'
    }
    
    return mapeamento_antigo.get(tag_antiga, 'Internacional (Economia e Pol√≠tica)')

def marcar_cluster_irrelevante(db: Session, cluster_id: int, debug: bool = True) -> bool:
    """
    Marca um cluster como irrelevante quando o prompt retorna lista vazia.
    """
    try:
        cluster = get_cluster_by_id(db, cluster_id)
        if not cluster:
            return False
        
        # Marca como irrelevante
        cluster.prioridade = "IRRELEVANTE"
        cluster.tag = "IRRELEVANTE"
        cluster.resumo_cluster = "Not√≠cia irrelevante para a mesa de Special Situations"
        
        db.commit()
        
        if debug:
            print(f"    üö´ DEBUG: Cluster {cluster_id} marcado como IRRELEVANTE")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ERRO: Falha ao marcar cluster {cluster_id} como irrelevante: {e}")
        return False

def classificar_e_resumir_cluster(db: Session, cluster_id: int, client, debug: bool = True) -> bool:
    """
    Classifica e resume um cluster usando os prompts do prompts.py.
    Esta fun√ß√£o usa o PROMPT_EXTRACAO_PERMISSIVO_V8 para classificar o cluster
    e depois gera o resumo apropriado baseado na prioridade.
    """
    try:
        cluster = get_cluster_by_id(db, cluster_id)
        if not cluster:
            if debug:
                print(f"    ‚ùå DEBUG: Cluster {cluster_id} n√£o encontrado")
            return False
        
        artigos = get_artigos_by_cluster(db, cluster_id)
        if not artigos:
            if debug:
                print(f"    ‚ùå DEBUG: Nenhum artigo encontrado para cluster {cluster_id}")
            return False
        
        if debug:
            print(f"    üîç DEBUG: Cluster {cluster_id} tem {len(artigos)} artigos")
        
        # Coleta todos os textos dos artigos para an√°lise
        textos = []
        for i, artigo in enumerate(artigos):
            if artigo.texto_processado:
                texto_artigo = f"FONTE: {artigo.jornal or 'Desconhecida'}\n{artigo.texto_processado}"
                textos.append(texto_artigo)
                if debug:
                    print(f"    üìÑ DEBUG: Artigo {i+1}: {artigo.titulo_extraido or 'Sem t√≠tulo'}")
                    print(f"    üìÑ DEBUG: Texto: {artigo.texto_processado[:100]}...")
        
        texto_completo = "\n\n".join(textos)
        
        if debug:
            print(f"    üìù DEBUG: Texto completo para an√°lise ({len(texto_completo)} chars):")
            print(f"    {'='*50}")
            print(texto_completo[:500] + "..." if len(texto_completo) > 500 else texto_completo)
            print(f"    {'='*50}")
        
        # Usa o prompt de extra√ß√£o para classificar o cluster
        from backend.prompts import PROMPT_EXTRACAO_PERMISSIVO_V8
        
        prompt_classificacao = f"""
        {PROMPT_EXTRACAO_PERMISSIVO_V8}
        
        NOT√çCIA PARA AN√ÅLISE:
        {texto_completo}
        
        Analise esta not√≠cia e retorne a classifica√ß√£o conforme o guia acima.
        """
        
        if debug:
            print(f"    ü§ñ DEBUG: Enviando prompt para Gemini...")
            print(f"    ü§ñ DEBUG: Tamanho do prompt: {len(prompt_classificacao)} chars")
            print(f"    ü§ñ DEBUG: Primeiros 300 chars do prompt:")
            print(f"    {'='*50}")
            print(prompt_classificacao[:300] + "...")
            print(f"    {'='*50}")
        
        response = client.generate_content(
            prompt_classificacao,
            generation_config={
                'temperature': 0.3,
                'top_p': 0.9,
                'max_output_tokens': 1024
            }
        )
        
        if not response.text:
            if debug:
                print(f"    ‚ùå DEBUG: API retornou resposta vazia para cluster {cluster_id}")
            return False
        
        if debug:
            print(f"    ü§ñ DEBUG: Resposta do Gemini ({len(response.text)} chars):")
            print(f"    {'='*50}")
            print(response.text)
            print(f"    {'='*50}")
        
        # Extrai JSON da resposta
        resultado = extrair_json_da_resposta(response.text)
        
        if debug:
            print(f"    üîç DEBUG: Resultado extra√≠do: {resultado}")
        
        # PRIMEIRO, verifica se a resposta √© uma lista vazia, que significa "irrelevante"
        if isinstance(resultado, list) and len(resultado) == 0:
            if debug:
                print(f"    üö´ DEBUG: Not√≠cia irrelevante detectada (API retornou lista vazia)")
            return marcar_cluster_irrelevante(db, cluster_id, debug)
        
        # DEPOIS, continua com a valida√ß√£o original para outros tipos de erro
        if not resultado or not isinstance(resultado, list):
            if debug:
                print(f"    ‚ùå DEBUG: Resposta inv√°lida para cluster {cluster_id}")
                print(f"    ‚ùå DEBUG: Tipo do resultado: {type(resultado)}")
                print(f"    ‚ùå DEBUG: Conte√∫do: {resultado}")
            return False
        
        # Pega o primeiro resultado (deveria ser s√≥ um)
        classificacao = resultado[0]
        
        if debug:
            print(f"    ‚úÖ DEBUG: Classifica√ß√£o extra√≠da: {classificacao}")
        
        # Atualiza o cluster com a classifica√ß√£o
        prioridade_original = cluster.prioridade
        tag_original = cluster.tag
        
        cluster.prioridade = classificacao.get('prioridade', 'P3_MONITORAMENTO')
        cluster.tag = mapear_tag_prompt_para_modelo(classificacao.get('tag', 'Sem categoria'))
        
        if debug:
            print(f"    üîÑ DEBUG: Prioridade: {prioridade_original} ‚Üí {cluster.prioridade}")
            print(f"    üîÑ DEBUG: Tag: {tag_original} ‚Üí {cluster.tag}")
        
        # Gera resumo baseado na prioridade usando fun√ß√£o unificada
        prioridade = cluster.prioridade
        
        # Mapeia prioridade para n√≠vel de detalhe
        mapa_niveis = {
            'P1_CRITICO': 'Executivo (P1_CRITICO)',
            'P2_ESTRATEGICO': 'Padr√£o (P2_ESTRATEGICO)',
            'P3_MONITORAMENTO': 'Conciso (P3_MONITORAMENTO)'
        }
        
        nivel_detalhe = mapa_niveis.get(prioridade)
        if nivel_detalhe:
            if debug:
                print(f"    üìù DEBUG: Gerando resumo {prioridade} com n√≠vel {nivel_detalhe}...")
            
            if gerar_resumo_unificado(db, cluster_id, client, nivel_detalhe):
                print(f"    üìù {prioridade}: Resumo gerado com sucesso")
            else:
                print(f"    ‚ùå Falha ao gerar resumo {prioridade}")
                return False
        else:
            if debug:
                print(f"    ‚ö†Ô∏è DEBUG: Prioridade {prioridade} n√£o mapeada para n√≠vel de detalhe")
        
        # Salva as mudan√ßas
        db.commit()
        
        if debug:
            print(f"    ‚úÖ DEBUG: Cluster {cluster_id} salvo com sucesso")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ERRO: Falha ao classificar e resumir cluster {cluster_id}: {e}")
        if debug:
            import traceback
            traceback.print_exc()
        return False

def gerar_resumo_unificado(db: Session, cluster_id: int, client, nivel_detalhe: str) -> bool:
    """
    Gera um resumo para um cluster com um n√≠vel de detalhe vari√°vel.
    Usa o PROMPT_RESUMO_FINAL_V3 para consist√™ncia.
    """
    try:
        cluster = get_cluster_by_id(db, cluster_id)
        if not cluster:
            return False
        
        artigos = get_artigos_by_cluster(db, cluster_id)
        if not artigos:
            return False
        
        # Coleta todos os textos dos artigos
        textos = []
        for artigo in artigos:
            if artigo.texto_processado:
                textos.append(f"FONTE: {artigo.jornal or 'Desconhecida'}\n{artigo.texto_processado}")
        
        texto_completo = "\n\n".join(textos)
        
        # Prepara dados do cluster para o prompt
        dados_do_grupo = {
            "tema_principal": cluster.titulo_cluster,
            "categoria": cluster.tag,
            "prioridade": cluster.prioridade,
            "noticias": [
                {
                    "titulo": artigo.titulo_extraido or "Sem t√≠tulo",
                    "texto": artigo.texto_processado or "",
                    "jornal": artigo.jornal or "Fonte desconhecida"
                }
                for artigo in artigos
            ]
        }
        
        # Usa o PROMPT_RESUMO_FINAL_V3 importado de prompts.py
        prompt_completo = PROMPT_RESUMO_FINAL_V3.format(
            NIVEL_DE_DETALHE=nivel_detalhe,
            DADOS_DO_GRUPO=json.dumps(dados_do_grupo, indent=2, ensure_ascii=False)
        )
        
        response = client.generate_content(
            prompt_completo,
            generation_config={
                'temperature': 0.3,
                'top_p': 0.9,
                'max_output_tokens': 2048 if nivel_detalhe == 'Executivo (P1_CRITICO)' else 512
            }
        )
        
        if not response.text:
            return False
        
        # Extrai JSON da resposta
        resultado_json = extrair_json_da_resposta(response.text)
        
        if resultado_json and 'resumo_final' in resultado_json:
            # Remove o t√≠tulo "Resumo Executivo:" se ele aparecer
            resumo_limpo = resultado_json['resumo_final']
            if ': ' in resumo_limpo:
                resumo_limpo = resumo_limpo.split(': ', 1)[1]
            
            cluster.resumo_cluster = resumo_limpo
            db.commit()
            return True
        
        return False
        
    except Exception as e:
        print(f"ERRO: Falha ao gerar resumo unificado para cluster {cluster_id}: {e}")
        return False

def agrupar_noticias_incremental(db: Session, client) -> bool:
    """
    Agrupamento incremental: anexa novas not√≠cias a clusters existentes ou cria novos clusters.
    Usa o prompt PROMPT_AGRUPAMENTO_INCREMENTAL_V1 para decis√£o inteligente.
    Processa em lotes se houver muitas not√≠cias para evitar truncamento.
    """
    try:
        # Busca artigos prontos para agrupamento que n√£o foram associados a clusters
        hoje = get_date_brasil_str()
        artigos_novos = db.query(ArtigoBruto).filter(
            ArtigoBruto.status == "pronto_agrupar",
            ArtigoBruto.cluster_id.is_(None)  # Artigos n√£o associados a clusters
        ).all()
        
        if not artigos_novos:
            print("INFO: Nenhum artigo novo encontrado para agrupamento incremental")
            return True
        
        # Busca clusters existentes criados hoje
        clusters_existentes = db.query(ClusterEvento).filter(
            func.date(ClusterEvento.created_at) == hoje,
            ClusterEvento.status == 'ativo'
        ).all()
        
        print(f"üîó AGRUPAMENTO INCREMENTAL: {len(artigos_novos)} artigos novos, {len(clusters_existentes)} clusters existentes")
        
        # Determina se precisa processar em lotes
        TAMANHO_LOTE_MAXIMO = 150  # M√°ximo de not√≠cias por lote (incremental mais abrangente)
        processar_em_lotes = len(artigos_novos) > TAMANHO_LOTE_MAXIMO
        
        if processar_em_lotes:
            print(f"üì¶ PROCESSAMENTO EM LOTES: {len(artigos_novos)} not√≠cias divididas em lotes de {TAMANHO_LOTE_MAXIMO}")
            
            # Divide artigos em lotes
            lotes = [artigos_novos[i:i + TAMANHO_LOTE_MAXIMO] for i in range(0, len(artigos_novos), TAMANHO_LOTE_MAXIMO)]
            
            total_anexacoes = 0
            total_novos_clusters = 0
            
            for i, lote in enumerate(lotes, 1):
                print(f"\nüì¶ PROCESSANDO LOTE {i}/{len(lotes)} ({len(lote)} not√≠cias)...")
                
                sucesso_lote = processar_lote_incremental(db, client, lote, clusters_existentes, i)
                
                if sucesso_lote:
                    anexacoes, novos_clusters = sucesso_lote
                    total_anexacoes += anexacoes
                    total_novos_clusters += novos_clusters
                    print(f"‚úÖ LOTE {i} CONCLUIDO: {anexacoes} anexa√ß√µes, {novos_clusters} novos clusters")
                else:
                    print(f"‚ùå LOTE {i} FALHOU")
                    return False
            
            print(f"üéâ AGRUPAMENTO INCREMENTAL CONCLUIDO: {total_anexacoes} anexa√ß√µes, {total_novos_clusters} novos clusters")
            
            # Marca artigos como "processado" ap√≥s clusteriza√ß√£o
            marcar_artigos_processados(db, artigos_novos)
            
            return True
        else:
            # Processa tudo de uma vez (caso original)
            resultado = processar_lote_incremental(db, client, artigos_novos, clusters_existentes, 1)
            if resultado:
                # Marca artigos como "processado" ap√≥s clusteriza√ß√£o
                marcar_artigos_processados(db, artigos_novos)
            return resultado
        
    except Exception as e:
        print(f"‚ùå ERRO: Falha no agrupamento incremental: {e}")
        import traceback
        traceback.print_exc()
        return False

def marcar_artigos_processados(db: Session, artigos: List[ArtigoBruto]) -> None:
    """
    Marca artigos como "processado" ap√≥s clusteriza√ß√£o bem-sucedida.
    """
    try:
        for artigo in artigos:
            artigo.status = "processado"
            artigo.processed_at = get_datetime_brasil_str()
        
        db.commit()
        print(f"‚úÖ {len(artigos)} artigos marcados como 'processado' ap√≥s clusteriza√ß√£o")
        
    except Exception as e:
        print(f"‚ùå ERRO: Falha ao marcar artigos como processado: {e}")
        db.rollback()

def processar_lote_incremental(db: Session, client, artigos_lote: List[ArtigoBruto], clusters_existentes: List[ClusterEvento], numero_lote: int = 1) -> tuple:
    """
    Processa um lote de artigos no agrupamento incremental.
    Retorna (anexacoes, novos_clusters) ou False se falhar.
    """
    try:
        # Prepara dados para o prompt incremental
        novas_noticias = []
        for i, artigo in enumerate(artigos_lote):
            noticia_data = {
                "id": i,
                "titulo": artigo.titulo_extraido or "Sem t√≠tulo"
            }
            novas_noticias.append(noticia_data)
        
        clusters_existentes_data = []
        for i, cluster in enumerate(clusters_existentes):
            artigos_cluster = get_artigos_by_cluster(db, cluster.id)
            titulos = [
                a.titulo_extraido or (a.texto_processado[:80] + "...") if (a.texto_processado or "") else "Sem t√≠tulo"
                for a in artigos_cluster
            ]
            titulos = titulos[:30]
            cluster_data = {
                "cluster_id": cluster.id,
                "tema_principal": cluster.titulo_cluster,
                "titulos_internos": titulos
            }
            clusters_existentes_data.append(cluster_data)
        
        # Mapeamento de ID para artigo original
        mapa_id_para_artigo = {i: artigo for i, artigo in enumerate(artigos_lote)}
        
        # Monta o prompt incremental
        from backend.prompts import PROMPT_AGRUPAMENTO_INCREMENTAL_V2
        prompt_completo = PROMPT_AGRUPAMENTO_INCREMENTAL_V2.format(
            NOVAS_NOTICIAS=json.dumps(novas_noticias, indent=2, ensure_ascii=False),
            CLUSTERS_EXISTENTES=json.dumps(clusters_existentes_data, indent=2, ensure_ascii=False)
        )
        
        print(f"üì§ ENVIANDO LOTE {numero_lote}: {len(novas_noticias)} not√≠cias para an√°lise incremental...")
        
        # Chama a API para an√°lise incremental
        try:
            response = client.generate_content(
                prompt_completo,
                generation_config={
                    'temperature': 0.1,  # Mais determin√≠stico
                    'top_p': 0.8,
                    'max_output_tokens': 8192,  # Aumentado para lotes maiores
                    'candidate_count': 1
                }
            )
            
            if not response.text:
                print("‚ùå ERRO: API retornou resposta vazia para agrupamento incremental")
                return False
            
            print(f"üì• RESPOSTA RECEBIDA LOTE {numero_lote}: {len(response.text)} caracteres")
            
            # Extrai JSON da resposta
            classificacoes = extrair_json_da_resposta(response.text)
            
            if not classificacoes or not isinstance(classificacoes, list):
                print("‚ùå ERRO: Resposta de agrupamento incremental inv√°lida")
                print(f"üìã Resposta recebida: {response.text[:500]}...")
                return False
            
            print(f"‚úÖ SUCESSO LOTE {numero_lote}: {len(classificacoes)} classifica√ß√µes recebidas")
            
            # Processa cada classifica√ß√£o
            anexacoes = 0
            novos_clusters = 0
            
            for classificacao in classificacoes:
                try:
                    tipo = classificacao.get("tipo")
                    noticia_id = classificacao.get("noticia_id")
                    
                    if noticia_id not in mapa_id_para_artigo:
                        print(f"  ‚ö†Ô∏è ID de not√≠cia inv√°lido: {noticia_id}")
                        continue
                    
                    artigo = mapa_id_para_artigo[noticia_id]
                    
                    if tipo == "anexar":
                        # Anexa a cluster existente
                        cluster_id_existente = classificacao.get("cluster_id_existente")
                        cluster_existente = next((c for c in clusters_existentes if c.id == cluster_id_existente), None)
                        
                        if cluster_existente:
                            associate_artigo_to_cluster(db, artigo.id, cluster_existente.id)
                            anexacoes += 1
                            print(f"  ‚úÖ Anexado: '{artigo.titulo_extraido}' ‚Üí Cluster {cluster_existente.id}")
                        else:
                            print(f"  ‚ùå Cluster {cluster_id_existente} n√£o encontrado")
                    
                    elif tipo == "novo_cluster":
                        # Cria novo cluster
                        tema_principal = classificacao.get("tema_principal", f"Novo Cluster - {artigo.titulo_extraido}")
                        
                        # Calcula embedding do artigo
                        embedding_medio = None
                        if artigo.embedding:
                            embedding_medio = artigo.embedding
                        
                        # Cria cluster
                        from backend.models import ClusterEventoCreate
                        cluster_data = ClusterEventoCreate(
                            titulo_cluster=tema_principal,
                            resumo_cluster=None,  # Ser√° preenchido na ETAPA 3
                            tag="Internacional (Economia e Pol√≠tica)",  # Padr√£o, ser√° redefinido na ETAPA 3
                            prioridade="P3_MONITORAMENTO",  # Padr√£o, ser√° redefinido na ETAPA 3
                            embedding_medio=embedding_medio
                        )
                        
                        cluster = create_cluster(db, cluster_data)
                        associate_artigo_to_cluster(db, artigo.id, cluster.id)
                        novos_clusters += 1
                        print(f"  ‚úÖ Novo Cluster: '{tema_principal}' com '{artigo.titulo_extraido}'")
                    
                    else:
                        print(f"  ‚ö†Ô∏è Tipo de classifica√ß√£o inv√°lido: {tipo}")
                
                except Exception as e:
                    print(f"  ‚ùå Erro ao processar classifica√ß√£o: {e}")
                    continue
            
            return (anexacoes, novos_clusters)
            
        except Exception as e:
            print(f"‚ùå ERRO na chamada da API incremental: {e}")
            return False
        
    except Exception as e:
        print(f"‚ùå ERRO: Falha no processamento do lote {numero_lote}: {e}")
        import traceback
        traceback.print_exc()
        return False


def agrupar_noticias_com_prompt(db: Session, client) -> bool:
    """
    Agrupa not√≠cias usando prompt, agora processando em lotes (batches)
    para evitar truncamento de resposta da API com grandes volumes.
    """
    try:
        # Busca todas as not√≠cias prontas para agrupamento
        artigos_para_agrupar = db.query(ArtigoBruto).filter(
            ArtigoBruto.status == "pronto_agrupar",
            ArtigoBruto.cluster_id.is_(None)
        ).all()
        
        if not artigos_para_agrupar:
            print("INFO: Nenhum artigo novo para agrupamento.")
            return True
        
        print(f"üîó INICIANDO AGRUPAMENTO: {len(artigos_para_agrupar)} artigos a serem processados em lotes de {BATCH_SIZE_AGRUPAMENTO}.")
        
        # Mapeamento de ID para artigo original para todo o conjunto
        mapa_id_para_artigo = {i: artigo for i, artigo in enumerate(artigos_para_agrupar)}
        
        # Divide a lista de artigos em lotes
        lotes = [artigos_para_agrupar[i:i + BATCH_SIZE_AGRUPAMENTO] for i in range(0, len(artigos_para_agrupar), BATCH_SIZE_AGRUPAMENTO)]
        
        clusters_criados_total = 0
        artigos_agrupados_total = 0

        for num_lote, lote_artigos in enumerate(lotes, 1):
            print(f"\n--- Processando Lote {num_lote}/{len(lotes)} ({len(lote_artigos)} artigos) ---")
            
            # Prepara dados apenas para o lote atual
            noticias_lote_para_prompt = []
            mapa_id_lote_para_artigo = {}
            for i, artigo in enumerate(lote_artigos):
                 # O 'id' aqui √© relativo ao lote, para o prompt.
                noticia_data = {
                    "id": i,
                    "titulo": artigo.titulo_extraido or "Sem t√≠tulo",
                    "jornal": artigo.jornal or "Fonte desconhecida",
                    "trecho": (artigo.texto_processado[:150] + "...") if len(artigo.texto_processado or "") > 150 else (artigo.texto_processado or "")
                }
                noticias_lote_para_prompt.append(noticia_data)
                mapa_id_lote_para_artigo[i] = artigo # Mapeia o ID do lote para o objeto artigo completo

            # Monta o prompt completo para o lote
            prompt_completo = f"""
{PROMPT_AGRUPAMENTO_V1}

NOT√çCIAS PARA AGRUPAR (LOTE {num_lote}/{len(lotes)}):
{json.dumps(noticias_lote_para_prompt, indent=2, ensure_ascii=False)}

IMPORTANTE: Retorne APENAS o JSON v√°lido para este lote.
"""
            
            print(f"üì§ ENVIANDO Lote {num_lote} para a API...")
            
            try:
                # Chama a API para o lote
                response = client.generate_content(
                    prompt_completo,
                    generation_config={
                        'temperature': 0.05,  # Ainda mais determin√≠stico
                        'top_p': 0.7,
                        'max_output_tokens': 8192,  # Reduzido para lotes menores
                        'candidate_count': 1,
                        'top_k': 10
                    }
                )
                
                if not response.text:
                    print(f"‚ö†Ô∏è AVISO: API retornou resposta vazia para o lote {num_lote}. Pulando este lote.")
                    continue
                
                print(f"üì• RESPOSTA RECEBIDA para o Lote {num_lote}: {len(response.text)} caracteres")
                
                # Usa a nova fun√ß√£o de extra√ß√£o robusta
                grupos_brutos = extrair_json_da_resposta(response.text)
                
                if not grupos_brutos or not isinstance(grupos_brutos, list):
                    print(f"‚ùå ERRO: Resposta de agrupamento inv√°lida para o lote {num_lote}.")
                    continue
                
                print(f"‚úÖ SUCESSO LOTE {num_lote}: {len(grupos_brutos)} grupos criados.")
                
                # Processa os clusters do lote
                for grupo_data in grupos_brutos:
                    try:
                        tema_principal = grupo_data.get("tema_principal", f"Grupo Lote {num_lote}")
                        ids_no_lote = grupo_data.get("ids_originais", [])
                        artigos_do_grupo = [mapa_id_lote_para_artigo[id_lote] for id_lote in ids_no_lote if id_lote in mapa_id_lote_para_artigo]
                        
                        if not artigos_do_grupo:
                            continue
                        
                        # ETAPA 2: S√ì AGRUPA - usa valores padr√£o, ser√£o redefinidos na ETAPA 3
                        prioridade_grupo = "P3_MONITORAMENTO"  # Padr√£o, ser√° redefinido na ETAPA 3
                        tag_grupo = "Internacional (Economia e Pol√≠tica)"  # Padr√£o, ser√° redefinido na ETAPA 3
                        
                        # Calcula embedding m√©dio do grupo
                        embeddings = []
                        for artigo in artigos_do_grupo:
                            if artigo.embedding:
                                embeddings.append(bytes_to_embedding(artigo.embedding))
                        
                        embedding_medio = None
                        if embeddings:
                            import numpy as np
                            embedding_medio = np.mean(embeddings, axis=0).tobytes()
                        
                        # Cria cluster
                        from backend.models import ClusterEventoCreate
                        cluster_data = ClusterEventoCreate(
                            titulo_cluster=tema_principal,
                            resumo_cluster=None,  # Ser√° preenchido na ETAPA 3
                            tag=tag_grupo,
                            prioridade=prioridade_grupo,
                            embedding_medio=embedding_medio
                        )
                        
                        cluster = create_cluster(db, cluster_data)
                        clusters_criados_total += 1
                        
                        # Associa artigos ao cluster
                        for artigo in artigos_do_grupo:
                            associate_artigo_to_cluster(db, artigo.id, cluster.id)
                            artigos_agrupados_total += 1
                        
                        print(f"  ‚úÖ Grupo: '{tema_principal}' - {len(artigos_do_grupo)} artigos")
                        
                    except Exception as e:
                        print(f"  ‚ùå Erro ao processar grupo no lote {num_lote}: {e}")
                        continue

            except Exception as e_lote:
                print(f"‚ùå ERRO CR√çTICO ao processar o lote {num_lote}: {e_lote}")
                continue # Pula para o pr√≥ximo lote

        print(f"\nüéâ ETAPA 2 CONCLU√çDA: {clusters_criados_total} clusters criados, {artigos_agrupados_total} artigos agrupados no total.")
        
        # Marca artigos como "processado" ap√≥s clusteriza√ß√£o
        artigos_agrupados = db.query(ArtigoBruto).filter(
            ArtigoBruto.status == "pronto_agrupar",
            ArtigoBruto.cluster_id.isnot(None)  # Artigos que foram agrupados
        ).all()
        marcar_artigos_processados(db, artigos_agrupados)
        
        return True
        
    except Exception as e:
        print(f"‚ùå ERRO GERAL na fun√ß√£o de agrupamento: {e}")
        import traceback
        traceback.print_exc()
        return False

def processar_artigos_em_lote(limite: int = 10) -> bool:
    """
    Processa artigos em lote (modo alternativo para reprocessamento).
    Usado quando queremos reagrupar todos os artigos do dia.
    """
    db = SessionLocal()
    try:
        # Busca artigos pendentes
        artigos_pendentes = get_artigos_pendentes(db, limite=limite)
        
        if not artigos_pendentes:
            print("‚úÖ Nenhum artigo pendente encontrado")
            return True
        
        print(f"üì∞ Encontrados {len(artigos_pendentes)} artigos pendentes")
        
        # ETAPA 1: Processa todos os artigos pendentes
        print(f"\nüîÑ ETAPA 1: Processando {len(artigos_pendentes)} artigos pendentes...")
        
        sucessos = 0
        erros = 0
        
        for i, artigo in enumerate(artigos_pendentes, 1):
            print(f"  üì§ Processando artigo {i}/{len(artigos_pendentes)} (ID: {artigo.id})...")
            
            # Processa artigo sem clusteriza√ß√£o autom√°tica
            if processar_artigo_sem_cluster(db, artigo.id, client):
                sucessos += 1
            else:
                erros += 1
            
            time.sleep(0.1)
        
        print(f"\n‚úÖ Processamento de artigos finalizado:")
        print(f"   üì∞ Artigos processados: {len(artigos_pendentes)}")
        print(f"   ‚úÖ Sucessos: {sucessos}")
        print(f"   ‚ùå Erros: {erros}")
        
        # ETAPA 2: Busca artigos processados hoje para agrupamento
        hoje = get_date_brasil_str()
        artigos_hoje = db.query(ArtigoBruto).filter(
            ArtigoBruto.status == "processado",
            ArtigoBruto.processed_at >= hoje
        ).all()
        
        if not artigos_hoje:
            print("‚úÖ Nenhum artigo processado hoje para agrupamento")
            return True
        
        print(f"\nüîó ETAPA 2: Agrupando {len(artigos_hoje)} artigos processados hoje...")
        
        # ETAPA 3: Agrupa not√≠cias por similaridade
        grupos = agrupar_noticias_por_similaridade(db, artigos_hoje)
        
        if not grupos:
            print("‚úÖ Nenhum grupo formado")
            return True
        
        # ETAPA 4: Gera resumos apenas para grupos P1 e P2
        print(f"\nüìù ETAPA 3: Gerando resumos para {len(grupos)} grupos...")
        
        clusters_criados = 0
        resumos_gerados = 0
        
        for i, grupo in enumerate(grupos, 1):
            print(f"  üìù Processando grupo {i}/{len(grupos)} com {len(grupo)} not√≠cias...")
            
            # Verifica prioridade do grupo
            prioridade_grupo = grupo[0].prioridade
            print(f"    üìä Prioridade do grupo: {prioridade_grupo}")
            
            # Cria cluster
            try:
                from backend.models import ClusterEventoCreate
                
                # Calcula embedding m√©dio do cluster
                embeddings = []
                for artigo in grupo:
                    if artigo.embedding:
                        embeddings.append(bytes_to_embedding(artigo.embedding))
                
                embedding_medio = None
                if embeddings:
                    import numpy as np
                    embedding_medio = np.mean(embeddings, axis=0).tobytes()
                
                # Cria cluster com dados b√°sicos
                cluster_data = ClusterEventoCreate(
                    titulo_cluster=f"Cluster {i} - {len(grupo)} not√≠cias",
                    resumo_cluster=None,  # Ser√° preenchido se necess√°rio
                    tag=grupo[0].tag,
                    prioridade=prioridade_grupo,
                    embedding_medio=embedding_medio
                )
                
                # Cria o cluster
                cluster = create_cluster(db, cluster_data)
                clusters_criados += 1
                
                # Associa artigos ao cluster
                for artigo in grupo:
                    associate_artigo_to_cluster(db, artigo.id, cluster.id)
                
                # Gera resumo apenas para P1 e P2
                if prioridade_grupo in ['P1_CRITICO', 'P2_ESTRATEGICO']:
                    print(f"    üìù Gerando resumo para cluster {cluster.id} (Prioridade: {prioridade_grupo})...")
                    
                if gerar_resumo_cluster(db, cluster.id, client):
                    resumos_gerados += 1
                    print(f"    ‚úÖ Resumo gerado com sucesso")
                else:
                    print(f"    ‚ùå Falha ao gerar resumo")
                # else:
                #     print(f"    ‚ÑπÔ∏è Cluster {cluster.id} (Prioridade: {prioridade_grupo}) n√£o requer resumo. Pulando.")
                
            except Exception as e:
                print(f"    ‚ùå Erro ao criar cluster: {e}")
                continue
        
        print(f"\nüéâ Processamento em lote finalizado:")
        print(f"   üì∞ Artigos processados: {sucessos}")
        print(f"   üîó Clusters criados: {clusters_criados}")
        print(f"   üìù Resumos gerados: {resumos_gerados}")
        print(f"   üìä Grupos processados: {len(grupos)}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erro geral no processamento: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        db.close()

def processar_artigo_sem_cluster(db: Session, id_artigo: int, client) -> bool:
    """
    Processa um artigo sem fazer clusteriza√ß√£o autom√°tica.
    Usado no modo em lote.
    """
    try:
        # Busca dados brutos do artigo
        artigo = db.query(ArtigoBruto).filter(ArtigoBruto.id == id_artigo).first()
        if not artigo:
            return False
        
        # Processa o artigo sem clusteriza√ß√£o (copia a l√≥gica do processar_artigo_pipeline mas sem a ETAPA 7)
        create_log(db, "INFO", "processor", 
                  f"Iniciando processamento do artigo {id_artigo} (sem clusteriza√ß√£o)",
                  {"fonte": artigo.fonte_coleta})
        
        # ETAPA 1: Verificar se j√° tem metadados estruturados
        metadados = artigo.metadados or {}
        
        # Se j√° tem dados estruturados (JSON), usa diretamente
        if metadados.get('titulo') and metadados.get('fonte_original'):
            print(f"    üìÑ Artigo j√° estruturado, usando metadados existentes")
            
            # Migra tag antiga se necess√°rio
            tag_original = metadados.get('tag', 'Economia e Tecnologia')
            tag_migrada = migrar_tag_antiga_para_nova(tag_original)
            
            noticia_data = {
                'titulo': metadados.get('titulo', 'Sem t√≠tulo'),
                'texto_completo': artigo.texto_bruto,
                'jornal': metadados.get('fonte_original', 'Fonte desconhecida'),
                'autor': 'N/A',  # N√£o temos autor nos dados originais
                'pagina': '1',
                'data': metadados.get('data_publicacao') or get_date_brasil_str(),
                'categoria': metadados.get('categoria', 'Geral'),
                'tag': 'PENDING',  # Ser√° redefinida na ETAPA 3
                'prioridade': 'PENDING'  # Ser√° redefinida na ETAPA 3
            }
            
        else:
            # Para PDFs ou artigos sem estrutura, faz extra√ß√£o b√°sica
            print(f"    üìÑ Artigo sem estrutura, fazendo extra√ß√£o b√°sica")
            
            # Extra√ß√£o b√°sica sem LLM
            linhas = artigo.texto_bruto.split('\n')
            titulo = linhas[0].strip() if linhas else "Sem t√≠tulo"
            
            # Tenta identificar jornal/fonte dos metadados
            jornal = metadados.get('fonte_original') or 'Fonte desconhecida'
            
            noticia_data = {
                'titulo': titulo,
                'texto_completo': artigo.texto_bruto,
                'jornal': jornal,
                'autor': 'N/A',
                'pagina': '1',
                'data': metadados.get('data_publicacao') or get_date_brasil_str(),
                'categoria': metadados.get('categoria', 'Geral'),
                'tag': 'PENDING',  # Ser√° redefinida na ETAPA 3
                'prioridade': 'PENDING'  # Ser√° redefinida na ETAPA 3
            }
        
        # ETAPA 2: Migra√ß√£o e corre√ß√£o de dados
        from backend.processing import migrar_noticia_cache_legado, corrigir_tag_invalida
        noticia_data = migrar_noticia_cache_legado(noticia_data)
        
        # Corrige a tag se necess√°rio
        if 'tag' in noticia_data:
            noticia_data['tag'] = corrigir_tag_invalida(noticia_data['tag'])
        
        # ETAPA 3: Valida√ß√£o com Pydantic
        try:
            print(f"    üîç Validando dados com Pydantic...")
            
            noticia_obj = Noticia(**noticia_data)
            noticia_validada = noticia_obj.model_dump()
            print(f"    ‚úÖ Valida√ß√£o Pydantic bem-sucedida")
        except Exception as e:
            print(f"    ‚ùå Erro de valida√ß√£o Pydantic: {e}")
            create_log(db, "ERROR", "processor", 
                      f"Erro de valida√ß√£o Pydantic do artigo {id_artigo}: {e}")
            update_artigo_status(db, id_artigo, 'erro')
            return False
        
        # ETAPA 4: Gerar embedding
        from backend.processing import gerar_embedding
        texto_para_embedding = f"{noticia_validada['titulo']} {noticia_validada['texto_completo']}"
        embedding_artigo = gerar_embedding(texto_para_embedding)
        
        if not embedding_artigo:
            create_log(db, "WARNING", "processor", 
                      f"Falha ao gerar embedding do artigo {id_artigo}")
            import numpy as np
            embedding_artigo = np.zeros(384, dtype=np.float32).tobytes()
        
        # ETAPA 5: Atualizar artigo com dados processados (SEM clusteriza√ß√£o)
        dados_processados = {
            'titulo': noticia_validada['titulo'],
            'texto_completo': noticia_validada['texto_completo'],
            'jornal': noticia_validada['jornal'],
            'autor': noticia_validada['autor'],
            'pagina': noticia_validada['pagina'],
            'data': noticia_validada['data'],
            'categoria': noticia_validada['categoria'],
            'tag': noticia_validada['tag'],
            'prioridade': noticia_validada['prioridade'],
            'relevance_score': noticia_validada.get('relevance_score'),
            'relevance_reason': noticia_validada.get('relevance_reason')
        }
        
        # Atualiza dados processados e marca como "pronto_agrupar"
        update_artigo_dados_sem_status(db, id_artigo, dados_processados, embedding_artigo)
        
        # Marca como pronto para agrupamento (status mais curto)
        update_artigo_status(db, id_artigo, "pronto_agrupar")
        
        # N√ÉO faz clusteriza√ß√£o aqui - ser√° feita na ETAPA 2
        
        create_log(db, "INFO", "processor", 
                  f"Artigo {id_artigo} pronto para agrupamento")
        return True
        
    except Exception as e:
        print(f"    ‚ùå Erro geral no processamento: {e}")
        import traceback
        traceback.print_exc()
        create_log(db, "ERROR", "processor", 
                  f"Erro geral no processamento do artigo {id_artigo}: {e}")
        update_artigo_status(db, id_artigo, 'erro')
        return False

def agrupar_noticias_por_similaridade(db: Session, artigos_processados: List[ArtigoBruto]) -> List[List[ArtigoBruto]]:
    """
    Agrupa not√≠cias por similaridade usando embeddings.
    Usado no modo em lote.
    """
    if not artigos_processados:
        return []
    
    print(f"    üîó Agrupando {len(artigos_processados)} artigos por similaridade...")
    
    grupos = []
    artigos_visitados = set()
    
    for i, artigo in enumerate(artigos_processados):
        if artigo.id in artigos_visitados:
            continue
        
        # Cria novo grupo
        grupo = [artigo]
        artigos_visitados.add(artigo.id)
        
        # Busca artigos similares
        if artigo.embedding:
            embedding_artigo = bytes_to_embedding(artigo.embedding)
            
            for j, outro_artigo in enumerate(artigos_processados):
                if (outro_artigo.id not in artigos_visitados and 
                    outro_artigo.embedding and
                    outro_artigo.tag == artigo.tag):  # Mesma tag
                    
                    embedding_outro = bytes_to_embedding(outro_artigo.embedding)
                    similaridade = calcular_similaridade_cosseno(embedding_artigo, embedding_outro)
                    
                    if similaridade > 0.7:  # Threshold de similaridade
                        grupo.append(outro_artigo)
                        artigos_visitados.add(outro_artigo.id)
        
        grupos.append(grupo)
    
    print(f"    ‚úÖ Criados {len(grupos)} grupos de not√≠cias")
    return grupos

def main():
    """Fun√ß√£o principal"""
    print("=" * 60)
    print("üîÑ BTG AlphaFeed - Orquestrador (Fluxo de Neg√≥cio Correto)")
    print("=" * 60)
    
    # Configura√ß√£o
    limite = 999  # Limite para desenvolvimento
    modo_incremental = True  # True = incremental, False = em lote
    
    print(f"üìä Limite de artigos: {limite}")
    print(f"üéØ Modo: {'Incremental' if modo_incremental else 'Em Lote'}")
    
    # Verifica configura√ß√£o inicial
    print(f"üîß GEMINI_API_KEY configurada: {'Sim' if os.getenv('GEMINI_API_KEY') else 'N√£o'}")
    print(f"üîß DATABASE_URL configurada: {'Sim' if os.getenv('DATABASE_URL') else 'N√£o'}")
    
    # Executa processamento
    if modo_incremental:
        sucesso = processar_artigos_pendentes(limite)
    else:
        sucesso = processar_artigos_em_lote(limite)
    
    if sucesso:
        print("\nüéâ Processamento completo conclu√≠do com sucesso!")
        print("üí° Verifique o frontend para ver os clusters e resumos gerados")
    else:
        print("\n‚ùå Processamento falhou")
    
    print("=" * 60)

if __name__ == "__main__":
    main() 