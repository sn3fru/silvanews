#!/usr/bin/env python3
"""
Orquestrador para processar artigos pendentes seguindo a lÃ³gica do AlphaFeed.
Implementa o fluxo de negÃ³cio correto: 
1. Processar todas as notÃ­cias (extrair dados raw)
2. Criar clusters/agrupamentos por fato gerador
3. Classificar prioridade e gerar resumos seletivos
"""

import os
import sys
import json
import re
import time
from datetime import datetime, date
from pathlib import Path
from typing import Dict, Any, List, Optional

# Adiciona o diretÃ³rio backend ao path
backend_dir = Path(__file__).parent / "backend"
sys.path.insert(0, str(backend_dir))

# ConfiguraÃ§Ã£o SSL para desenvolvimento
import ssl
ssl._create_default_https_context = ssl._create_unverified_context

# Imports do backend
try:
    from dotenv import load_dotenv
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
    print("âœ… Google Gemini disponÃ­vel")
except ImportError:
    GEMINI_AVAILABLE = False
    print("âŒ AVISO: Google Gemini nÃ£o estÃ¡ disponÃ­vel.")
from sqlalchemy.orm import Session
from sqlalchemy import func

from backend.database import SessionLocal, ArtigoBruto, ClusterEvento
from backend.crud import (
    get_artigos_pendentes, create_log, update_artigo_status, 
    update_artigo_processado, update_artigo_dados_sem_status, associate_artigo_to_cluster,
    create_cluster, get_active_clusters_today, get_artigos_by_cluster,
    get_cluster_by_id, update_cluster_priority, update_cluster_tags
)
from backend.models import Noticia, NoticiaResumida, ResumoFinal
from backend.processing import (
    gerar_embedding, bytes_to_embedding, calcular_similaridade_cosseno,
    processar_artigo_pipeline, gerar_resumo_cluster, find_or_create_cluster
)
from backend.prompts import PROMPT_AGRUPAMENTO_V1, PROMPT_RESUMO_FINAL_V3, PROMPT_PRIORIZACAO_EXECUTIVA_V1
from backend.utils import get_date_brasil_str, get_datetime_brasil_str

# Carrega variÃ¡veis de ambiente
env_file = backend_dir / ".env"
load_dotenv(env_file)
print(f"SUCESSO: Arquivo .env carregado: {env_file}")

# ConfiguraÃ§Ã£o de lotes para evitar truncamento
BATCH_SIZE_AGRUPAMENTO = 150  # Lote maior para aumentar consolidaÃ§Ã£o por fato

# ConfiguraÃ§Ã£o do Gemini
api_key = os.getenv("GEMINI_API_KEY")
if not api_key:
    print("ERRO: GEMINI_API_KEY nÃ£o configurada")
    sys.exit(1)

genai.configure(api_key=api_key)
client = genai.GenerativeModel('gemini-2.0-flash')
print("SUCESSO: Gemini configurado com sucesso!")

def extrair_json_da_resposta(resposta: str) -> Any:
    """
    Extrai e decodifica um objeto JSON de uma string de resposta do LLM,
    com um fluxo de tentativas robusto e simplificado.
    """
    import json
    import re

    if not isinstance(resposta, str) or not resposta.strip():
        print("âŒ ERRO: Resposta da API estÃ¡ vazia.")
        return None

    print(f"ğŸ” Processando resposta de {len(resposta)} caracteres...")
    
    json_str = ""
    # Tenta extrair de um bloco de cÃ³digo markdown primeiro, que Ã© o formato mais confiÃ¡vel.
    match = re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', resposta, re.DOTALL)
    if match:
        json_str = match.group(1).strip()
        print(f"ğŸ“‹ JSON extraÃ­do de bloco de cÃ³digo: {len(json_str)} caracteres")
    else:
        # Se nÃ£o houver bloco de cÃ³digo, busca pelo inÃ­cio de um objeto ou array.
        start_pos = resposta.find('[')
        if start_pos == -1:
            start_pos = resposta.find('{')
        
        if start_pos != -1:
            json_str = resposta[start_pos:].strip()
            print(f"ğŸ“‹ JSON extraÃ­do por marcador de inÃ­cio: {len(json_str)} caracteres")
        else:
            print("âŒ ERRO: Nenhum marcador de inÃ­cio de JSON ('[' ou '{') encontrado na resposta.")
            return None

    # --- FLUXO DE TENTATIVAS DE PARSING ---

    # TENTATIVA 1: Parse Direto
    try:
        return json.loads(json_str)
    except json.JSONDecodeError as e:
        print(f"âš ï¸ Falha na Tentativa 1 (Parse Direto): {e}")

    # TENTATIVA 2: Extrair a Parte VÃ¡lida (para JSONs truncados)
    json_parcial = extrair_json_valido(json_str)
    if json_parcial:
        print(f"ğŸ”§ Tentativa 2 (ExtraÃ§Ã£o Parcial) obteve um JSON de {len(json_parcial)} caracteres.")
        try:
            return json.loads(json_parcial)
        except json.JSONDecodeError as e:
            print(f"âš ï¸ Falha na Tentativa 2 mesmo apÃ³s extraÃ§Ã£o parcial: {e}")

    # TENTATIVA 3: CorreÃ§Ã£o de Strings (como Ãºltimo recurso)
    json_corrigido = corrigir_json_strings(json_str)
    if json_corrigido != json_str:
        print(f"ğŸ”§ Tentativa 3 (CorreÃ§Ã£o de Strings) alterou o JSON para {len(json_corrigido)} caracteres.")
        try:
            return json.loads(json_corrigido)
        except json.JSONDecodeError as e:
            print(f"âš ï¸ Falha na Tentativa 3 mesmo apÃ³s correÃ§Ã£o: {e}")

    print("âŒ ERRO FINAL: Todas as tentativas de extrair um JSON vÃ¡lido falharam.")
    print(f"ğŸ“‹ Primeiros 500 caracteres da resposta problemÃ¡tica: {resposta[:500]}...")
    return None

def corrigir_json_strings(json_str: str) -> str:
    """
    Realiza correÃ§Ãµes bÃ¡sicas e seguras em uma string JSON.
    O foco Ã© remover caracteres invÃ¡lidos e garantir que o escape de aspas
    seja consistente, sem tentar adivinhar o conteÃºdo de strings truncadas.
    """
    import re
    
    # 1. Remove caracteres de controle ASCII, exceto os comuns como \n, \r, \t.
    # Isso limpa "lixo" invisÃ­vel que quebra o parser.
    json_corrigido = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f]', '', json_str)
    
    # 2. Garante que barras invertidas que nÃ£o sÃ£o parte de uma sequÃªncia de escape vÃ¡lida
    # sejam escapadas. Ex: "caminho\no_arquivo" -> "caminho\\no_arquivo"
    # Isso evita erros de "invalid escape sequence".
    # A expressÃ£o (?!["\\/bfnrtu]) Ã© um "negative lookahead", garantindo que nÃ£o vamos
    # escapar barras que jÃ¡ fazem parte de uma sequÃªncia vÃ¡lida.
    json_corrigido = re.sub(r'\\(?!["\\/bfnrtu])', r'\\\\', json_corrigido)
    
    return json_corrigido

def extrair_json_valido(json_str: str) -> str:
    """
    Tenta extrair a maior porÃ§Ã£o inicial de uma string JSON que seja vÃ¡lida.
    Funciona encontrando o Ãºltimo ponto de terminaÃ§Ã£o de um objeto JSON completo
    antes do ponto de truncamento.
    """
    # Procura pelo final do Ãºltimo objeto completo em uma lista.
    # O padrÃ£o Ã©: uma chave fechando, seguida opcionalmente por espaÃ§os, e uma vÃ­rgula.
    # Exemplo: ... {"id": 123}, {"id": 456} <-- queremos parar aqui
    last_good_char_pos = json_str.rfind('},')
    
    if last_good_char_pos != -1:
        # Encontramos um objeto intermediÃ¡rio. Pegamos tudo atÃ© ele e fechamos o array.
        # Adicionamos 1 para incluir a chave '}' na fatia.
        partial_json = json_str[:last_good_char_pos + 1]
        
        # Se o JSON original comeÃ§ava com '[', nÃ³s fechamos o array com ']'
        if partial_json.strip().startswith('['):
             return partial_json.strip() + ']'
        # Se era um objeto Ãºnico, apenas retornamos a parte vÃ¡lida
        return partial_json

    # Se nÃ£o encontrou '},', pode ser um JSON com um Ãºnico objeto ou jÃ¡ vÃ¡lido.
    # Como fallback, retorna None, pois a extraÃ§Ã£o falhou em encontrar um ponto de corte seguro.
    return None

def processar_artigos_pendentes(limite: int = 10) -> bool:
    """
    Processa artigos pendentes seguindo o fluxo de negÃ³cio correto:
    1. Processa TODAS as notÃ­cias pendentes (extraÃ§Ã£o de dados)
    2. Cria clusters/agrupamentos por fato gerador usando prompt de agrupamento
    3. Classifica prioridade e gera resumos seletivos
    """
    db = SessionLocal()
    try:
        # Busca artigos pendentes
        artigos_pendentes = get_artigos_pendentes(db, limite=limite)
        
        if not artigos_pendentes:
            print("SUCESSO: Nenhum artigo pendente encontrado")
            return True
        
        print(f"ARTIGOS: Encontrados {len(artigos_pendentes)} artigos pendentes")
        
        # EstatÃ­sticas do banco
        total_artigos = db.query(ArtigoBruto).count()
        artigos_processados = db.query(ArtigoBruto).filter(ArtigoBruto.status == "processado").count()
        artigos_erro = db.query(ArtigoBruto).filter(ArtigoBruto.status == "erro").count()
        clusters_existentes = db.query(ClusterEvento).count()
        
        print(f"ESTATISTICAS: Total artigos: {total_artigos}, Processados: {artigos_processados}, Erros: {artigos_erro}, Clusters: {clusters_existentes}")
        
        # ETAPA 1: Processar TODAS as notÃ­cias pendentes
        print(f"\nETAPA 1: Processando {len(artigos_pendentes)} artigos pendentes...")
        
        sucessos = 0
        erros = 0
        
        for i, artigo in enumerate(artigos_pendentes, 1):
            print(f"  PROCESSANDO: Artigo {i}/{len(artigos_pendentes)} (ID: {artigo.id})...")
            
            # Usa a funÃ§Ã£o do backend para processar cada artigo (SEM clusterizaÃ§Ã£o)
            if processar_artigo_sem_cluster(db, artigo.id, client):
                sucessos += 1
                print(f"    SUCESSO: Artigo {artigo.id} processado")
            else:
                erros += 1
                print(f"    ERRO: Falha no processamento do artigo {artigo.id}")
            
            time.sleep(0.1)  # Pausa leve entre processamentos (throttle)
        
        print(f"ETAPA 1 CONCLUIDA: Sucessos: {sucessos}, Erros: {erros}")
        
        # ETAPA 2: Agrupamento inteligente com pivot automÃ¡tico
        print(f"\nETAPA 2: Agrupamento inteligente com pivot automÃ¡tico...")
        
        # Verifica se hÃ¡ clusters existentes hoje para decidir o modo
        hoje = get_date_brasil_str()
        clusters_existentes = db.query(ClusterEvento).filter(
            func.date(ClusterEvento.created_at) == hoje,
            ClusterEvento.status == 'ativo'
        ).count()
        
        if clusters_existentes > 0:
            # Modo incremental: anexa a clusters existentes
            print(f"ğŸ¯ MODO INCREMENTAL: {clusters_existentes} clusters existentes encontrados")
            sucesso_agrupamento = agrupar_noticias_incremental(db, client)
        else:
            # Modo em lote: cria clusters do zero
            print("ğŸ¯ MODO EM LOTE: Nenhum cluster existente, criando do zero")
            sucesso_agrupamento = agrupar_noticias_com_prompt(db, client)
        
        if sucesso_agrupamento:
            print("ETAPA 2 CONCLUIDA: Agrupamento realizado com sucesso")
        else:
            print("ETAPA 2 FALHOU: Erro no agrupamento")
            return False
        
        # ETAPA 3: Classificar e gerar resumos usando prompts
        print(f"\nETAPA 3: Classificando clusters e gerando resumos...")
        
        # Busca apenas clusters ativos hoje que NÃƒO tÃªm resumo (novos)
        hoje = get_date_brasil_str()
        clusters_hoje = db.query(ClusterEvento).filter(
            ClusterEvento.status == 'ativo',
            ClusterEvento.created_at >= hoje,
            ClusterEvento.resumo_cluster.is_(None)  # Apenas clusters sem resumo
        ).all()
        
        resumos_gerados = 0
        
        for cluster in clusters_hoje:
            print(f"  ğŸ“ Processando cluster {cluster.id}: '{cluster.titulo_cluster}'")
            
            # Classifica o cluster usando prompts do prompts.py
            if classificar_e_resumir_cluster(db, cluster.id, client):
                resumos_gerados += 1
                print(f"    âœ… Cluster {cluster.id} - Classificado e resumido")
            else:
                print(f"    âŒ Cluster {cluster.id} - Falha na classificaÃ§Ã£o")
        
        print(f"ETAPA 3 CONCLUIDA: Resumos gerados: {resumos_gerados}")

        # ETAPA 4: PriorizaÃ§Ã£o Executiva Final (reclassificaÃ§Ã£o rÃ­gida)
        print(f"\nETAPA 4: PriorizaÃ§Ã£o Executiva Final...")
        if priorizacao_executiva_final(db, client):
            print("ETAPA 4 CONCLUIDA: PriorizaÃ§Ã£o executiva aplicada")
        else:
            print("ETAPA 4 FALHOU: Erro na priorizaÃ§Ã£o executiva")
        
        # Resumo final
        print(f"\nPROCESSAMENTO CONCLUIDO:")
        print(f"  Artigos processados: {sucessos}")
        print(f"  Resumos gerados: {resumos_gerados}")
        print(f"  PriorizaÃ§Ã£o executiva: concluÃ­da")
        
        return True
        
    except Exception as e:
        print(f"ERRO: Erro geral no processamento: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        db.close()

def mapear_tag_prompt_para_modelo(tag_prompt: str) -> str:
    """
    Mapeia as tags do prompt para as tags vÃ¡lidas do modelo.
    Agora as tags sÃ£o as mesmas, entÃ£o sÃ³ retorna a tag original.
    """
    # As tags do prompt jÃ¡ sÃ£o as mesmas do modelo
    return tag_prompt

def migrar_tag_antiga_para_nova(tag_antiga: str) -> str:
    """
    Migra tags antigas para as novas tags do TAGS_SPECIAL_SITUATIONS.
    """
    mapeamento_antigo = {
        'Economia e Tecnologia': 'Internacional (Economia e PolÃ­tica)',
        'Governo e Politica': 'PolÃ­tica EconÃ´mica (Brasil)',
        'Judicionario': 'JurÃ­dico, FalÃªncias e RegulatÃ³rio',
        'Empresas Privadas': 'Mercado de Capitais e FinanÃ§as Corporativas'
    }
    
    return mapeamento_antigo.get(tag_antiga, 'Internacional (Economia e PolÃ­tica)')

def marcar_cluster_irrelevante(db: Session, cluster_id: int, debug: bool = True) -> bool:
    """
    Marca um cluster como irrelevante quando o prompt retorna lista vazia.
    """
    try:
        cluster = get_cluster_by_id(db, cluster_id)
        if not cluster:
            return False
        
        # Marca como irrelevante
        cluster.prioridade = "IRRELEVANTE"
        cluster.tag = "IRRELEVANTE"
        cluster.resumo_cluster = "NotÃ­cia irrelevante para a mesa de Special Situations"
        
        db.commit()
        
        if debug:
            print(f"    ğŸš« DEBUG: Cluster {cluster_id} marcado como IRRELEVANTE")
        
        return True
        
    except Exception as e:
        print(f"âŒ ERRO: Falha ao marcar cluster {cluster_id} como irrelevante: {e}")
        return False

def classificar_e_resumir_cluster(db: Session, cluster_id: int, client, debug: bool = True) -> bool:
    """
    Classifica e resume um cluster usando os prompts do prompts.py.
    Esta funÃ§Ã£o usa o PROMPT_EXTRACAO_PERMISSIVO_V8 para classificar o cluster
    e depois gera o resumo apropriado baseado na prioridade.
    """
    try:
        cluster = get_cluster_by_id(db, cluster_id)
        if not cluster:
            if debug:
                print(f"    âŒ DEBUG: Cluster {cluster_id} nÃ£o encontrado")
            return False
        
        artigos = get_artigos_by_cluster(db, cluster_id)
        if not artigos:
            if debug:
                print(f"    âŒ DEBUG: Nenhum artigo encontrado para cluster {cluster_id}")
            return False
        
        if debug:
            print(f"    ğŸ” DEBUG: Cluster {cluster_id} tem {len(artigos)} artigos")
        
        # Coleta todos os textos dos artigos para anÃ¡lise
        textos = []
        for i, artigo in enumerate(artigos):
            if artigo.texto_processado:
                texto_artigo = f"FONTE: {artigo.jornal or 'Desconhecida'}\n{artigo.texto_processado}"
                textos.append(texto_artigo)
                if debug:
                    print(f"    ğŸ“„ DEBUG: Artigo {i+1}: {artigo.titulo_extraido or 'Sem tÃ­tulo'}")
                    print(f"    ğŸ“„ DEBUG: Texto: {artigo.texto_processado[:100]}...")
        
        texto_completo = "\n\n".join(textos)
        
        if debug:
            print(f"    ğŸ“ DEBUG: Texto completo para anÃ¡lise ({len(texto_completo)} chars):")
            print(f"    {'='*50}")
            print(texto_completo[:500] + "..." if len(texto_completo) > 500 else texto_completo)
            print(f"    {'='*50}")
        
        # Usa o prompt de extraÃ§Ã£o para classificar o cluster
        from backend.prompts import PROMPT_EXTRACAO_PERMISSIVO_V8
        
        prompt_classificacao = f"""
        {PROMPT_EXTRACAO_PERMISSIVO_V8}
        
        NOTÃCIA PARA ANÃLISE:
        {texto_completo}
        
        Analise esta notÃ­cia e retorne a classificaÃ§Ã£o conforme o guia acima.
        """
        
        if debug:
            print(f"    ğŸ¤– DEBUG: Enviando prompt para Gemini...")
            print(f"    ğŸ¤– DEBUG: Tamanho do prompt: {len(prompt_classificacao)} chars")
            print(f"    ğŸ¤– DEBUG: Primeiros 300 chars do prompt:")
            print(f"    {'='*50}")
            print(prompt_classificacao[:300] + "...")
            print(f"    {'='*50}")
        
        response = client.generate_content(
            prompt_classificacao,
            generation_config={
                'temperature': 0.3,
                'top_p': 0.9,
                'max_output_tokens': 1024
            }
        )
        
        if not response.text:
            if debug:
                print(f"    âŒ DEBUG: API retornou resposta vazia para cluster {cluster_id}")
            return False
        
        if debug:
            print(f"    ğŸ¤– DEBUG: Resposta do Gemini ({len(response.text)} chars):")
            print(f"    {'='*50}")
            print(response.text)
            print(f"    {'='*50}")
        
        # Extrai JSON da resposta
        resultado = extrair_json_da_resposta(response.text)
        
        if debug:
            print(f"    ğŸ” DEBUG: Resultado extraÃ­do: {resultado}")
        
        # PRIMEIRO, verifica se a resposta Ã© uma lista vazia, que significa "irrelevante"
        if isinstance(resultado, list) and len(resultado) == 0:
            if debug:
                print(f"    ğŸš« DEBUG: NotÃ­cia irrelevante detectada (API retornou lista vazia)")
            return marcar_cluster_irrelevante(db, cluster_id, debug)
        
        # DEPOIS, continua com a validaÃ§Ã£o original para outros tipos de erro
        if not resultado or not isinstance(resultado, list):
            if debug:
                print(f"    âŒ DEBUG: Resposta invÃ¡lida para cluster {cluster_id}")
                print(f"    âŒ DEBUG: Tipo do resultado: {type(resultado)}")
                print(f"    âŒ DEBUG: ConteÃºdo: {resultado}")
            return False
        
        # Pega o primeiro resultado (deveria ser sÃ³ um)
        classificacao = resultado[0]
        
        if debug:
            print(f"    âœ… DEBUG: ClassificaÃ§Ã£o extraÃ­da: {classificacao}")
        
        # Atualiza o cluster com a classificaÃ§Ã£o
        prioridade_original = cluster.prioridade
        tag_original = cluster.tag
        
        cluster.prioridade = classificacao.get('prioridade', 'P3_MONITORAMENTO')
        cluster.tag = mapear_tag_prompt_para_modelo(classificacao.get('tag', 'Sem categoria'))
        
        if debug:
            print(f"    ğŸ”„ DEBUG: Prioridade: {prioridade_original} â†’ {cluster.prioridade}")
            print(f"    ğŸ”„ DEBUG: Tag: {tag_original} â†’ {cluster.tag}")
        
        # Gera resumo baseado na prioridade usando funÃ§Ã£o unificada
        prioridade = cluster.prioridade
        
        # Mapeia prioridade para nÃ­vel de detalhe
        mapa_niveis = {
            'P1_CRITICO': 'Executivo (P1_CRITICO)',
            'P2_ESTRATEGICO': 'PadrÃ£o (P2_ESTRATEGICO)',
            'P3_MONITORAMENTO': 'Conciso (P3_MONITORAMENTO)'
        }
        
        nivel_detalhe = mapa_niveis.get(prioridade)
        if nivel_detalhe:
            if debug:
                print(f"    ğŸ“ DEBUG: Gerando resumo {prioridade} com nÃ­vel {nivel_detalhe}...")
            
            if gerar_resumo_unificado(db, cluster_id, client, nivel_detalhe):
                print(f"    ğŸ“ {prioridade}: Resumo gerado com sucesso")
            else:
                print(f"    âŒ Falha ao gerar resumo {prioridade}")
                return False
        else:
            if debug:
                print(f"    âš ï¸ DEBUG: Prioridade {prioridade} nÃ£o mapeada para nÃ­vel de detalhe")
        
        # Salva as mudanÃ§as
        db.commit()
        
        if debug:
            print(f"    âœ… DEBUG: Cluster {cluster_id} salvo com sucesso")
        
        return True
        
    except Exception as e:
        print(f"âŒ ERRO: Falha ao classificar e resumir cluster {cluster_id}: {e}")
        if debug:
            import traceback
            traceback.print_exc()
        return False

def gerar_resumo_unificado(db: Session, cluster_id: int, client, nivel_detalhe: str) -> bool:
    """
    Gera um resumo para um cluster com um nÃ­vel de detalhe variÃ¡vel.
    Usa o PROMPT_RESUMO_FINAL_V3 para consistÃªncia.
    """
    try:
        cluster = get_cluster_by_id(db, cluster_id)
        if not cluster:
            return False
        
        artigos = get_artigos_by_cluster(db, cluster_id)
        if not artigos:
            return False
        
        # Coleta todos os textos dos artigos
        textos = []
        for artigo in artigos:
            if artigo.texto_processado:
                textos.append(f"FONTE: {artigo.jornal or 'Desconhecida'}\n{artigo.texto_processado}")
        
        texto_completo = "\n\n".join(textos)
        
        # Prepara dados do cluster para o prompt
        dados_do_grupo = {
            "tema_principal": cluster.titulo_cluster,
            "categoria": cluster.tag,
            "prioridade": cluster.prioridade,
            "noticias": [
                {
                    "titulo": artigo.titulo_extraido or "Sem tÃ­tulo",
                    "texto": artigo.texto_processado or "",
                    "jornal": artigo.jornal or "Fonte desconhecida"
                }
                for artigo in artigos
            ]
        }
        
        # Usa o PROMPT_RESUMO_FINAL_V3 importado de prompts.py
        prompt_completo = PROMPT_RESUMO_FINAL_V3.format(
            NIVEL_DE_DETALHE=nivel_detalhe,
            DADOS_DO_GRUPO=json.dumps(dados_do_grupo, indent=2, ensure_ascii=False)
        )
        
        response = client.generate_content(
            prompt_completo,
            generation_config={
                'temperature': 0.3,
                'top_p': 0.9,
                'max_output_tokens': 2048 if nivel_detalhe == 'Executivo (P1_CRITICO)' else 512
            }
        )
        
        if not response.text:
            return False
        
        # Extrai JSON da resposta
        resultado_json = extrair_json_da_resposta(response.text)
        
        if resultado_json and 'resumo_final' in resultado_json:
            # Remove o tÃ­tulo "Resumo Executivo:" se ele aparecer
            resumo_limpo = resultado_json['resumo_final']
            if ': ' in resumo_limpo:
                resumo_limpo = resumo_limpo.split(': ', 1)[1]
            
            cluster.resumo_cluster = resumo_limpo
            db.commit()
            return True
        
        return False
        
    except Exception as e:
        print(f"ERRO: Falha ao gerar resumo unificado para cluster {cluster_id}: {e}")
        return False


def priorizacao_executiva_final(db: Session, client, debug: bool = True) -> bool:
    """
    Aplica a priorizaÃ§Ã£o executiva (pÃ³s-agrupamento e pÃ³s-resumo) usando
    PROMPT_PRIORIZACAO_EXECUTIVA_V1 sobre os clusters ativos do dia.
    Reclassifica prioridade, ajusta score (se armazenado futuramente) e registra alteraÃ§Ãµes.
    """
    try:
        hoje = get_date_brasil_str()
        clusters_hoje = db.query(ClusterEvento).filter(
            ClusterEvento.status == 'ativo',
            ClusterEvento.created_at >= hoje
        ).all()

        if not clusters_hoje:
            if debug:
                print("â„¹ï¸ PriorizaÃ§Ã£o executiva: nenhum cluster ativo hoje")
            return True

        itens_finais = []
        id_map = {}
        for idx, c in enumerate(clusters_hoje):
            itens_finais.append({
                "id": idx,
                "cluster_id": c.id,
                "titulo_final": c.titulo_cluster,
                "prioridade_atribuida_inicial": c.prioridade,
                "tag_atribuida_inicial": c.tag,
                "score_inicial": None,
                "resumo_final": (c.resumo_cluster or "")[:1200]
            })
            id_map[idx] = c.id

        prompt = PROMPT_PRIORIZACAO_EXECUTIVA_V1.format(
            ITENS_FINAIS=json.dumps(itens_finais, indent=2, ensure_ascii=False)
        )

        if debug:
            print(f"ğŸ§® DEBUG: Enviando priorizaÃ§Ã£o executiva para {len(itens_finais)} itens...")

        response = client.generate_content(
            prompt,
            generation_config={
                'temperature': 0.1,
                'top_p': 0.8,
                'max_output_tokens': 2048
            }
        )

        if not response.text:
            if debug:
                print("âŒ PriorizaÃ§Ã£o executiva: resposta vazia")
            return False

        resultado = extrair_json_da_resposta(response.text)
        if not isinstance(resultado, list):
            if debug:
                print("âŒ PriorizaÃ§Ã£o executiva: formato invÃ¡lido")
            return False

        alteracoes = 0
        for item in resultado:
            try:
                rid = item.get("id")
                decisao = item.get("decisao_prioridade_final")
                tag = item.get("tag_final") or item.get("tag_atribuida_inicial")
                justificativa = item.get("justificativa_executiva")

                if rid is None or id_map.get(rid) is None:
                    continue
                cluster_id = id_map[rid]
                cluster = get_cluster_by_id(db, cluster_id)
                if not cluster:
                    continue

                # Atualiza prioridade se diferente
                if decisao and decisao != cluster.prioridade:
                    update_cluster_priority(db, cluster_id, decisao, motivo=justificativa or "priorizaÃ§Ã£o executiva")
                    alteracoes += 1

                # Atualiza tag se fornecida e vÃ¡lida
                if tag and tag != cluster.tag:
                    update_cluster_tags(db, cluster_id, [tag], motivo="priorizaÃ§Ã£o executiva")
                    alteracoes += 1
            except Exception:
                continue

        if debug:
            print(f"âœ… PriorizaÃ§Ã£o executiva concluÃ­da. AlteraÃ§Ãµes aplicadas: {alteracoes}")
        return True

    except Exception as e:
        print(f"âŒ ERRO: PriorizaÃ§Ã£o executiva falhou: {e}")
        return False

def agrupar_noticias_incremental(db: Session, client) -> bool:
    """
    Agrupamento incremental: anexa novas notÃ­cias a clusters existentes ou cria novos clusters.
    Usa o prompt PROMPT_AGRUPAMENTO_INCREMENTAL_V1 para decisÃ£o inteligente.
    Processa em lotes se houver muitas notÃ­cias para evitar truncamento.
    """
    try:
        # Busca artigos prontos para agrupamento que nÃ£o foram associados a clusters
        hoje = get_date_brasil_str()
        artigos_novos = db.query(ArtigoBruto).filter(
            ArtigoBruto.status == "pronto_agrupar",
            ArtigoBruto.cluster_id.is_(None)  # Artigos nÃ£o associados a clusters
        ).all()
        
        if not artigos_novos:
            print("INFO: Nenhum artigo novo encontrado para agrupamento incremental")
            return True
        
        # Busca clusters existentes criados hoje
        clusters_existentes = db.query(ClusterEvento).filter(
            func.date(ClusterEvento.created_at) == hoje,
            ClusterEvento.status == 'ativo'
        ).all()
        
        print(f"ğŸ”— AGRUPAMENTO INCREMENTAL: {len(artigos_novos)} artigos novos, {len(clusters_existentes)} clusters existentes")
        
        # Determina se precisa processar em lotes
        TAMANHO_LOTE_MAXIMO = 150  # MÃ¡ximo de notÃ­cias por lote (incremental mais abrangente)
        processar_em_lotes = len(artigos_novos) > TAMANHO_LOTE_MAXIMO
        
        if processar_em_lotes:
            print(f"ğŸ“¦ PROCESSAMENTO EM LOTES: {len(artigos_novos)} notÃ­cias divididas em lotes de {TAMANHO_LOTE_MAXIMO}")
            
            # Divide artigos em lotes
            lotes = [artigos_novos[i:i + TAMANHO_LOTE_MAXIMO] for i in range(0, len(artigos_novos), TAMANHO_LOTE_MAXIMO)]
            
            total_anexacoes = 0
            total_novos_clusters = 0
            
            for i, lote in enumerate(lotes, 1):
                print(f"\nğŸ“¦ PROCESSANDO LOTE {i}/{len(lotes)} ({len(lote)} notÃ­cias)...")
                
                sucesso_lote = processar_lote_incremental(db, client, lote, clusters_existentes, i)
                
                if sucesso_lote:
                    anexacoes, novos_clusters = sucesso_lote
                    total_anexacoes += anexacoes
                    total_novos_clusters += novos_clusters
                    print(f"âœ… LOTE {i} CONCLUIDO: {anexacoes} anexaÃ§Ãµes, {novos_clusters} novos clusters")
                else:
                    print(f"âŒ LOTE {i} FALHOU")
                    return False
            
            print(f"ğŸ‰ AGRUPAMENTO INCREMENTAL CONCLUIDO: {total_anexacoes} anexaÃ§Ãµes, {total_novos_clusters} novos clusters")
            
            # Marca artigos como "processado" apÃ³s clusterizaÃ§Ã£o
            marcar_artigos_processados(db, artigos_novos)
            
            return True
        else:
            # Processa tudo de uma vez (caso original)
            resultado = processar_lote_incremental(db, client, artigos_novos, clusters_existentes, 1)
            if resultado:
                # Marca artigos como "processado" apÃ³s clusterizaÃ§Ã£o
                marcar_artigos_processados(db, artigos_novos)
            return resultado
        
    except Exception as e:
        print(f"âŒ ERRO: Falha no agrupamento incremental: {e}")
        import traceback
        traceback.print_exc()
        return False

def marcar_artigos_processados(db: Session, artigos: List[ArtigoBruto]) -> None:
    """
    Marca artigos como "processado" apÃ³s clusterizaÃ§Ã£o bem-sucedida.
    """
    try:
        for artigo in artigos:
            artigo.status = "processado"
            artigo.processed_at = get_datetime_brasil_str()
        
        db.commit()
        print(f"âœ… {len(artigos)} artigos marcados como 'processado' apÃ³s clusterizaÃ§Ã£o")
        
    except Exception as e:
        print(f"âŒ ERRO: Falha ao marcar artigos como processado: {e}")
        db.rollback()

def processar_lote_incremental(db: Session, client, artigos_lote: List[ArtigoBruto], clusters_existentes: List[ClusterEvento], numero_lote: int = 1) -> tuple:
    """
    Processa um lote de artigos no agrupamento incremental.
    Retorna (anexacoes, novos_clusters) ou False se falhar.
    """
    try:
        # Prepara dados para o prompt incremental
        novas_noticias = []
        for i, artigo in enumerate(artigos_lote):
            noticia_data = {
                "id": i,
                "titulo": artigo.titulo_extraido or "Sem tÃ­tulo"
            }
            novas_noticias.append(noticia_data)
        
        clusters_existentes_data = []
        for i, cluster in enumerate(clusters_existentes):
            artigos_cluster = get_artigos_by_cluster(db, cluster.id)
            titulos = [
                a.titulo_extraido or (a.texto_processado[:80] + "...") if (a.texto_processado or "") else "Sem tÃ­tulo"
                for a in artigos_cluster
            ]
            titulos = titulos[:30]
            cluster_data = {
                "cluster_id": cluster.id,
                "tema_principal": cluster.titulo_cluster,
                "titulos_internos": titulos
            }
            clusters_existentes_data.append(cluster_data)
        
        # Mapeamento de ID para artigo original
        mapa_id_para_artigo = {i: artigo for i, artigo in enumerate(artigos_lote)}
        
        # Monta o prompt incremental
        from backend.prompts import PROMPT_AGRUPAMENTO_INCREMENTAL_V2
        prompt_completo = PROMPT_AGRUPAMENTO_INCREMENTAL_V2.format(
            NOVAS_NOTICIAS=json.dumps(novas_noticias, indent=2, ensure_ascii=False),
            CLUSTERS_EXISTENTES=json.dumps(clusters_existentes_data, indent=2, ensure_ascii=False)
        )
        
        print(f"ğŸ“¤ ENVIANDO LOTE {numero_lote}: {len(novas_noticias)} notÃ­cias para anÃ¡lise incremental...")
        
        # Chama a API para anÃ¡lise incremental
        try:
            response = client.generate_content(
                prompt_completo,
                generation_config={
                    'temperature': 0.1,  # Mais determinÃ­stico
                    'top_p': 0.8,
                    'max_output_tokens': 8192,  # Aumentado para lotes maiores
                    'candidate_count': 1
                }
            )
            
            if not response.text:
                print("âŒ ERRO: API retornou resposta vazia para agrupamento incremental")
                return False
            
            print(f"ğŸ“¥ RESPOSTA RECEBIDA LOTE {numero_lote}: {len(response.text)} caracteres")
            
            # Extrai JSON da resposta
            classificacoes = extrair_json_da_resposta(response.text)
            
            if not classificacoes or not isinstance(classificacoes, list):
                print("âŒ ERRO: Resposta de agrupamento incremental invÃ¡lida")
                print(f"ğŸ“‹ Resposta recebida: {response.text[:500]}...")
                return False
            
            print(f"âœ… SUCESSO LOTE {numero_lote}: {len(classificacoes)} classificaÃ§Ãµes recebidas")
            
            # Processa cada classificaÃ§Ã£o
            anexacoes = 0
            novos_clusters = 0
            
            for classificacao in classificacoes:
                try:
                    tipo = classificacao.get("tipo")
                    noticia_id = classificacao.get("noticia_id")
                    
                    if noticia_id not in mapa_id_para_artigo:
                        print(f"  âš ï¸ ID de notÃ­cia invÃ¡lido: {noticia_id}")
                        continue
                    
                    artigo = mapa_id_para_artigo[noticia_id]
                    
                    if tipo == "anexar":
                        # Anexa a cluster existente
                        cluster_id_existente = classificacao.get("cluster_id_existente")
                        cluster_existente = next((c for c in clusters_existentes if c.id == cluster_id_existente), None)
                        
                        if cluster_existente:
                            associate_artigo_to_cluster(db, artigo.id, cluster_existente.id)
                            anexacoes += 1
                            print(f"  âœ… Anexado: '{artigo.titulo_extraido}' â†’ Cluster {cluster_existente.id}")
                        else:
                            print(f"  âŒ Cluster {cluster_id_existente} nÃ£o encontrado")
                    
                    elif tipo == "novo_cluster":
                        # Cria novo cluster
                        tema_principal = classificacao.get("tema_principal", f"Novo Cluster - {artigo.titulo_extraido}")
                        
                        # Calcula embedding do artigo
                        embedding_medio = None
                        if artigo.embedding:
                            embedding_medio = artigo.embedding
                        
                        # Cria cluster
                        from backend.models import ClusterEventoCreate
                        cluster_data = ClusterEventoCreate(
                            titulo_cluster=tema_principal,
                            resumo_cluster=None,  # SerÃ¡ preenchido na ETAPA 3
                            tag="Internacional (Economia e PolÃ­tica)",  # PadrÃ£o, serÃ¡ redefinido na ETAPA 3
                            prioridade="P3_MONITORAMENTO",  # PadrÃ£o, serÃ¡ redefinido na ETAPA 3
                            embedding_medio=embedding_medio
                        )
                        
                        cluster = create_cluster(db, cluster_data)
                        associate_artigo_to_cluster(db, artigo.id, cluster.id)
                        novos_clusters += 1
                        print(f"  âœ… Novo Cluster: '{tema_principal}' com '{artigo.titulo_extraido}'")
                    
                    else:
                        print(f"  âš ï¸ Tipo de classificaÃ§Ã£o invÃ¡lido: {tipo}")
                
                except Exception as e:
                    print(f"  âŒ Erro ao processar classificaÃ§Ã£o: {e}")
                    continue
            
            return (anexacoes, novos_clusters)
            
        except Exception as e:
            print(f"âŒ ERRO na chamada da API incremental: {e}")
            return False
        
    except Exception as e:
        print(f"âŒ ERRO: Falha no processamento do lote {numero_lote}: {e}")
        import traceback
        traceback.print_exc()
        return False


def agrupar_noticias_com_prompt(db: Session, client) -> bool:
    """
    Agrupa notÃ­cias usando prompt, agora processando em lotes (batches)
    para evitar truncamento de resposta da API com grandes volumes.
    """
    try:
        # Busca todas as notÃ­cias prontas para agrupamento
        artigos_para_agrupar = db.query(ArtigoBruto).filter(
            ArtigoBruto.status == "pronto_agrupar",
            ArtigoBruto.cluster_id.is_(None)
        ).all()
        
        if not artigos_para_agrupar:
            print("INFO: Nenhum artigo novo para agrupamento.")
            return True
        
        print(f"ğŸ”— INICIANDO AGRUPAMENTO: {len(artigos_para_agrupar)} artigos a serem processados em lotes de {BATCH_SIZE_AGRUPAMENTO}.")
        
        # Mapeamento de ID para artigo original para todo o conjunto
        mapa_id_para_artigo = {i: artigo for i, artigo in enumerate(artigos_para_agrupar)}
        
        # Divide a lista de artigos em lotes
        lotes = [artigos_para_agrupar[i:i + BATCH_SIZE_AGRUPAMENTO] for i in range(0, len(artigos_para_agrupar), BATCH_SIZE_AGRUPAMENTO)]
        
        clusters_criados_total = 0
        artigos_agrupados_total = 0

        for num_lote, lote_artigos in enumerate(lotes, 1):
            print(f"\n--- Processando Lote {num_lote}/{len(lotes)} ({len(lote_artigos)} artigos) ---")
            
            # Prepara dados apenas para o lote atual
            noticias_lote_para_prompt = []
            mapa_id_lote_para_artigo = {}
            for i, artigo in enumerate(lote_artigos):
                 # O 'id' aqui Ã© relativo ao lote, para o prompt.
                noticia_data = {
                    "id": i,
                    "titulo": artigo.titulo_extraido or "Sem tÃ­tulo",
                    "jornal": artigo.jornal or "Fonte desconhecida",
                    "trecho": (artigo.texto_processado[:150] + "...") if len(artigo.texto_processado or "") > 150 else (artigo.texto_processado or "")
                }
                noticias_lote_para_prompt.append(noticia_data)
                mapa_id_lote_para_artigo[i] = artigo # Mapeia o ID do lote para o objeto artigo completo

            # Monta o prompt completo para o lote
            prompt_completo = f"""
{PROMPT_AGRUPAMENTO_V1}

NOTÃCIAS PARA AGRUPAR (LOTE {num_lote}/{len(lotes)}):
{json.dumps(noticias_lote_para_prompt, indent=2, ensure_ascii=False)}

IMPORTANTE: Retorne APENAS o JSON vÃ¡lido para este lote.
"""
            
            print(f"ğŸ“¤ ENVIANDO Lote {num_lote} para a API...")
            
            try:
                # Chama a API para o lote
                response = client.generate_content(
                    prompt_completo,
                    generation_config={
                        'temperature': 0.05,  # Ainda mais determinÃ­stico
                        'top_p': 0.7,
                        'max_output_tokens': 8192,  # Reduzido para lotes menores
                        'candidate_count': 1,
                        'top_k': 10
                    }
                )
                
                if not response.text:
                    print(f"âš ï¸ AVISO: API retornou resposta vazia para o lote {num_lote}. Pulando este lote.")
                    continue
                
                print(f"ğŸ“¥ RESPOSTA RECEBIDA para o Lote {num_lote}: {len(response.text)} caracteres")
                
                # Usa a nova funÃ§Ã£o de extraÃ§Ã£o robusta
                grupos_brutos = extrair_json_da_resposta(response.text)
                
                if not grupos_brutos or not isinstance(grupos_brutos, list):
                    print(f"âŒ ERRO: Resposta de agrupamento invÃ¡lida para o lote {num_lote}.")
                    continue
                
                print(f"âœ… SUCESSO LOTE {num_lote}: {len(grupos_brutos)} grupos criados.")
                
                # Processa os clusters do lote
                for grupo_data in grupos_brutos:
                    try:
                        tema_principal = grupo_data.get("tema_principal", f"Grupo Lote {num_lote}")
                        ids_no_lote = grupo_data.get("ids_originais", [])
                        artigos_do_grupo = [mapa_id_lote_para_artigo[id_lote] for id_lote in ids_no_lote if id_lote in mapa_id_lote_para_artigo]
                        
                        if not artigos_do_grupo:
                            continue
                        
                        # ETAPA 2: SÃ“ AGRUPA - usa valores padrÃ£o, serÃ£o redefinidos na ETAPA 3
                        prioridade_grupo = "P3_MONITORAMENTO"  # PadrÃ£o, serÃ¡ redefinido na ETAPA 3
                        tag_grupo = "Internacional (Economia e PolÃ­tica)"  # PadrÃ£o, serÃ¡ redefinido na ETAPA 3
                        
                        # Calcula embedding mÃ©dio do grupo
                        embeddings = []
                        for artigo in artigos_do_grupo:
                            if artigo.embedding:
                                embeddings.append(bytes_to_embedding(artigo.embedding))
                        
                        embedding_medio = None
                        if embeddings:
                            import numpy as np
                            embedding_medio = np.mean(embeddings, axis=0).tobytes()
                        
                        # Cria cluster
                        from backend.models import ClusterEventoCreate
                        cluster_data = ClusterEventoCreate(
                            titulo_cluster=tema_principal,
                            resumo_cluster=None,  # SerÃ¡ preenchido na ETAPA 3
                            tag=tag_grupo,
                            prioridade=prioridade_grupo,
                            embedding_medio=embedding_medio
                        )
                        
                        cluster = create_cluster(db, cluster_data)
                        clusters_criados_total += 1
                        
                        # Associa artigos ao cluster
                        for artigo in artigos_do_grupo:
                            associate_artigo_to_cluster(db, artigo.id, cluster.id)
                            artigos_agrupados_total += 1
                        
                        print(f"  âœ… Grupo: '{tema_principal}' - {len(artigos_do_grupo)} artigos")
                        
                    except Exception as e:
                        print(f"  âŒ Erro ao processar grupo no lote {num_lote}: {e}")
                        continue

            except Exception as e_lote:
                print(f"âŒ ERRO CRÃTICO ao processar o lote {num_lote}: {e_lote}")
                continue # Pula para o prÃ³ximo lote

        print(f"\nğŸ‰ ETAPA 2 CONCLUÃDA: {clusters_criados_total} clusters criados, {artigos_agrupados_total} artigos agrupados no total.")
        
        # Marca artigos como "processado" apÃ³s clusterizaÃ§Ã£o
        artigos_agrupados = db.query(ArtigoBruto).filter(
            ArtigoBruto.status == "pronto_agrupar",
            ArtigoBruto.cluster_id.isnot(None)  # Artigos que foram agrupados
        ).all()
        marcar_artigos_processados(db, artigos_agrupados)
        
        return True
        
    except Exception as e:
        print(f"âŒ ERRO GERAL na funÃ§Ã£o de agrupamento: {e}")
        import traceback
        traceback.print_exc()
        return False

def processar_artigos_em_lote(limite: int = 10) -> bool:
    """
    Processa artigos em lote (modo alternativo para reprocessamento).
    Usado quando queremos reagrupar todos os artigos do dia.
    """
    db = SessionLocal()
    try:
        # Busca artigos pendentes
        artigos_pendentes = get_artigos_pendentes(db, limite=limite)
        
        if not artigos_pendentes:
            print("âœ… Nenhum artigo pendente encontrado")
            return True
        
        print(f"ğŸ“° Encontrados {len(artigos_pendentes)} artigos pendentes")
        
        # ETAPA 1: Processa todos os artigos pendentes
        print(f"\nğŸ”„ ETAPA 1: Processando {len(artigos_pendentes)} artigos pendentes...")
        
        sucessos = 0
        erros = 0
        
        for i, artigo in enumerate(artigos_pendentes, 1):
            print(f"  ğŸ“¤ Processando artigo {i}/{len(artigos_pendentes)} (ID: {artigo.id})...")
            
            # Processa artigo sem clusterizaÃ§Ã£o automÃ¡tica
            if processar_artigo_sem_cluster(db, artigo.id, client):
                sucessos += 1
            else:
                erros += 1
            
            time.sleep(0.1)
        
        print(f"\nâœ… Processamento de artigos finalizado:")
        print(f"   ğŸ“° Artigos processados: {len(artigos_pendentes)}")
        print(f"   âœ… Sucessos: {sucessos}")
        print(f"   âŒ Erros: {erros}")
        
        # ETAPA 2: Busca artigos processados hoje para agrupamento
        hoje = get_date_brasil_str()
        artigos_hoje = db.query(ArtigoBruto).filter(
            ArtigoBruto.status == "processado",
            ArtigoBruto.processed_at >= hoje
        ).all()
        
        if not artigos_hoje:
            print("âœ… Nenhum artigo processado hoje para agrupamento")
            return True
        
        print(f"\nğŸ”— ETAPA 2: Agrupando {len(artigos_hoje)} artigos processados hoje...")
        
        # ETAPA 3: Agrupa notÃ­cias por similaridade
        grupos = agrupar_noticias_por_similaridade(db, artigos_hoje)
        
        if not grupos:
            print("âœ… Nenhum grupo formado")
            return True
        
        # ETAPA 4: Gera resumos apenas para grupos P1 e P2
        print(f"\nğŸ“ ETAPA 3: Gerando resumos para {len(grupos)} grupos...")
        
        clusters_criados = 0
        resumos_gerados = 0
        
        for i, grupo in enumerate(grupos, 1):
            print(f"  ğŸ“ Processando grupo {i}/{len(grupos)} com {len(grupo)} notÃ­cias...")
            
            # Verifica prioridade do grupo
            prioridade_grupo = grupo[0].prioridade
            print(f"    ğŸ“Š Prioridade do grupo: {prioridade_grupo}")
            
            # Cria cluster
            try:
                from backend.models import ClusterEventoCreate
                
                # Calcula embedding mÃ©dio do cluster
                embeddings = []
                for artigo in grupo:
                    if artigo.embedding:
                        embeddings.append(bytes_to_embedding(artigo.embedding))
                
                embedding_medio = None
                if embeddings:
                    import numpy as np
                    embedding_medio = np.mean(embeddings, axis=0).tobytes()
                
                # Cria cluster com dados bÃ¡sicos
                cluster_data = ClusterEventoCreate(
                    titulo_cluster=f"Cluster {i} - {len(grupo)} notÃ­cias",
                    resumo_cluster=None,  # SerÃ¡ preenchido se necessÃ¡rio
                    tag=grupo[0].tag,
                    prioridade=prioridade_grupo,
                    embedding_medio=embedding_medio
                )
                
                # Cria o cluster
                cluster = create_cluster(db, cluster_data)
                clusters_criados += 1
                
                # Associa artigos ao cluster
                for artigo in grupo:
                    associate_artigo_to_cluster(db, artigo.id, cluster.id)
                
                # Gera resumo apenas para P1 e P2
                if prioridade_grupo in ['P1_CRITICO', 'P2_ESTRATEGICO']:
                    print(f"    ğŸ“ Gerando resumo para cluster {cluster.id} (Prioridade: {prioridade_grupo})...")
                    
                if gerar_resumo_cluster(db, cluster.id, client):
                    resumos_gerados += 1
                    print(f"    âœ… Resumo gerado com sucesso")
                else:
                    print(f"    âŒ Falha ao gerar resumo")
                # else:
                #     print(f"    â„¹ï¸ Cluster {cluster.id} (Prioridade: {prioridade_grupo}) nÃ£o requer resumo. Pulando.")
                
            except Exception as e:
                print(f"    âŒ Erro ao criar cluster: {e}")
                continue
        
        print(f"\nğŸ‰ Processamento em lote finalizado:")
        print(f"   ğŸ“° Artigos processados: {sucessos}")
        print(f"   ğŸ”— Clusters criados: {clusters_criados}")
        print(f"   ğŸ“ Resumos gerados: {resumos_gerados}")
        print(f"   ğŸ“Š Grupos processados: {len(grupos)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erro geral no processamento: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        db.close()

def processar_artigo_sem_cluster(db: Session, id_artigo: int, client) -> bool:
    """
    Processa um artigo sem fazer clusterizaÃ§Ã£o automÃ¡tica.
    Usado no modo em lote.
    """
    try:
        # Busca dados brutos do artigo
        artigo = db.query(ArtigoBruto).filter(ArtigoBruto.id == id_artigo).first()
        if not artigo:
            return False
        
        # Processa o artigo sem clusterizaÃ§Ã£o (copia a lÃ³gica do processar_artigo_pipeline mas sem a ETAPA 7)
        create_log(db, "INFO", "processor", 
                  f"Iniciando processamento do artigo {id_artigo} (sem clusterizaÃ§Ã£o)",
                  {"fonte": artigo.fonte_coleta})
        
        # ETAPA 1: Verificar se jÃ¡ tem metadados estruturados
        metadados = artigo.metadados or {}
        
        # Se jÃ¡ tem dados estruturados (JSON), usa diretamente
        if metadados.get('titulo') and metadados.get('fonte_original'):
            print(f"    ğŸ“„ Artigo jÃ¡ estruturado, usando metadados existentes")
            
            # Migra tag antiga se necessÃ¡rio
            tag_original = metadados.get('tag', 'Economia e Tecnologia')
            tag_migrada = migrar_tag_antiga_para_nova(tag_original)
            
            noticia_data = {
                'titulo': metadados.get('titulo', 'Sem tÃ­tulo'),
                'texto_completo': artigo.texto_bruto,
                'jornal': metadados.get('fonte_original', 'Fonte desconhecida'),
                'autor': 'N/A',  # NÃ£o temos autor nos dados originais
                'pagina': '1',
                'data': metadados.get('data_publicacao') or get_date_brasil_str(),
                'categoria': metadados.get('categoria', 'Geral'),
                'tag': 'PENDING',  # SerÃ¡ redefinida na ETAPA 3
                'prioridade': 'PENDING'  # SerÃ¡ redefinida na ETAPA 3
            }
            
        else:
            # Para PDFs ou artigos sem estrutura, faz extraÃ§Ã£o bÃ¡sica
            print(f"    ğŸ“„ Artigo sem estrutura, fazendo extraÃ§Ã£o bÃ¡sica")
            
            # ExtraÃ§Ã£o bÃ¡sica sem LLM
            linhas = artigo.texto_bruto.split('\n')
            titulo = linhas[0].strip() if linhas else "Sem tÃ­tulo"
            
            # Tenta identificar jornal/fonte dos metadados
            jornal = metadados.get('fonte_original') or 'Fonte desconhecida'
            
            noticia_data = {
                'titulo': titulo,
                'texto_completo': artigo.texto_bruto,
                'jornal': jornal,
                'autor': 'N/A',
                'pagina': '1',
                'data': metadados.get('data_publicacao') or get_date_brasil_str(),
                'categoria': metadados.get('categoria', 'Geral'),
                'tag': 'PENDING',  # SerÃ¡ redefinida na ETAPA 3
                'prioridade': 'PENDING'  # SerÃ¡ redefinida na ETAPA 3
            }
        
        # ETAPA 2: MigraÃ§Ã£o e correÃ§Ã£o de dados
        from backend.processing import migrar_noticia_cache_legado, corrigir_tag_invalida
        noticia_data = migrar_noticia_cache_legado(noticia_data)
        
        # Corrige a tag se necessÃ¡rio
        if 'tag' in noticia_data:
            noticia_data['tag'] = corrigir_tag_invalida(noticia_data['tag'])
        
        # ETAPA 3: ValidaÃ§Ã£o com Pydantic
        try:
            print(f"    ğŸ” Validando dados com Pydantic...")
            
            noticia_obj = Noticia(**noticia_data)
            noticia_validada = noticia_obj.model_dump()
            print(f"    âœ… ValidaÃ§Ã£o Pydantic bem-sucedida")
        except Exception as e:
            print(f"    âŒ Erro de validaÃ§Ã£o Pydantic: {e}")
            create_log(db, "ERROR", "processor", 
                      f"Erro de validaÃ§Ã£o Pydantic do artigo {id_artigo}: {e}")
            update_artigo_status(db, id_artigo, 'erro')
            return False
        
        # ETAPA 4: Gerar embedding
        from backend.processing import gerar_embedding
        texto_para_embedding = f"{noticia_validada['titulo']} {noticia_validada['texto_completo']}"
        embedding_artigo = gerar_embedding(texto_para_embedding)
        
        if not embedding_artigo:
            create_log(db, "WARNING", "processor", 
                      f"Falha ao gerar embedding do artigo {id_artigo}")
            import numpy as np
            embedding_artigo = np.zeros(384, dtype=np.float32).tobytes()
        
        # ETAPA 5: Atualizar artigo com dados processados (SEM clusterizaÃ§Ã£o)
        dados_processados = {
            'titulo': noticia_validada['titulo'],
            'texto_completo': noticia_validada['texto_completo'],
            'jornal': noticia_validada['jornal'],
            'autor': noticia_validada['autor'],
            'pagina': noticia_validada['pagina'],
            'data': noticia_validada['data'],
            'categoria': noticia_validada['categoria'],
            'tag': noticia_validada['tag'],
            'prioridade': noticia_validada['prioridade'],
            'relevance_score': noticia_validada.get('relevance_score'),
            'relevance_reason': noticia_validada.get('relevance_reason')
        }
        
        # Atualiza dados processados e marca como "pronto_agrupar"
        update_artigo_dados_sem_status(db, id_artigo, dados_processados, embedding_artigo)
        
        # Marca como pronto para agrupamento (status mais curto)
        update_artigo_status(db, id_artigo, "pronto_agrupar")
        
        # NÃƒO faz clusterizaÃ§Ã£o aqui - serÃ¡ feita na ETAPA 2
        
        create_log(db, "INFO", "processor", 
                  f"Artigo {id_artigo} pronto para agrupamento")
        return True
        
    except Exception as e:
        print(f"    âŒ Erro geral no processamento: {e}")
        import traceback
        traceback.print_exc()
        create_log(db, "ERROR", "processor", 
                  f"Erro geral no processamento do artigo {id_artigo}: {e}")
        update_artigo_status(db, id_artigo, 'erro')
        return False

def agrupar_noticias_por_similaridade(db: Session, artigos_processados: List[ArtigoBruto]) -> List[List[ArtigoBruto]]:
    """
    Agrupa notÃ­cias por similaridade usando embeddings.
    Usado no modo em lote.
    """
    if not artigos_processados:
        return []
    
    print(f"    ğŸ”— Agrupando {len(artigos_processados)} artigos por similaridade...")
    
    grupos = []
    artigos_visitados = set()
    
    for i, artigo in enumerate(artigos_processados):
        if artigo.id in artigos_visitados:
            continue
        
        # Cria novo grupo
        grupo = [artigo]
        artigos_visitados.add(artigo.id)
        
        # Busca artigos similares
        if artigo.embedding:
            embedding_artigo = bytes_to_embedding(artigo.embedding)
            
            for j, outro_artigo in enumerate(artigos_processados):
                if (outro_artigo.id not in artigos_visitados and 
                    outro_artigo.embedding and
                    outro_artigo.tag == artigo.tag):  # Mesma tag
                    
                    embedding_outro = bytes_to_embedding(outro_artigo.embedding)
                    similaridade = calcular_similaridade_cosseno(embedding_artigo, embedding_outro)
                    
                    if similaridade > 0.7:  # Threshold de similaridade
                        grupo.append(outro_artigo)
                        artigos_visitados.add(outro_artigo.id)
        
        grupos.append(grupo)
    
    print(f"    âœ… Criados {len(grupos)} grupos de notÃ­cias")
    return grupos

def main():
    """FunÃ§Ã£o principal"""
    print("=" * 60)
    print("ğŸ”„ BTG AlphaFeed - Orquestrador (Fluxo de NegÃ³cio Correto)")
    print("=" * 60)
    
    # ConfiguraÃ§Ã£o
    limite = 999  # Limite para desenvolvimento
    modo_incremental = True  # True = incremental, False = em lote
    
    print(f"ğŸ“Š Limite de artigos: {limite}")
    print(f"ğŸ¯ Modo: {'Incremental' if modo_incremental else 'Em Lote'}")
    
    # Verifica configuraÃ§Ã£o inicial
    print(f"ğŸ”§ GEMINI_API_KEY configurada: {'Sim' if os.getenv('GEMINI_API_KEY') else 'NÃ£o'}")
    print(f"ğŸ”§ DATABASE_URL configurada: {'Sim' if os.getenv('DATABASE_URL') else 'NÃ£o'}")
    
    # Executa processamento
    if modo_incremental:
        sucesso = processar_artigos_pendentes(limite)
    else:
        sucesso = processar_artigos_em_lote(limite)
    
    if sucesso:
        print("\nğŸ‰ Processamento completo concluÃ­do com sucesso!")
        print("ğŸ’¡ Verifique o frontend para ver os clusters e resumos gerados")
    else:
        print("\nâŒ Processamento falhou")
    
    print("=" * 60)

if __name__ == "__main__":
    main() 