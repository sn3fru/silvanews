## BTG AlphaFeed

Plataforma de inteligencia de mercado que transforma alto volume de noticias (~1000/dia) em um feed orientado a eventos (clusters) para Special Situations, com suporte para noticias nacionais e internacionais.

> **Documentacao completa:** [`docs/SYSTEM.md`](docs/SYSTEM.md) (arquitetura e referencia) | [`docs/OPERATIONS.md`](docs/OPERATIONS.md) (setup e API) | [`docs/V2-MIGRATION-PLAN.md`](docs/V2-MIGRATION-PLAN.md) (Graph-RAG v2)

### O que e (visao executiva)

- Identifica o fato gerador por tras de multiplas noticias e consolida em um unico evento.
- Classifica por prioridade (P1 critico, P2 estrategico, P3 monitoramento) e por categoria tematica (tags).
- Gera resumos executivos no tamanho certo por prioridade (P1 longo, P2 medio, P3 curto).
- Oferece visualizacao por data, filtros e drill-down para as fontes originais.
- Separacao entre noticias nacionais e internacionais com tags e criterios de priorizacao especificos.
- **NOVO v2.0**: Arquitetura Graph-RAG Agentica com memoria temporal (grafo de conhecimento + LangGraph).

### Como funciona na pr√°tica

- Carregue PDFs/JSONs de crawlers (upload manual ou via API) ‚Üí s√£o salvos como artigos brutos com **texto original completo** preservado.
- O processamento orquestrado agrupa por fato gerador, classifica e gera resumos dos **clusters** (n√£o das not√≠cias individuais).
- O frontend exibe o feed por data, com filtros de prioridade e tags din√¢micas vindas dos dados reais.

### Novidades recentes (pipeline mais rigoroso)

- **Suporte a not√≠cias internacionais**: Sistema de abas Brasil/Internacional no frontend com:
  - Detec√ß√£o autom√°tica do tipo de fonte (nacional/internacional) baseada no jornal (com heur√≠stica ampliada para FT/WSJ/NYT/Bloomberg etc.)
  - Tags espec√≠ficas para contexto internacional (ex: "Global M&A", "Central Banks and Monetary Policy")
  - Crit√©rios de prioriza√ß√£o adaptados para mercado global (valores em d√≥lares, empresas Fortune 500)
  - Filtros independentes por tipo de fonte na API
- **Preserva√ß√£o do texto original**: O `texto_bruto` dos PDFs √© **NUNCA alterado** durante o processamento, garantindo acesso ao conte√∫do original completo.
- **Resumos de clusters**: O `texto_processado` cont√©m resumos dos **clusters de eventos**, n√£o de not√≠cias individuais.
- Endurecimento do `PROMPT_EXTRACAO_PERMISSIVO_V8`: lista de rejei√ß√£o ampliada (crimes comuns, casos pessoais, fofoca/entretenimento, esportes, pol√≠tica partid√°ria, efem√©rides e programas sociais sem tese) e gating P1/P2/P3 mais duro.
- Prioriza√ß√£o Executiva Final integrada: etapa adicional p√≥s-resumo/p√≥s-agrupamento que reclassifica como P1/P2/P3/IRRELEVANTE com justificativa e a√ß√£o recomendada (`PROMPT_PRIORIZACAO_EXECUTIVA_V1`).
- Nova Etapa 4 ‚Äì Consolida√ß√£o Final de Clusters: reagrupamento conservador de clusters do dia usando `PROMPT_CONSOLIDACAO_CLUSTERS_V1` com base em t√≠tulos, tags e prioridades j√° atribu√≠dos. A maioria dos clusters permanece inalterada; quando h√° duplicidade (p.ex. varia√ß√µes de "PGFN arrecada√ß√£o recorde"), a etapa sugere merges, move artigos para um destino, ajusta t√≠tulo/tag/prioridade quando necess√°rio e arquiva (soft delete) os duplicados.
- Robustez: ajustes para evitar truncamento do LLM e JSON quebrado ‚Äî lotes menores, campos de resumo encurtados e fallback de parsing.
  - Incremental: lotes de at√© 100, `titulos_internos` reduzido a 10 por cluster, heur√≠stica que impede cria√ß√£o de m√∫ltiplos clusters gen√©ricos (ex.: "Not√≠cia sem t√≠tulo"). Processamento incremental e em lote agora separam por tipo_fonte (NACIONAL/INTERNACIONAL) ‚Äî nunca se misturam no mesmo prompt.
  - Prioriza√ß√£o: lotes de at√© 40, `resumo_final` enviado truncado a ~240 caracteres, `max_output_tokens` 8192 para batched.
  - Consolida√ß√£o: lotes de at√© 50; parsing de sugest√µes tolerante a erros; fallback determin√≠stico por t√≠tulo/tag.

### Filtros e visualiza√ß√£o (frontend)

- **Abas Brasil/Internacional**: Separa not√≠cias por tipo de fonte com filtros independentes.
- Card do Estagi√°rio otimizado: design mais compacto com t√≠tulo e descri√ß√£o na mesma linha.
- Seletor de data no topo: alterna entre hoje e datas hist√≥ricas (tudo GMT-3).
- Filtros: prioridade (P1/P2/P3) e tags din√¢micas (derivadas dos clusters reais).
- P3 Monitoramento: cards consolidados por tag com lista em bullets; clique abre modal com detalhes.
- Deep-dive: modal por evento com resumo, fontes e abas (chat com cluster e gerenciamento).

### Tags e Prioridades (como configurar)

- **Configura√ß√£o via Frontend**: Acesse `/frontend/settings.html` ‚Üí aba "Prompts" para editar tags e prioridades de forma visual e intuitiva.
- **Persist√™ncia no Banco**: Tags e prioridades s√£o agora armazenadas no PostgreSQL, permitindo edi√ß√µes em produ√ß√£o sem perda de dados.
- **Estrutura das Tags**: Cada tag tem nome, descri√ß√£o, exemplos e ordem de exibi√ß√£o.
- **Estrutura das Prioridades**: Itens organizados por n√≠vel (P1_CRITICO, P2_ESTRATEGICO, P3_MONITORAMENTO) com descri√ß√µes personaliz√°veis.
- **Fallback Autom√°tico**: O sistema carrega do banco de dados, mas mant√©m compatibilidade com as estruturas originais do `backend/prompts.py`.
- **Migra√ß√£o**: Use `python seed_prompts.py` para popular o banco com dados iniciais ap√≥s criar as tabelas.

### Pipeline (passo a passo) ‚Äî v2.0 Graph-RAG Integrado

O pipeline e executado via `run_complete_workflow.py` que orquestra todas as etapas:

```bash
python run_complete_workflow.py           # ciclo unico (ingestao + processamento + migracao + notificacao)
python run_complete_workflow.py --scheduler --interval 60  # loop continuo (de hora em hora)
```

**Etapas internas (executadas automaticamente):**

1) **Ingestao** (`load_news.py`)
   - PDFs: usa `PROMPT_EXTRACAO_PDF_RAW_V1` para extrair TEXTO COMPLETO ORIGINAL (sem resumo)
   - JSONs: ingeridos diretamente sem LLM
   - Deduplicacao semantica via `embedding_v2` (768d Gemini) ‚Äî rejeita artigos com >85% de similaridade
   - Salva como artigos brutos (status `pendente`) com `texto_bruto` preservado
2) **Processamento + Enriquecimento Graph-RAG** (Etapa 1)
   - `process_articles.py::processar_artigo_sem_cluster` + `enriquecer_artigo_v2`
   - Valida dados, aplica heuristicas leves, gera embedding v1 (384d hash)
   - **v2**: Gera `embedding_v2` (768d Gemini), extrai entidades via LLM (NER), resolve nomes canonicos e persiste no grafo (`graph_entities` + `graph_edges`)
   - Marca `pronto_agrupar`
3) **Agrupamento + Dicas de Similaridade** (Etapa 2)
   - Lote: `agrupar_noticias_com_prompt` ‚Üí `PROMPT_AGRUPAMENTO_V1`
   - Incremental: `agrupar_noticias_incremental` ‚Üí `PROMPT_AGRUPAMENTO_INCREMENTAL_V2`
   - **v2**: Calcula similaridade cosseno entre artigos via `embedding_v2` e injeta DICAS DE SIMILARIDADE no prompt (ex: "Noticias 3 e 7 tem 92% de similaridade semantica")
   - Isolado por tipo_fonte (NACIONAL/INTERNACIONAL nunca se misturam)
4) **Classificacao, Resumo + Contexto Historico** (Etapa 3)
   - `classificar_e_resumir_cluster` ‚Üí `PROMPT_ANALISE_E_SINTESE_CLUSTER_V1`
   - **v2**: Busca contexto historico via `get_context_for_cluster()` (grafo: entidades 7 dias + vetorial: artigos similares 30 dias) e injeta no prompt como CONTEXTO HISTORICO. Permite resumos como "Este e o terceiro anuncio do tipo nesta semana..."
5) **Consolidacao + Similaridade de Clusters** (Etapa 4)
   - `consolidacao_final_clusters` ‚Üí `PROMPT_CONSOLIDACAO_CLUSTERS_V1`
   - **v2**: Calcula embedding medio por cluster (media dos `embedding_v2` dos artigos) e identifica pares com alta similaridade para sugerir merges mais inteligentes
6) **Migracao** ‚Üí Sincroniza local ‚Üí Heroku (`migrate_incremental --include-all`)
7) **Notificacoes** ‚Üí Envia clusters novos via Telegram (se configurado)
8) **Exposicao**: API `FastAPI` alimenta o frontend; CRUD e endpoints admin.

```mermaid
graph TD
  A[Upload PDFs/JSON<br/>load_news.py] --> B[(PostgreSQL<br/>artigos_brutos: pendente)]
  B --> C[Etapa 1: processar + Graph-RAG v2<br/>embedding_v2 + NER + grafo]
  C --> D[(artigos_brutos: pronto_agrupar<br/>+ graph_entities + graph_edges)]
  D --> E[Etapa 2: agrupar<br/>+ dicas similaridade v2]
  E --> F[Etapa 3: classificar/resumir<br/>+ contexto historico do grafo]
  F --> F2[Etapa 4: consolidacao<br/>+ similaridade entre clusters]
  F2 --> G[(clusters_eventos + resumos + grafo)]
  G --> M[migrate_incremental<br/>local ‚Üí Heroku]
  M --> T[notify_telegram<br/>clusters novos]
  G --> H[FastAPI /api/feed]
  H --> I[Frontend /frontend]
```

### LLMs e prompts (o que roda e para que)

- Extracao de PDFs: `PROMPT_EXTRACAO_PDF_RAW_V1`
- **v2 NER (Etapa 1)**: `PROMPT_ENTITY_EXTRACTION` ‚Äî extrai entidades (PERSON, ORG, GOV, EVENT, CONCEPT) via Gemini Flash
- **v2 Embedding (Etapa 1)**: `gemini-embedding-001` ‚Äî gera embedding_v2 de 768 dimensoes para cada artigo
- Agrupamento em lote: `PROMPT_AGRUPAMENTO_V1` + **dicas de similaridade v2**
- Agrupamento incremental: `PROMPT_AGRUPAMENTO_INCREMENTAL_V2` + **dicas de similaridade v2**
- Classificacao e resumo (Etapa 3): `PROMPT_ANALISE_E_SINTESE_CLUSTER_V1` + **contexto historico do grafo v2**
- Consolidacao final (Etapa 4): `PROMPT_CONSOLIDACAO_CLUSTERS_V1` + **similaridade entre clusters v2**
- Chat com cluster: `PROMPT_CHAT_CLUSTER_V1`

### Busca Sem√¢ntica (novo m√≥dulo)

- Pacote: `btg_alphafeed/semantic_search/` com:
  - `embedder.py`: gera√ß√£o de embeddings (`text-embedding-3-small` via OpenAI se dispon√≠vel; fallback determin√≠stico).
  - `store.py`: persist√™ncia em `semantic_embeddings` (tabela dedicada, n√£o interfere no pipeline atual).
  - `search.py`: busca por similaridade de cosseno em mem√≥ria (compat√≠vel sem pgvector).
  - `backfill_embeddings.py`: utilit√°rio CLI para gerar embeddings dos artigos existentes.
- Integra√ß√£o com o agente Estagi√°rio: nova tool `semantic_search(consulta, limite?, modelo?)` para consultas abertas.
- Como rodar o backfill:
  ```bash
  conda activate pymc2
  python -m btg_alphafeed.semantic_search.backfill_embeddings --limit 1000 --model text-embedding-3-small
  ```

### Agente Estagi√°rio: plano ‚Üí execu√ß√£o (agentic)

- O agente agora opera com camadas LLM para planejar e executar:
  1. Entender inten√ß√£o (consultar not√≠cias, ADMIN, EDI√á√ÉO no DB, ou AN√ÅLISE DE FEEDBACK)
  2. Se EDI√á√ÉO (trocar tag/prioridade):
     - Entende opera√ß√£o via LLM (JSON: operation/cluster_id/cluster_title/new_tag/new_priority)
     - Se n√£o houver `new_tag`/`new_priority`, consulta o LLM com o CAT√ÅLOGO do banco e o contexto do cluster para decidir a tag/prioridade corretas
     - Resolve o cluster por ID ou t√≠tulo parcial do dia e aplica via CRUD
  3. Se AN√ÅLISE DE FEEDBACK (ex.: "quais not√≠cias t√™m refor√ßo positivo?"):
     - Busca not√≠cias com likes/dislikes do dia
     - Lista t√≠tulos e informa√ß√µes para ajuste de prompts
     - Fornece estat√≠sticas de aprova√ß√£o
  4. Se CONSULTA (ex.: "quero trocar de carro para um el√©trico, tem promo√ß√£o?"):
     - Pede ao LLM um SPEC JSON de busca (priorities/tags/keywords)
     - Coleta candidatos do DB, triagem via LLM, aprofunda top-K e sintetiza resposta

Endpoints do agente

- `POST /api/estagiario/start`, `POST /api/estagiario/send`, `GET /api/estagiario/messages/{session_id}`

Prompts opcionais/POC (n√£o usados no pipeline padr√£o):

- `PROMPT_RESUMO_CRITICO_V1` (POC de resumo cr√≠tico)
- `PROMPT_RADAR_MONITORAMENTO_V1` (POC de bullets de radar P3)
- `PROMPT_AGRUPAMENTO_INCREMENTAL_V1` (substitu√≠do por `V2` no pipeline)
- `PROMPT_SANITIZACAO_CLUSTER_V1` (dispon√≠vel como segunda linha de defesa; n√£o integrado por padr√£o)
- `PROMPT_EXTRACAO_JSON_V1` (alias de `PROMPT_EXTRACAO_PERMISSIVO_V8`, mantido para compatibilidade)
- Onde ajustar: `backend/prompts.py` (textos, tags, prioridades). API key via `backend/.env` (`GEMINI_API_KEY`).

### Arquitetura em 1 minuto

- Backend FastAPI + SQLAlchemy (PostgreSQL 5433)
- **Estrutura de dados**: `artigos_brutos` (texto original dos PDFs) ‚Üí `clusters_eventos` (resumos dos clusters)
  - **NOVO**: Coluna `tipo_fonte` em ambas as tabelas para separar not√≠cias nacionais/internacionais
- **Preserva√ß√£o de dados**: `texto_bruto` nunca √© alterado; `texto_processado` cont√©m resumos dos clusters
- Frontend est√°tico em `frontend/`
- Orquestra√ß√£o por scripts CLI para ingest√£o e processamento

## Guia R√°pido

### Pr√©-requisitos

- Anaconda instalado; usar o ambiente `pymc2`
- PostgreSQL local ativo na porta `5433`
- `backend/.env` com vari√°veis m√≠nimas:
  ```env
  DATABASE_URL="postgresql+psycopg2://postgres_local@localhost:5433/devdb"
  GEMINI_API_KEY="<sua_chave_gemini>"
  ```

### 1) Ativar ambiente e entrar no projeto

```bash
conda activate pymc2
cd "C:\Users\marcos.silva\OneDrive - ENFORCE GESTAO DE ATIVOS S.A\jupyter\projetos\novo-topnews\pdfs\silva-front\btg_alphafeed"
```

### 2) Migra√ß√£o do banco de dados (NOVO - executar uma vez)

Se voc√™ j√° tem um banco existente, execute a migra√ß√£o para adicionar suporte a not√≠cias internacionais:

```bash
python add_tipo_fonte_migration.py
```

### 3) Comandos essenciais

- **Pipeline completo (recomendado)** ‚Äî ingestao + processamento v2 + migracao + notificacao:

```bash
python run_complete_workflow.py
```

- **Pipeline em loop** (de hora em hora, para operacao continua):

```bash
python run_complete_workflow.py --scheduler --interval 60
```

- Upload manual de PDFs/JSON (diretorio completo):

```bash
python load_news.py --dir ../pdfs --direct --yes
```

- Processar artigos isoladamente (sem migracao/notificacao):

```bash
python process_articles.py
```

- Iniciar o backend (use apenas se nao houver outro servidor rodando):

```bash
python start_dev.py
```

- Acesso rapido: Frontend `http://localhost:8000/frontend` | Docs `http://localhost:8000/docs` | Health `http://localhost:8000/health`

### Estimativa de Custos (LLM)

Para uma estimativa r√°pida de custos por etapa do pipeline, execute:

```bash
python estimativa_custos.py
```

O script simula tokens de entrada/sa√≠da por etapa usando as not√≠cias no banco e imprime um comparativo por cen√°rio de modelos.

## Sincronizar Banco Local ‚Üí Heroku (Incremental)

Rodar a partir da pasta `silva-front` ou `btg_alphafeed`:

```bash
  conda activate pymc2
  python -m btg_alphafeed.migrate_incremental \
    --source "postgresql+psycopg2://postgres_local@localhost:5433/devdb" \
    --dest   "postgres://<usuario>:<senha>@<host>:5432/<db>" \
    --include-logs --include-chat
```

- Op√ß√µes √∫teis:
  - `--no-update-existing`: n√£o atualiza registros existentes (apenas insere novos)
  - `--since <ISO-UTC>`: inicia a partir de um timestamp espec√≠fico
  - `--meta-file btg_alphafeed/last_migration.txt`: armazena o √∫ltimo timestamp migrado
  - `--only clusters,artigos,logs`: restringe entidades

### Replace do dia (produ√ß√£o) com reprocessamento local

Use quando voc√™ reprocessou o dia localmente (novos prompts) e quer substituir o dia em produ√ß√£o de forma limpa (sem manter clusters antigos do mesmo dia).

Passo a passo:

1. Reprocessar localmente o dia (mant√©m brutos, limpa processados de hoje e reprocessa o pipeline completo):

   ```bash
   conda activate pymc2
   cd btg_alphafeed
   python reprocess_today.py
   ```
2. Replace no destino (produ√ß√£o) APENAS do dia desejado:

   ```bash
   conda activate pymc2
   cd btg_alphafeed
   python migrate_replace_today.py \
     --source "postgresql+psycopg2://postgres_local@localhost:5433/devdb" \
     --dest   "postgres://<usuario>:<senha>@<host>:5432/<db>" \
     --day $(python -c "from datetime import date; print(date.today().isoformat())")
   ```

   O utilit√°rio limpa no destino os dados do dia (desassocia artigos, remove clusters/altera√ß√µes/chat/s√≠ntese do dia) e migra do origem os clusters/artigos/s√≠ntese do mesmo dia, for√ßando `cluster_id` dos artigos.
3. (Opcional) Rodar incremental normal para demais dias/logs/chat:

   ```bash
   python -m btg_alphafeed.migrate_incremental \
     --source "postgresql+psycopg2://postgres_local@localhost:5433/devdb" \
     --dest   "postgres://<usuario>:<senha>@<host>:5432/<db>" \
     --include-logs --include-chat
   ```

## Suporte a Not√≠cias Internacionais (NOVO)

O sistema agora diferencia automaticamente entre fontes nacionais e internacionais:

**Detec√ß√£o Autom√°tica de Fonte**:

- Jornais nacionais: Folha, Estad√£o, O Globo, Valor, etc.
- Jornais internacionais: NYT, WSJ, Financial Times, Bloomberg, etc.
- Default: nacional (para compatibilidade com dados existentes)

**Tags Internacionais Dispon√≠veis**:

- Global M&A and Corporate Transactions
- Global Legal and Regulatory
- Sovereign Debt and Credit
- Global Distressed and Restructuring
- Global Capital Markets
- Central Banks and Monetary Policy
- Geopolitics and Trade
- Technology and Innovation

**Crit√©rios de Prioriza√ß√£o Internacional**:

- P1: Defaults soberanos > $5B, Chapter 11 de Fortune 500, mega-mergers > $20B
- P2: Mudan√ßas de rating de pa√≠ses G20, M&As cross-border > $5B
- P3: Earnings regulares, indicadores econ√¥micos, desenvolvimentos pol√≠ticos

## Tarefas Comuns

- Pipeline completo (recomendado):

```bash
python run_complete_workflow.py
```

- Limpar banco (script interativo, com backup):

```bash
python limpar_banco.py
```

- Ver estado da API e DB:

```bash
curl http://localhost:8000/health
```

### Reprocessamento seletivo (do dia atual)

- Reverter clusters problem√°ticos (move artigos para `pronto_agrupar` e arquiva clusters):
  ```bash
  # Sele√ß√£o autom√°tica por t√≠tulos gen√©ricos (ex.: "Not√≠cia sem t√≠tulo")
  python revert_bad_clusters.py
  # OU por IDs espec√≠ficos
  python revert_bad_clusters.py --ids 8349,8350
  ```
- Reagrupar apenas os revertidos e concluir Etapas 3 e 4:
  ```bash
  python reprocess_incremental_today.py
  ```

### Novo (Prot√≥tipo) ‚Äì An√°lise de Feedback para Ajuste de Prompt

- Agora √© poss√≠vel registrar like/dislike de not√≠cias diretamente no feed (üëç/üëé ao lado do t√≠tulo de cada card). O backend agrega esse feedback por cluster e exp√µe no `GET /api/feed` dentro do campo `feedback` de cada item: `{ likes, dislikes, last }`.
- Foi adicionado um prot√≥tipo de an√°lise de feedback que sugere ajustes no prompt de agrupamento sem alterar os arquivos em produ√ß√£o. Ele gera um relat√≥rio com o diff do prompt proposto.

Rodar o prot√≥tipo de an√°lise de feedback e gerar diff do prompt:

```bash
conda activate pymc2
python analisar_feedback_prompt.py --limit 200 --output reports/prompt_diff_feedback.md
```

Sa√≠da esperada:

- Arquivo `reports/prompt_diff_feedback.md` com:
  - Prompt atual
  - Prompt proposto (adiciona addendum baseado em padr√µes de like/dislike)
  - Diff (unified) entre os dois para revis√£o humana

Importante: este processo n√£o altera `backend/prompts.py`. √â apenas para estudo e valida√ß√£o.

## Playbooks (cenarios prontos)

### 1) Pipeline completo do dia (RECOMENDADO)

```bash
conda activate pymc2
cd btg_alphafeed
python run_complete_workflow.py
# Executa: ingestao PDFs ‚Üí processamento v2 (Graph-RAG) ‚Üí migracao Heroku ‚Üí notificacoes
```

### 2) Pipeline em loop (operacao continua, de hora em hora)

```bash
conda activate pymc2
cd btg_alphafeed
python run_complete_workflow.py --scheduler --interval 60
# Deduplicacao semantica garante que artigos repetidos nao sao reprocessados
```

### 3) Reprocessar apenas pendentes (sem ingestao, sem migracao)

```bash
conda activate pymc2
cd btg_alphafeed
python process_articles.py
```

### 4) Sincronizar local ‚Üí Heroku (manual)

```bash
conda activate pymc2
cd btg_alphafeed
python -m migrate_incremental \
  --source "postgresql+psycopg2://postgres_local@localhost:5433/devdb" \
  --dest   "postgres://<usuario>:<senha>@<host>:5432/<db>" \
  --include-all
```

### 5) Backfill do grafo (uma vez, apos setup inicial)

```bash
python scripts/backfill_graph.py --days 30 --batch 50
```

### 6) Verificar data sem dados (sanidade)

```bash
curl "http://localhost:8000/api/feed?data=2099-01-01"
# Esperado: metricas zeradas e lista vazia
```

## Regras de Neg√≥cio e Conven√ß√µes

- Tags: usar apenas as definidas em `backend/prompts.py` (`TAGS_SPECIAL_SITUATIONS`)
- JSONs de crawlers: ingest√£o direta; LLM s√≥ na fase de agrupamento/s√≠ntese
- `process_articles.py`: orquestra e chama a l√≥gica do backend; sem regras de neg√≥cio pr√≥prias
- Status dos artigos: `pendente` ‚Üí `pronto_agrupar` ‚Üí `processado`
- Prioridades e resumos: P1 (longo), P2 (m√©dio), P3 (curto)

## Endpoints principais

- `GET /api/feed?data=YYYY-MM-DD`
  - Cada item do feed inclui `feedback: { likes, dislikes, last }` agregados por cluster
- `POST /admin/processar-pendentes`
- `POST /api/admin/upload-file` e `GET /admin/upload-progress/{file_id}`
- `GET /health`
- Frontend servido em `/frontend`

### Endpoints de BI e Feedback

- `GET /api/bi/series-por-dia?dias=30`
- `GET /api/bi/noticias-por-fonte?limit=20`
- `GET /api/bi/noticias-por-autor?limit=20`
- `POST /api/feedback?artigo_id=<id>&feedback=like|dislike`
- `GET /api/feedback?processed=`
- `POST /api/feedback/{id}/process`

## Dicas de Ambiente

- Sempre usar Anaconda Prompt e `conda activate pymc2`
- Porta padr√£o do Postgres local: `5433`
- Evite subir o backend se j√° existir um servidor rodando

## Troubleshooting (r√°pido)

- Sem `GEMINI_API_KEY`: PDFs s√£o ingeridos como 1 artigo por p√°gina (fallback)
- Conex√£o DB falhando: confirme porta `5433` e `DATABASE_URL`
- Sem artigos pendentes: rode primeiro o upload (`load_news.py`)
- API offline: use o modo `--direct` do `load_news.py`

## Testes

```bash
python tests/test_imports.py
python test_fluxo_completo.py
```

## Seguran√ßa

- Nunca commitar `.env` ou credenciais
- Evite colar URLs de produ√ß√£o com usu√°rio/senha em documentos/commits

## Formatos suportados (resumo)

- JSON de crawlers: campos t√≠picos `id_hash`, `titulo`, `texto_completo`, `link`, `fonte`, `data_publicacao`
- PDF: OCR + LLM quando dispon√≠vel; fallback 1 artigo por p√°gina

## Referencias rapidas

- **Pipeline completo**: `python run_complete_workflow.py` (ingestao + processamento v2 + migracao + notificacao)
- **Pipeline loop**: `python run_complete_workflow.py --scheduler --interval 60`
- Upload manual: `python load_news.py --dir ../pdfs --direct --yes`
- Processar somente: `python process_articles.py`
- Backend: `python start_dev.py`
- Migracao Heroku: `python -m migrate_incremental --source "..." --dest "..." --include-all`

---

## Documenta√ß√£o para LLMs (manuten√ß√£o e navega√ß√£o do c√≥digo)

### Mapa de pastas (o que cada uma faz)

- `btg_alphafeed/backend/main.py`: aplica√ß√£o FastAPI, rotas principais (`/api/feed`, admin, upload, health), serve frontend e orquestra background tasks.
- `btg_alphafeed/backend/database.py`: engine, sess√µes, modelos SQLAlchemy, helpers de conex√£o e utilidades de schema.
- `btg_alphafeed/backend/crud.py`: opera√ß√µes de banco de dados (artigos, clusters, associa√ß√µes, atualiza√ß√µes de status, m√©tricas, logs).
- `btg_alphafeed/backend/processing.py`: pipeline de processamento unit√°rio (embeddings, similaridade, processamento de artigo, resumo de cluster, utilit√°rios de cluster).
- `btg_alphafeed/backend/models.py`: modelos Pydantic para request/response.
- `btg_alphafeed/backend/prompts.py`: fonte da verdade de `TAGS_SPECIAL_SITUATIONS`, `LISTA_RELEVANCIA_HIERARQUICA` e todos os prompts de LLM.
- `btg_alphafeed/backend/utils.py`: utilidades gerais (datas GMT-3, etc.).
- `btg_alphafeed/backend/collectors/file_loader.py`: classe `FileLoader` usada por `load_news.py` para ingerir PDFs/JSONs.
- `btg_alphafeed/frontend/index.html|script.js|style.css`: UI do feed, seletor de data, modal de deep-dive, filtros din√¢micos.
- `btg_alphafeed/frontend/settings.html|settings.js`: UI administrativa/CRUD e opera√ß√µes de manuten√ß√£o, com abas de BI (s√©ries por dia, por fonte e por autor) e Feedback (like/dislike por artigo, com marca√ß√£o de processado).
- `btg_alphafeed/load_news.py`: CLI para ingest√£o de PDFs/JSONs (salva artigos brutos, com `--direct` para DB).
- `btg_alphafeed/process_articles.py`: orquestrador do pipeline v2 (processar + enriquecer Graph-RAG ‚Üí agrupar com dicas similaridade ‚Üí classificar/resumir com contexto historico ‚Üí consolidar com similaridade clusters).
- `btg_alphafeed/run_complete_workflow.py`: ponto de entrada principal. Orquestra: ingestao ‚Üí processamento ‚Üí migracao ‚Üí notificacao. Suporta `--scheduler` para loop continuo.
- `btg_alphafeed/migrate_incremental.py`: sync incremental local ‚Üí Heroku (idempotente, com filtros `--only`, `--include-logs`, `--include-chat`).
- `btg_alphafeed/migrate_databases.py`: migra√ß√£o completa (one-shot) local ‚Üí Heroku.
- `btg_alphafeed/limpar_banco.py`: limpeza seletiva com backup.

### Fluxos criticos (pontos de extensao)

- Ingestao: estender `backend/collectors/file_loader.py` para novas fontes/formatos.
- Classificacao/Tags: alterar apenas `backend/prompts.py` e manter coerencia com `TAGS_SPECIAL_SITUATIONS`.
- Prioridade/Resumo: `backend/prompts.py` (PROMPT_RESUMO_*), tamanho conforme P1/P2/P3.
- Agrupamento incremental: `PROMPT_AGRUPAMENTO_INCREMENTAL_V2` + dicas de similaridade v2 em `process_articles.py`.
- Enriquecimento v2 (NER + embeddings + grafo): `process_articles.py::enriquecer_artigo_v2` + `backend/agents/graph_crud.py`.
- Contexto historico (Etapa 3): `backend/agents/graph_crud.py::get_context_for_cluster` injeta grafo + vetorial no prompt.
- API/Endpoints: adicionar rotas em `backend/main.py` e delegar CRUD para `backend/crud.py`.

### Regras de contribui√ß√£o (para evitar duplica√ß√£o e l√≥gica fora do lugar)

- Orquestra√ß√£o em scripts (CLI): `load_news.py` e `process_articles.py` apenas chamam o backend.
- L√≥gica de neg√≥cio: `backend/processing.py` + `backend/crud.py` (nunca em CLI nem em rotas diretamente). A l√≥gica de consolida√ß√£o (Etapa 4) usa utilit√°rios em `backend/crud.py` para merges seguros: `merge_clusters`, `update_cluster_title`, `update_cluster_priority`, `update_cluster_tags` e `soft_delete_cluster`.
- Esquema de dados (DB): `backend/database.py` (modelos/relacionamentos) e migrations externas quando necess√°rio.
- Prompts/taxonomia: `backend/prompts.py` (fonte √∫nica). O frontend consome tags vindas dos dados reais (sem listas fixas).

### Consulta r√°pida de responsabilidades

- Gerar resumo de um cluster: `backend/processing.py::gerar_resumo_cluster`
- Associar artigo a cluster: `backend/crud.py::associate_artigo_to_cluster`
- Buscar feed por data: `backend/main.py::get_feed`
- Upload via API: `backend/main.py::upload_file_endpoint` e progresso em `upload_progress`
- Processar pendentes (admin): `backend/main.py::processar_artigos_pendentes` ‚Üí chama fun√ß√µes do pipeline

## Agente Estagi√°rio (beta)

- O que √©: agente de consulta sobre as not√≠cias do dia. Reusa ORM/CRUD do backend e pode sintetizar respostas em Markdown quando `GEMINI_API_KEY` estiver configurada.
- Modo ReAct (opcional): defina `ESTAGIARIO_REACT=1` no ambiente para ativar o executor que orquestra ferramentas formais (`agents/estagiario/tools`).
- Endpoints:
  - `POST /api/estagiario/start` ‚Äî inicia sess√£o de chat do dia.
  - `POST /api/estagiario/send` ‚Äî envia pergunta e retorna resposta do agente.
  - `GET /api/estagiario/messages/{session_id}` ‚Äî hist√≥rico de mensagens.

---

## v2.0 - Arquitetura Graph-RAG Integrada

A v2.0 integra um **Grafo de Conhecimento** e **Embeddings Semanticos** diretamente nas 4 etapas do pipeline principal. Nao e mais um modo sombra separado ‚Äî o motor v2 e o motor principal.

### Componentes

| Componente                      | Arquivo                             | Descricao                                             |
| ------------------------------- | ----------------------------------- | ----------------------------------------------------- |
| **Grafo de Conhecimento** | `backend/database.py`             | Tabelas `graph_entities` + `graph_edges`          |
| **CRUD do Grafo**         | `backend/agents/graph_crud.py`    | Entity Resolution, arestas, queries temporais, contexto |
| **Nos Agenticos**         | `backend/agents/nodes.py`         | Gatekeeper, NER, Entity Resolution, Historian, Writer |
| **Workflow LangGraph**    | `backend/workflow.py`             | StateGraph (disponivel para debug/comparacao)         |
| **Enriquecimento v2**     | `process_articles.py::enriquecer_artigo_v2` | Embedding + NER + Grafo por artigo (Etapa 1) |
| **Backfill**              | `scripts/backfill_graph.py`       | Popula grafo com dados historicos                     |

### Como o v2 funciona em cada etapa

| Etapa | O que o v2 adiciona | Funcao chave |
|-------|---------------------|--------------|
| **Etapa 1** | Gera `embedding_v2` (768d Gemini) + extrai entidades via NER + persiste no grafo | `enriquecer_artigo_v2()` |
| **Etapa 2** | Calcula similaridade cosseno entre artigos e injeta DICAS DE SIMILARIDADE no prompt | `cosine_similarity_bytes()` |
| **Etapa 3** | Busca contexto historico do grafo (7 dias) e artigos similares (30 dias), injeta no prompt | `get_context_for_cluster()` |
| **Etapa 4** | Calcula embedding medio por cluster e identifica pares similares para sugerir merges | `cosine_similarity_bytes()` |

### Setup Rapido

```bash
# 1. Instalar dependencias (se ainda nao instaladas)
pip install langgraph

# 2. Migrar banco de dados (criar tabelas do grafo)
python scripts/migrate_graph_tables.py

# 3. Popular grafo com dados historicos (uma vez)
python scripts/backfill_graph.py --days 30 --batch 50

# 4. Rodar pipeline completo (v2 integrado)
python run_complete_workflow.py
```

### Variaveis de Ambiente

| Variavel           | Padrao | Descricao                                                  |
| ------------------ | ------ | ---------------------------------------------------------- |
| `V2_SHADOW_MODE` | `0`  | Se `1`, roda workflow LangGraph completo apos pipeline (debug) |
| `GEMINI_API_KEY` | -    | Chave da API Gemini (obrigatoria para v2)                  |

### Degradacao Graciosa

Se o Gemini estiver indisponivel ou a chave nao configurada, o pipeline funciona identicamente a v1 (sem embeddings, sem grafo, sem contexto historico). Nenhuma etapa falha ‚Äî apenas perde o enriquecimento v2.
