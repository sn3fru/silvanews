## BTG AlphaFeed

Plataforma de intelig√™ncia de mercado que transforma alto volume de not√≠cias em um feed orientado a eventos (clusters) para Special Situations.

### O que √© (vis√£o executiva)
- Identifica o fato gerador por tr√°s de m√∫ltiplas not√≠cias e consolida em um √∫nico evento.
- Classifica por prioridade (P1 cr√≠tico, P2 estrat√©gico, P3 monitoramento) e por categoria tem√°tica (tags).
- Gera resumos executivos no tamanho certo por prioridade (P1 longo, P2 m√©dio, P3 curto).
- Oferece visualiza√ß√£o por data, filtros e drill-down para as fontes originais.

### Como funciona na pr√°tica
- Carregue PDFs/JSONs de crawlers (upload manual ou via API) ‚Üí s√£o salvos como artigos brutos.
- O processamento orquestrado agrupa por fato gerador, classifica e gera resumos.
- O frontend exibe o feed por data, com filtros de prioridade e tags din√¢micas vindas dos dados reais.

### Novidades recentes (pipeline mais rigoroso)
- Endurecimento do `PROMPT_EXTRACAO_PERMISSIVO_V8`: lista de rejei√ß√£o ampliada (crimes comuns, casos pessoais, fofoca/entretenimento, esportes, pol√≠tica partid√°ria, efem√©rides e programas sociais sem tese) e gating P1/P2/P3 mais duro.
- Prioriza√ß√£o Executiva Final integrada: etapa adicional p√≥s-resumo/p√≥s-agrupamento que reclassifica como P1/P2/P3/IRRELEVANTE com justificativa e a√ß√£o recomendada (`PROMPT_PRIORIZACAO_EXECUTIVA_V1`).
- O fluxo autom√°tico acionado pelo bot√£o ‚ÄúProcessar Artigos Pendentes‚Äù tamb√©m aplica essa prioriza√ß√£o final.

### Filtros e visualiza√ß√£o (frontend)
- Seletor de data no topo: alterna entre hoje e datas hist√≥ricas (tudo GMT-3).
- Filtros: prioridade (P1/P2/P3) e tags din√¢micas (derivadas dos clusters reais).
- P3 Monitoramento: cards consolidados por tag com lista em bullets; clique abre modal com detalhes.
- Deep-dive: modal por evento com resumo, fontes e abas (chat com cluster e gerenciamento).

### Tags e Prioridades (como configurar)
- Tags oficiais: editar `backend/prompts.py` em `TAGS_SPECIAL_SITUATIONS` (√∫nica fonte da verdade).
- Prioridades: editar `LISTA_RELEVANCIA_HIERARQUICA` no mesmo arquivo para ajustar crit√©rios e exemplos.
- O mapeamento determin√≠stico assunto ‚ûú (prioridade, tag) √© gerado a partir dessas estruturas.

### Pipeline (passo a passo)
1) Ingest√£o: `load_news.py` l√™ PDFs/JSONs e grava artigos brutos (status `pendente`).
2) Processamento inicial: extrai dados, gera embeddings e marca `pronto_agrupar`.
3) Agrupamento: cria/atualiza clusters por fato gerador (modo em lote ou incremental autom√°tico).
4) Classifica√ß√£o e Resumos: define prioridade/tag e gera resumo no tamanho certo.
5) Prioriza√ß√£o Executiva Final: reclassifica rigidamente P1/P2/P3/IRRELEVANTE e ajusta decis√£o final.
6) Exposi√ß√£o: API `FastAPI` alimenta o frontend; CRUD e endpoints admin.

```mermaid
graph TD
  A[Upload PDFs/JSON<br/>load_news.py] --> B[(PostgreSQL<br/>artigos_brutos: pendente)]
  B --> C[process_articles.py<br/>Etapa 1: processar artigos]
  C --> D[(artigos_brutos: pronto_agrupar)]
  D --> E[process_articles.py<br/>Etapa 2: agrupar (pivot auto))]
  E --> F[Etapa 3: classificar e resumir]
  F --> F2[Etapa 4: prioriza√ß√£o executiva]
  F2 --> G[(clusters_eventos + resumos + prioridades finais)]
  G --> H[FastAPI /api/feed]
  H --> I[Frontend /frontend]
  J[Admin: upload-file<br/>processar-pendentes] --> H
```

### LLMs e prompts (o que roda e para qu√™)
- Extra√ß√£o e triagem inicial: `PROMPT_EXTRACAO_PERMISSIVO_V8`.
- Agrupamento em lote: `PROMPT_AGRUPAMENTO_V1`.
- Agrupamento incremental (novas not√≠cias do dia): `PROMPT_AGRUPAMENTO_INCREMENTAL_V2` (contexto enriquecido com `titulos_internos`).
- Resumo executivo por prioridade: `PROMPT_RESUMO_FINAL_V3` e `PROMPT_RADAR_MONITORAMENTO_V1` (bullets P3).
- Sanitiza√ß√£o (gatekeeper): `PROMPT_SANITIZACAO_CLUSTER_V1`.
- Chat com cluster: `PROMPT_CHAT_CLUSTER_V1`.
- Prioriza√ß√£o executiva (p√≥s-pipeline): `PROMPT_PRIORIZACAO_EXECUTIVA_V1`.

Prompts opcionais/POC (n√£o usados no pipeline padr√£o):
- `PROMPT_RESUMO_CRITICO_V1` (POC de resumo cr√≠tico)
- `PROMPT_RADAR_MONITORAMENTO_V1` (POC de bullets de radar P3)
- `PROMPT_AGRUPAMENTO_INCREMENTAL_V1` (substitu√≠do por `V2` no pipeline)
- `PROMPT_SANITIZACAO_CLUSTER_V1` (dispon√≠vel como segunda linha de defesa; n√£o integrado por padr√£o)
- `PROMPT_EXTRACAO_JSON_V1` (alias de `PROMPT_EXTRACAO_PERMISSIVO_V8`, mantido para compatibilidade)
- Onde ajustar: `backend/prompts.py` (textos, tags, prioridades). API key via `backend/.env` (`GEMINI_API_KEY`).

### Arquitetura em 1 minuto
- Backend FastAPI + SQLAlchemy (PostgreSQL 5433)
- Frontend est√°tico em `backend/frontend`
- Orquestra√ß√£o por scripts CLI para ingest√£o e processamento

## Guia R√°pido

### Pr√©-requisitos
- Anaconda instalado; usar o ambiente `pymc2`
- PostgreSQL local ativo na porta `5433`
- `backend/.env` com vari√°veis m√≠nimas:
  ```env
  DATABASE_URL="postgresql+psycopg2://postgres_local@localhost:5433/devdb"
  GEMINI_API_KEY="<sua_chave_gemini>"
  ```

### 1) Ativar ambiente e entrar no projeto
```bash
conda activate pymc2
cd "C:\Users\marcos.silva\OneDrive - ENFORCE GESTAO DE ATIVOS S.A\jupyter\projetos\novo-topnews\pdfs\silva-front\btg_alphafeed"
```

### 2) Comandos essenciais
- Upload manual de PDFs/JSON (diret√≥rio completo):
  ```bash
  python load_news.py --dir ../pdfs --direct --yes
  ```
- Upload de um arquivo espec√≠fico:
```bash
  python load_news.py --file ../pdfs/arquivo.pdf --direct --yes
  ```
- Processar artigos (extra√ß√£o ‚Üí agrupamento ‚Üí resumos):
  ```bash
  python process_articles.py
  ```
- Iniciar o backend (use apenas se n√£o houver outro servidor rodando):
  ```bash
  python start_dev.py
  ```
- Acesso r√°pido: Frontend `http://localhost:8000/frontend` | Docs `http://localhost:8000/docs` | Health `http://localhost:8000/health`

### Estimativa de Custos (LLM)
Para uma estimativa r√°pida de custos por etapa do pipeline, execute:
```bash
python estimativa_custos.py
```
O script simula tokens de entrada/sa√≠da por etapa usando as not√≠cias no banco e imprime um comparativo por cen√°rio de modelos.

## Sincronizar Banco Local ‚Üí Heroku (Incremental)
Rodar a partir da pasta `silva-front` ou `btg_alphafeed`:
  ```bash
  conda activate pymc2
  python -m btg_alphafeed.migrate_incremental \
    --source "postgresql+psycopg2://postgres_local@localhost:5433/devdb" \
    --dest   "postgres://<usuario>:<senha>@<host>:5432/<db>" \
    --include-logs --include-chat
```
- Op√ß√µes √∫teis:
  - `--no-update-existing`: n√£o atualiza registros existentes (apenas insere novos)
  - `--since <ISO-UTC>`: inicia a partir de um timestamp espec√≠fico
  - `--meta-file btg_alphafeed/last_migration.txt`: armazena o √∫ltimo timestamp migrado
  - `--only clusters,artigos,logs`: restringe entidades

### Replace do dia (produ√ß√£o) com reprocessamento local
Use quando voc√™ reprocessou o dia localmente (novos prompts) e quer substituir o dia em produ√ß√£o de forma limpa (sem manter clusters antigos do mesmo dia).

Passo a passo:
1. Reprocessar localmente o dia (mant√©m brutos, limpa processados de hoje e reprocessa o pipeline completo):
   ```bash
   conda activate pymc2
   cd btg_alphafeed
   python reprocess_today.py
   ```

2. Replace no destino (produ√ß√£o) APENAS do dia desejado:
   ```bash
   conda activate pymc2
   cd btg_alphafeed
   python migrate_replace_today.py \
     --source "postgresql+psycopg2://postgres_local@localhost:5433/devdb" \
     --dest   "postgres://<usuario>:<senha>@<host>:5432/<db>" \
     --day $(python -c "from datetime import date; print(date.today().isoformat())")
   ```
   O utilit√°rio limpa no destino os dados do dia (desassocia artigos, remove clusters/altera√ß√µes/chat/s√≠ntese do dia) e migra do origem os clusters/artigos/s√≠ntese do mesmo dia, for√ßando `cluster_id` dos artigos.

3. (Opcional) Rodar incremental normal para demais dias/logs/chat:
   ```bash
   python -m btg_alphafeed.migrate_incremental \
     --source "postgresql+psycopg2://postgres_local@localhost:5433/devdb" \
     --dest   "postgres://<usuario>:<senha>@<host>:5432/<db>" \
     --include-logs --include-chat
   ```

## Tarefas Comuns
- Reexecutar pipeline completo (ingest√£o + processamento):
  ```bash
  python load_news.py --dir ../pdfs --direct --yes && python process_articles.py
  ```
- Migra√ß√£o completa (one-shot):
  ```bash
  python -m btg_alphafeed.migrate_databases --source "postgresql+psycopg2://postgres_local@localhost:5433/devdb" --dest "postgres://<usuario>:<senha>@<host>:5432/<db>"
  ```
- Limpar banco (script interativo, com backup):
  ```bash
  python limpar_banco.py
  ```
- Ver estado da API e DB:
  ```bash
  curl http://localhost:8000/health
  ```

### Novo (Prot√≥tipo) ‚Äì An√°lise de Feedback para Ajuste de Prompt
- Agora √© poss√≠vel registrar like/dislike de not√≠cias diretamente no feed (üëç/üëé ao lado do t√≠tulo de cada card). O backend agrega esse feedback por cluster e exp√µe no `GET /api/feed` dentro do campo `feedback` de cada item: `{ likes, dislikes, last }`.
- Foi adicionado um prot√≥tipo de an√°lise de feedback que sugere ajustes no prompt de agrupamento sem alterar os arquivos em produ√ß√£o. Ele gera um relat√≥rio com o diff do prompt proposto.

Rodar o prot√≥tipo de an√°lise de feedback e gerar diff do prompt:
```bash
conda activate pymc2
python analisar_feedback_prompt.py --limit 200 --output reports/prompt_diff_feedback.md
```
Sa√≠da esperada:
- Arquivo `reports/prompt_diff_feedback.md` com:
  - Prompt atual
  - Prompt proposto (adiciona addendum baseado em padr√µes de like/dislike)
  - Diff (unified) entre os dois para revis√£o humana

Importante: este processo n√£o altera `backend/prompts.py`. √â apenas para estudo e valida√ß√£o.

## Playbooks (cen√°rios prontos)

### 1) Rodar o dia do zero (ingest√£o PDFs/JSON + processamento + backend)
```bash
conda activate pymc2
cd btg_alphafeed
python load_news.py --dir ../pdfs --direct --yes
python process_articles.py
# iniciar backend somente se necess√°rio
python start_dev.py
```

### 2) Apenas ingest√£o de JSON (sem LLM) e processamento
```bash
conda activate pymc2
cd btg_alphafeed
python load_news.py --dir ../pdfs --direct --yes
python process_articles.py
```

### 3) Reprocessar apenas pendentes j√° carregados
```bash
conda activate pymc2
cd btg_alphafeed
python process_articles.py
```

### 4) Ingest√£o incremental de PDFs durante o dia + agrupamento incremental
```bash
conda activate pymc2
cd btg_alphafeed
# novas p√°ginas/PDFs na pasta ../pdfs
python load_news.py --dir ../pdfs --direct --yes
python process_articles.py
```

### 5) Sincronizar local ‚Üí Heroku (incremental, com logs e chat)
```bash
conda activate pymc2
cd silva-front
python -m btg_alphafeed.migrate_incremental \
  --source "postgresql+psycopg2://postgres_local@localhost:5433/devdb" \
  --dest   "postgres://<usuario>:<senha>@<host>:5432/<db>" \
  --include-logs --include-chat
```

### 6) Verificar data sem dados (sanidade)
```bash
curl "http://localhost:8000/api/feed?data=2099-01-01"
# Esperado: m√©tricas zeradas e lista vazia (sem dados de teste)
```

## Regras de Neg√≥cio e Conven√ß√µes
- Tags: usar apenas as definidas em `backend/prompts.py` (`TAGS_SPECIAL_SITUATIONS`)
- JSONs de crawlers: ingest√£o direta; LLM s√≥ na fase de agrupamento/s√≠ntese
- `process_articles.py`: orquestra e chama a l√≥gica do backend; sem regras de neg√≥cio pr√≥prias
- Status dos artigos: `pendente` ‚Üí `pronto_agrupar` ‚Üí `processado`
- Prioridades e resumos: P1 (longo), P2 (m√©dio), P3 (curto)

## Endpoints principais
- `GET /api/feed?data=YYYY-MM-DD`
  - Cada item do feed inclui `feedback: { likes, dislikes, last }` agregados por cluster
- `POST /admin/processar-pendentes`
- `POST /api/admin/upload-file` e `GET /admin/upload-progress/{file_id}`
- `GET /health`
- Frontend servido em `/frontend`

### Endpoints de BI e Feedback
- `GET /api/bi/series-por-dia?dias=30`
- `GET /api/bi/noticias-por-fonte?limit=20`
- `GET /api/bi/noticias-por-autor?limit=20`
- `POST /api/feedback?artigo_id=<id>&feedback=like|dislike`
- `GET /api/feedback?processed=`
- `POST /api/feedback/{id}/process`

## Dicas de Ambiente
- Sempre usar Anaconda Prompt e `conda activate pymc2`
- Porta padr√£o do Postgres local: `5433`
- Evite subir o backend se j√° existir um servidor rodando

## Troubleshooting (r√°pido)
- Sem `GEMINI_API_KEY`: PDFs s√£o ingeridos como 1 artigo por p√°gina (fallback)
- Conex√£o DB falhando: confirme porta `5433` e `DATABASE_URL`
- Sem artigos pendentes: rode primeiro o upload (`load_news.py`)
- API offline: use o modo `--direct` do `load_news.py`

## Testes
```bash
python tests/test_imports.py
python test_fluxo_completo.py
```

## Seguran√ßa
- Nunca commitar `.env` ou credenciais
- Evite colar URLs de produ√ß√£o com usu√°rio/senha em documentos/commits

## Formatos suportados (resumo)
- JSON de crawlers: campos t√≠picos `id_hash`, `titulo`, `texto_completo`, `link`, `fonte`, `data_publicacao`
- PDF: OCR + LLM quando dispon√≠vel; fallback 1 artigo por p√°gina

## Refer√™ncias r√°pidas
- Upload: `python load_news.py --dir ../pdfs --direct --yes`
- Processar: `python process_articles.py`
- Backend: `python start_dev.py`
- Sync Heroku: `python -m btg_alphafeed.migrate_incremental --source "postgresql+psycopg2://postgres_local@localhost:5433/devdb" --dest "postgres://<usuario>:<senha>@<host>:5432/<db>" --include-logs --include-chat`

---

## Documenta√ß√£o para LLMs (manuten√ß√£o e navega√ß√£o do c√≥digo)

### Mapa de pastas (o que cada uma faz)
- `btg_alphafeed/backend/main.py`: aplica√ß√£o FastAPI, rotas principais (`/api/feed`, admin, upload, health), serve frontend e orquestra background tasks.
- `btg_alphafeed/backend/database.py`: engine, sess√µes, modelos SQLAlchemy, helpers de conex√£o e utilidades de schema.
- `btg_alphafeed/backend/crud.py`: opera√ß√µes de banco de dados (artigos, clusters, associa√ß√µes, atualiza√ß√µes de status, m√©tricas, logs).
- `btg_alphafeed/backend/processing.py`: pipeline de processamento unit√°rio (embeddings, similaridade, processamento de artigo, resumo de cluster, utilit√°rios de cluster).
- `btg_alphafeed/backend/models.py`: modelos Pydantic para request/response.
- `btg_alphafeed/backend/prompts.py`: fonte da verdade de `TAGS_SPECIAL_SITUATIONS`, `LISTA_RELEVANCIA_HIERARQUICA` e todos os prompts de LLM.
- `btg_alphafeed/backend/utils.py`: utilidades gerais (datas GMT-3, etc.).
- `btg_alphafeed/backend/collectors/file_loader.py`: classe `FileLoader` usada por `load_news.py` para ingerir PDFs/JSONs.
- `btg_alphafeed/frontend/index.html|script.js|style.css`: UI do feed, seletor de data, modal de deep-dive, filtros din√¢micos.
- `btg_alphafeed/frontend/settings.html|settings.js`: UI administrativa/CRUD e opera√ß√µes de manuten√ß√£o, com abas de BI (s√©ries por dia, por fonte e por autor) e Feedback (like/dislike por artigo, com marca√ß√£o de processado).
- `btg_alphafeed/load_news.py`: CLI para ingest√£o de PDFs/JSONs (salva artigos brutos, com `--direct` para DB).
- `btg_alphafeed/process_articles.py`: orquestrador do pipeline (processar ‚Üí agrupar ‚Üí classificar/resumir). N√£o conter regra de neg√≥cio pr√≥pria.
- `btg_alphafeed/migrate_incremental.py`: sync incremental local ‚Üí Heroku (idempotente, com filtros `--only`, `--include-logs`, `--include-chat`).
- `btg_alphafeed/migrate_databases.py`: migra√ß√£o completa (one-shot) local ‚Üí Heroku.
- `btg_alphafeed/limpar_banco.py`: limpeza seletiva com backup.

### Fluxos cr√≠ticos (pontos de extens√£o)
- Ingest√£o: estender `backend/collectors/file_loader.py` para novas fontes/formatos.
- Classifica√ß√£o/Tags: alterar apenas `backend/prompts.py` e manter coer√™ncia com `TAGS_SPECIAL_SITUATIONS`.
- Prioridade/Resumo: `backend/prompts.py` (PROMPT_RESUMO_*), tamanho conforme P1/P2/P3.
- Agrupamento incremental: `PROMPT_AGRUPAMENTO_INCREMENTAL_V2` (usa `titulos_internos` dos artigos de cada cluster no payload) e l√≥gica de pivot autom√°tico em `process_articles.py`.
- API/Endpoints: adicionar rotas em `backend/main.py` e delegar CRUD para `backend/crud.py`.

### Regras de contribui√ß√£o (para evitar duplica√ß√£o e l√≥gica fora do lugar)
- Orquestra√ß√£o em scripts (CLI): `load_news.py` e `process_articles.py` apenas chamam o backend.
- L√≥gica de neg√≥cio: `backend/processing.py` + `backend/crud.py` (nunca em CLI nem em rotas diretamente).
- Esquema de dados (DB): `backend/database.py` (modelos/relacionamentos) e migrations externas quando necess√°rio.
- Prompts/taxonomia: `backend/prompts.py` (fonte √∫nica). O frontend consome tags vindas dos dados reais (sem listas fixas).

### Consulta r√°pida de responsabilidades
- Gerar resumo de um cluster: `backend/processing.py::gerar_resumo_cluster`
- Associar artigo a cluster: `backend/crud.py::associate_artigo_to_cluster`
- Buscar feed por data: `backend/main.py::get_feed`
- Upload via API: `backend/main.py::upload_file_endpoint` e progresso em `upload_progress`
- Processar pendentes (admin): `backend/main.py::processar_artigos_pendentes` ‚Üí chama fun√ß√µes do pipeline

