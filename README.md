# BTG AlphaFeed - Plataforma de Intelig√™ncia de Mercado

Sistema integrado de processamento e an√°lise de not√≠cias em tempo real para a mesa de Special Situations do BTG Pactual, seguindo o fluxo de neg√≥cio AlphaFeed.

## üöÄ **COMO USAR - Guia R√°pido**

### **Pr√©-requisitos**
1. **Anaconda instalado** no sistema
2. **Ambiente pymc2** criado e configurado
3. **Banco PostgreSQL** rodando na porta 5433
4. **Chave da API Gemini** configurada

### **Passo 1: Abrir Anaconda Prompt**
- Pressione `Win + R`
- Digite `anaconda prompt` e pressione Enter
- Ou procure por "Anaconda Prompt" no menu Iniciar

### **Passo 2: Ativar Ambiente e Navegar**
```bash
# Ativar ambiente
conda activate pymc2

# Navegar para o diret√≥rio
cd "C:\Users\marcos.silva\OneDrive - ENFORCE GESTAO DE ATIVOS S.A\jupyter\projetos\novo-topnews\pdfs\silva-front\btg_alphafeed"
```

### **Passo 3: Configurar Banco de Dados**
Crie um arquivo `.env` no diret√≥rio `backend/`:
```env
DATABASE_URL="postgresql://postgres_local@localhost:5433/devdb"
GEMINI_API_KEY="sua_chave_api_gemini"
```

### **Passo 4: Executar Fluxo Completo**
```bash
# Op√ß√£o A: Fluxo automatizado (Recomendado)
python run_complete_workflow.py

# Op√ß√£o B: Execu√ß√£o manual por etapas
python load_news.py --dir ../pdfs --direct --yes  # Carregar artigos brutos
python process_articles.py                        # Processar artigos (extra√ß√£o, agrupamento, resumos)
python start_dev.py                               # Iniciar backend
```

**Nota**: O `load_news.py` agora apenas **carrega artigos brutos** no banco (status: pendente). Todo o processamento (extra√ß√£o de dados, agrupamento, resumos) √© feito pelo `process_articles.py`.

### **Passo 5: Acessar o Sistema**
- **Frontend**: http://localhost:8000/frontend
- **API Docs**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/health

### **Passo 6: Usar o Seletor de Data**
- **Data Atual**: Por padr√£o, mostra dados de hoje
- **Data Hist√≥rica**: Clique no bot√£o de data para selecionar outra data
- **Filtro Autom√°tico**: O sistema filtra automaticamente not√≠cias, clusters e resumos da data selecionada

---

## üéØ **Vis√£o Geral da Plataforma**

O BTG AlphaFeed transforma o alto volume de not√≠cias n√£o estruturadas em um feed de eventos de neg√≥cio claros, priorizados e acion√°veis. Em vez de apresentar uma lista cronol√≥gica de artigos, a plataforma identifica o "fato gerador" por tr√°s das not√≠cias, agrupa todas as perspectivas sobre aquele fato e o apresenta como um √∫nico evento coeso.

### **Fluxo de Neg√≥cio (5 Fases)**

#### 1. **Coleta Universal de Artigos Brutos**
- **Fontes**: Crawlers Web, Telegram, PDFs, JSONs
- **Processamento**: 
  - **PDFs**: Extra√ß√£o inteligente de not√≠cias individuais (comportamento padr√£o)
  - **JSONs**: Processamento direto (j√° vem pr√©-processado)
- **Reposit√≥rio**: Central √∫nico de "artigos brutos"
- **Resultado**: Artigos com status "pendente" no banco

#### 2. **Detec√ß√£o e Agrupamento Din√¢mico de Eventos**
- **Processamento em Lote**: Processa todas as not√≠cias primeiro
- **Agrupamento Inteligente**: Usa LLM para agrupar por fato gerador (inspirado no `silva.py`)
- **Cria√ß√£o de Clusters**: Agrupa por fato gerador ap√≥s processamento
- **Resultado**: Clusters de eventos com artigos associados

#### 3. **Triagem e Classifica√ß√£o de Prioridade**
- **P1 - Cr√≠tico**: Fal√™ncias, M&A direto, crises de liquidez
- **P2 - Estrat√©gico**: Mudan√ßas regulat√≥rias, disputas corporativas
- **P3 - Monitoramento**: Contexto macro, tend√™ncias setoriais

#### 4. **Gera√ß√£o Seletiva de Resumos Executivos**
- **P1**: Resumo longo (par√°grafo completo, sem limites)
- **P2**: Resumo m√©dio (3-4 frases, com limite)
- **P3**: Resumo curto (uma frase)

#### 5. **Apresenta√ß√£o e Intera√ß√£o**
- **Feed Orientado a Eventos**: Clusters ordenados por prioridade
- **Seletor de Data**: Visualiza√ß√£o hist√≥rica por data
- **Rastreabilidade Total**: Acesso aos artigos originais
- **Drill-Down**: Clique no evento ‚Üí ver artigos fonte

---

## üèóÔ∏è **Arquitetura Detalhada**

### **Estrutura do Projeto**
```
btg_alphafeed/
‚îú‚îÄ‚îÄ backend/                    # API FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ main.py                # Endpoints principais (950 linhas)
‚îÇ   ‚îú‚îÄ‚îÄ models.py              # Modelos Pydantic
‚îÇ   ‚îú‚îÄ‚îÄ database.py            # Configura√ß√£o SQLAlchemy (285 linhas)
‚îÇ   ‚îú‚îÄ‚îÄ crud.py                # Opera√ß√µes de banco
‚îÇ   ‚îú‚îÄ‚îÄ processing.py          # L√≥gica de processamento (559 linhas)
‚îÇ   ‚îú‚îÄ‚îÄ prompts.py             # Prompts para LLM
‚îÇ   ‚îú‚îÄ‚îÄ utils.py               # Fun√ß√µes auxiliares
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt       # Depend√™ncias Python
‚îÇ   ‚îú‚îÄ‚îÄ .env                   # Configura√ß√µes (invis√≠vel)
‚îÇ   ‚îî‚îÄ‚îÄ collectors/            # Coletores de dados
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ exemplo_coletor.py # Exemplo de coletor
‚îÇ       ‚îî‚îÄ‚îÄ file_loader.py     # Carregador de arquivos
‚îú‚îÄ‚îÄ frontend/                  # Interface web
‚îÇ   ‚îú‚îÄ‚îÄ index.html             # Interface principal (154 linhas)
‚îÇ   ‚îú‚îÄ‚îÄ script.js              # L√≥gica frontend (456 linhas)
‚îÇ   ‚îú‚îÄ‚îÄ style.css              # Estilos CSS
‚îÇ   ‚îî‚îÄ‚îÄ settings.html          # Interface de configura√ß√µes
‚îú‚îÄ‚îÄ tests/                     # Scripts de teste
‚îÇ   ‚îî‚îÄ‚îÄ test_imports.py        # Teste de importa√ß√µes
‚îú‚îÄ‚îÄ README.md                  # Este arquivo
‚îú‚îÄ‚îÄ load_news.py               # Script de carregamento (135 linhas)
‚îú‚îÄ‚îÄ process_articles.py        # Script de processamento (398 linhas)
‚îú‚îÄ‚îÄ run_complete_workflow.py   # Fluxo automatizado (185 linhas)
‚îú‚îÄ‚îÄ test_fluxo_completo.py     # Teste completo (164 linhas)
‚îú‚îÄ‚îÄ test_ajustes.py            # Teste das altera√ß√µes (200+ linhas)
‚îú‚îÄ‚îÄ start_dev.py               # Script de inicializa√ß√£o (135 linhas)
‚îî‚îÄ‚îÄ limpar_banco.py            # Utilit√°rio de limpeza (278 linhas)
```

### **Tecnologias Utilizadas**

#### **Backend**
- **FastAPI**: Framework web moderno e r√°pido
- **SQLAlchemy**: ORM para PostgreSQL
- **PostgreSQL**: Banco de dados principal
- **Google Gemini API**: LLM para processamento
- **Pydantic**: Valida√ß√£o de dados
- **Uvicorn**: Servidor ASGI

#### **Frontend**
- **Vanilla JavaScript**: L√≥gica client-side
- **CSS Grid/Flexbox**: Layout responsivo
- **HTML5**: Estrutura sem√¢ntica
- **Fetch API**: Comunica√ß√£o com backend

#### **Processamento**
- **Embeddings**: Vetores de similaridade (implementa√ß√£o simples)
- **Cosine Similarity**: M√©trica de similaridade
- **JSON Processing**: Extra√ß√£o e valida√ß√£o de dados
- **Background Tasks**: Processamento ass√≠ncrono

---

## üìä **Estrutura do Banco de Dados**

### **Tabelas Principais**

#### **artigos_brutos** - Artigos Coletados
- `id`, `hash_unico`, `texto_bruto`, `url_original`, `fonte_coleta`
- `status` (pendente, processado, irrelevante, erro)
- `titulo_extraido`, `texto_processado`, `jornal`, `autor`
- `tag`, `prioridade`, `relevance_score`, `embedding`, `cluster_id`
- **√çndices**: status+created_at, hash_unico, fonte_coleta+created_at, tag+prioridade

#### **clusters_eventos** - Agrupamentos de Eventos
- `id`, `titulo_cluster`, `resumo_cluster`, `tag`, `prioridade`
- `embedding_medio`, `status`, `total_artigos`
- **√çndices**: status+created_at, tag+prioridade

#### **sinteses_executivas** - S√≠nteses Di√°rias
- `id`, `data_sintese`, `texto_sintese`
- M√©tricas: total_noticias_coletadas, total_eventos_unicos, etc.

#### **logs_processamento** - Logs do Sistema
- `id`, `timestamp`, `nivel`, `componente`, `mensagem`, `detalhes`
- **√çndices**: timestamp, nivel+componente

#### **configuracoes_coleta** - Configura√ß√µes dos Coletores
- `id`, `nome_coletor`, `ativo`, `configuracao`
- Controle de execu√ß√£o: ultima_execucao, proxima_execucao, intervalo_minutos

#### **chat_sessions** - Sess√µes de Chat
- `id`, `cluster_id`, `created_at`, `updated_at`
- **Relacionamentos**: cluster, messages
- **√çndices**: cluster_id, created_at, updated_at

#### **chat_messages** - Mensagens do Chat
- `id`, `session_id`, `role`, `content`, `timestamp`
- **Relacionamentos**: session
- **√çndices**: session_id, timestamp, role

#### **cluster_alteracoes** - Hist√≥rico de Altera√ß√µes
- `id`, `cluster_id`, `campo_alterado`, `valor_anterior`, `valor_novo`, `motivo`, `usuario`, `timestamp`
- **Relacionamentos**: cluster
- **√çndices**: cluster_id, timestamp, campo_alterado, usuario

---

## üì° **Endpoints da API**

### **Principais**
- `GET /api/feed` - Feed principal do frontend (suporta par√¢metro `data`)
- `GET /api/cluster/{id}` - Detalhes de um cluster

### **Chat e Intera√ß√£o**
- `POST /api/chat/send` - Envia mensagem para chat de um cluster
- `GET /api/chat/{cluster_id}/messages` - Obt√©m mensagens de uma sess√£o de chat

### **Gerenciamento de Clusters**
- `PUT /api/cluster/{cluster_id}/update` - Atualiza prioridade e tags de um cluster
- `GET /api/cluster/{cluster_id}/alteracoes` - Obt√©m hist√≥rico de altera√ß√µes de um cluster
- `GET /api/admin/alteracoes` - Obt√©m todas as altera√ß√µes recentes (endpoint administrativo)

### **Internos (para coletores)**
- `POST /internal/novo-artigo` - Criar novo artigo
- `POST /internal/processar-artigo` - Processar artigo

### **Administra√ß√£o**
- `GET /admin/stats` - Estat√≠sticas do sistema
- `POST /admin/processar-pendentes` - Processar artigos pendentes
- `POST /admin/gerar-resumo/{cluster_id}` - Gerar resumo de cluster
- `POST /admin/carregar-arquivos` - Carregar not√≠cias de arquivos
- `POST /admin/upload-file` - Upload de arquivos PDF/JSON
- `GET /admin/upload-progress/{file_id}` - Verificar progresso do upload em tempo real
- `POST /admin/process-articles` - Processar artigos pendentes (equivalente ao process_articles.py)
- `GET /admin/processing-status` - Verificar status do processamento

### **Settings (CRUD Completo)**
- `GET /api/settings/artigos` - Listar artigos com pagina√ß√£o
- `GET /api/settings/artigos/{id}` - Detalhes de artigo
- `PUT /api/settings/artigos/{id}` - Atualizar artigo
- `DELETE /api/settings/artigos/{id}` - Excluir artigo
- `GET /api/settings/clusters` - Listar clusters
- `GET /api/settings/clusters/{id}` - Detalhes de cluster
- `PUT /api/settings/clusters/{id}` - Atualizar cluster
- `DELETE /api/settings/clusters/{id}` - Excluir cluster
- `GET /api/settings/sinteses` - Listar s√≠nteses
- `GET /api/settings/sinteses/{id}` - Detalhes de s√≠ntese
- `PUT /api/settings/sinteses/{id}` - Atualizar s√≠ntese
- `DELETE /api/settings/sinteses/{id}` - Excluir s√≠ntese

### **Sa√∫de**
- `GET /health` - Status do sistema

---

## üé® **Interface Frontend**

### **Componentes Principais**

#### **Seletor de Data (Novo)**
- **Localiza√ß√£o**: Topo do frontend, acima das m√©tricas
- **Funcionalidade**: Permite selecionar data espec√≠fica para visualiza√ß√£o
- **Estados**: 
  - Data atual (padr√£o)
  - Data hist√≥rica (destaque visual)
- **Filtro**: Busca automaticamente not√≠cias, clusters e resumos da data selecionada

#### **Painel de Controle (Sidebar)**
- **Logo**: BTG AlphaFeed
- **Filtros de Prioridade**: P1 Cr√≠tico, P2 Estrat√©gico, P3 Monitoramento
- **Mesas de An√°lise**: Special Situations, D√≠vida Ativa, Crise de Liquidez, M&A Setor El√©trico
- **Filtros de Categoria**: Governo & Pol√≠tica, Economia & Tecnologia, Judici√°rio, Empresas Privadas

#### **Painel de M√©tricas (Header)**
- **Not√≠cias Coletadas**: Total de artigos da data selecionada
- **Eventos √önicos**: Total de clusters ativos da data
- **An√°lises Cr√≠ticas (P1)**: Clusters de prioridade cr√≠tica
- **Em Monitoramento (P2+P3)**: Clusters estrat√©gicos e de monitoramento
- **Settings**: Bot√£o para acessar configura√ß√µes

#### **Feed de Not√≠cias**
- **S√≠ntese Executiva**: Resumo da data selecionada
- **Cards de Clusters**: Cada card representa um evento √∫nico
  - **P1/P2**: Cards individuais com t√≠tulo, fontes, resumo e bot√£o "Aprofundar An√°lise"
  - **P3 - Monitoramento**: Cards agrupados por tag com lista de not√≠cias em bullets
    - Primeiras 2-3 palavras em negrito
    - Resto do texto em fonte normal
    - Clique em qualquer bullet abre modal com detalhes completos
    - Total de artigos do grupo no cabe√ßalho

#### **Modal de Deep-Dive**
- **T√≠tulo do Evento**: Nome do cluster
- **Resumo Executivo**: Texto completo do resumo (sempre vis√≠vel)
- **Fontes Originais**: Lista de artigos que comp√µem o evento (sempre vis√≠vel)
- **Abas de Navega√ß√£o**: 
  - Conversar com o Cluster: Chat interativo com LLM
  - Gerenciar An√°lise: Edi√ß√£o de prioridade e tags
- **Lista de Fontes**: T√≠tulos dos artigos originais com links

#### **Chat Interativo**
- **Sess√µes Persistentes**: Chat salvo no banco de dados
- **Contexto Completo**: LLM tem acesso a todas as fontes do cluster
- **Prompt Especializado**: Prompt otimizado para an√°lise de Special Situations
- **Interface Responsiva**: Mensagens do usu√°rio e assistente diferenciadas
- **Loading States**: Indicadores de processamento durante resposta do LLM

#### **Gerenciamento de Clusters**
- **Edi√ß√£o de Prioridade**: Altera√ß√£o entre P1, P2 e P3
- **Gerenciamento de Tags**: Adi√ß√£o/remo√ß√£o de tags com interface visual
- **Hist√≥rico de Altera√ß√µes**: Registro completo de todas as modifica√ß√µes
- **Motivo das Altera√ß√µes**: Campo opcional para justificar mudan√ßas
- **Valida√ß√£o**: Verifica√ß√£o de integridade antes de salvar

#### **Interface de Settings**
- **CRUD Completo**: Para artigos, clusters e s√≠nteses
- **Pagina√ß√£o**: Navega√ß√£o por p√°ginas
- **Filtros**: Por status, categoria, prioridade
- **Edi√ß√£o Inline**: Modificar dados diretamente na interface

### **Funcionalidades JavaScript**

#### **Gerenciamento de Estado**
- `currentApiData`: Armazena dados atuais da API
- `refreshInterval`: Controle de atualiza√ß√£o autom√°tica
- `selectedDate`: Data selecionada pelo usu√°rio
- `isHistoricalView`: Indica se est√° visualizando dados hist√≥ricos

#### **Comunica√ß√£o com API**
- `fetchApiData()`: Busca dados do feed principal (com suporte a data)
- `fetchClusterDetails()`: Busca detalhes de cluster espec√≠fico
- Tratamento de erros e retry autom√°tico

#### **Renderiza√ß√£o Din√¢mica**
- `renderMetricas()`: Atualiza painel de m√©tricas
- `renderSintese()`: Atualiza s√≠ntese executiva
- `renderFeed()`: Renderiza cards de clusters
- `filterAndRender()`: Aplica filtros e re-renderiza

#### **Interatividade**
- **Seletor de Data**: Clique para abrir calend√°rio, sele√ß√£o autom√°tica
- **Filtros**: Prioridade e categoria com atualiza√ß√£o em tempo real
- **Modal**: Abertura/fechamento com carregamento ass√≠ncrono
- **Deep-Dive**: Navega√ß√£o entre abas e visualiza√ß√£o de fontes
- **Auto-refresh**: Atualiza√ß√£o autom√°tica a cada 30 segundos

#### **UX/UI**
- **Loading States**: Indicadores de carregamento
- **Error Handling**: Mensagens de erro amig√°veis
- **Success Feedback**: Confirma√ß√µes de a√ß√µes
- **Responsive Design**: Adapta√ß√£o a diferentes tamanhos de tela
- **Estado Hist√≥rico**: Destaque visual quando visualizando dados antigos

#### **Funcionalidades P3 - Monitoramento (NOVO)**
- **Agrupamento por Tag**: Not√≠cias P3 s√£o agrupadas por categoria/tag
- **Cards Consolidados**: Um card maior por tag cont√©m todas as not√≠cias P3 daquela categoria
- **Lista de Bullets**: Not√≠cias listadas como bullets simples dentro do card
- **Formata√ß√£o Inteligente**: Primeiras 2-3 palavras em negrito, resto em fonte normal
- **Interatividade Mantida**: Clique em qualquer bullet abre modal com detalhes completos
- **Contador de Artigos**: Total de artigos do grupo exibido no cabe√ßalho do card
- **Estilo Distintivo**: Design diferenciado para cards P3 (gradiente cinza, borda lateral)

---

## üîß **Scripts Dispon√≠veis**

### **`load_news.py` (135 linhas) - ATUALIZADO**
- ‚úÖ **Comportamento Padr√£o**: Carregamento de artigos brutos (sem processamento)
- ‚úÖ **JSONs**: Processamento direto dos dados originais (sem prioridade/tags)
- ‚úÖ **PDFs**: OCR + LLM para extrair not√≠cias individuais no formato do JSON
- ‚úÖ **Estrutura Unificada**: JSONs e PDFs usam a mesma estrutura de dados
- ‚úÖ **Dados Originais**: Salva apenas dados originais, sem processamento
- ‚úÖ **Status Pendente**: Artigos criados com status "pendente" para processamento posterior
- ‚úÖ **Suporte a M√∫ltiplas Fontes**: PDFs e JSONs
- ‚úÖ **Modo Direto ou via API**: Conex√£o direta ou HTTP
- ‚úÖ **Deduplica√ß√£o Autom√°tica**: Por hash √∫nico
- ‚úÖ **Processamento em Lote**: Diret√≥rio completo ou arquivo espec√≠fico
- ‚úÖ **Logs de Progresso**: Feedback detalhado

### **`process_articles.py` (775 linhas) - ATUALIZADO**
- ‚úÖ **Processamento em Lote**: Processa TODAS as not√≠cias primeiro
- ‚úÖ **Agrupamento Inteligente**: Usa LLM para agrupar por fato gerador (inspirado no `silva.py`)
- ‚úÖ **Cria√ß√£o de Clusters**: Agrupa por fato gerador ap√≥s processamento
- ‚úÖ **Gera√ß√£o Seletiva**: Resumos diferentes por prioridade
  - **P1**: Resumo longo (par√°grafo completo, sem limites)
  - **P2**: Resumo m√©dio (3-4 frases, com limite)
  - **P3**: Resumo curto (uma frase)
- ‚úÖ **Logs Detalhados**: Progresso e estat√≠sticas
- ‚úÖ **Tratamento de Erros**: Fallbacks e recupera√ß√£o
- ‚úÖ **Performance Otimizada**: √çndices por data para queries r√°pidas

### **`test_fluxo_completo.py` (164 linhas)**
- ‚úÖ Testa todo o pipeline
- ‚úÖ Verifica clusteriza√ß√£o
- ‚úÖ Valida acesso a artigos originais
- ‚úÖ Testa endpoints da API
- ‚úÖ Valida√ß√£o de dados

### **`test_ajustes.py` (200+ linhas) - NOVO**
- ‚úÖ Testa seletor de data no frontend
- ‚úÖ Verifica fun√ß√µes de busca por data
- ‚úÖ Valida novo fluxo de processamento
- ‚úÖ Testa importa√ß√µes e conex√µes
- ‚úÖ Verifica arquivos atualizados

### **`run_complete_workflow.py` (185 linhas)**
- ‚úÖ Automatiza todo o fluxo
- ‚úÖ Execu√ß√£o sequencial das etapas
- ‚úÖ Tratamento de erros
- ‚úÖ Verifica√ß√µes de ambiente
- ‚úÖ Feedback em tempo real

### **`start_dev.py` (135 linhas)**
- ‚úÖ Inicializa√ß√£o do backend
- ‚úÖ Configura√ß√£o de CORS
- ‚úÖ Servir frontend est√°tico
- ‚úÖ Logs de inicializa√ß√£o

### **`limpar_banco.py` (278 linhas)**
- ‚úÖ Utilit√°rio de limpeza do banco
- ‚úÖ Remo√ß√£o seletiva de dados
- ‚úÖ Backup antes da limpeza
- ‚úÖ Confirma√ß√µes de seguran√ßa

---

## üè∑Ô∏è **Classifica√ß√£o e Tags**

### **Tags (categoria):**
- `Governo e Politica`: Pol√≠tica econ√¥mica dom√©stica
- `Economia e Tecnologia`: Inova√ß√£o e neg√≥cios tech
- `Judicionario`: Decis√µes judiciais e legislativas
- `Empresas Privadas`: Movimentos corporativos

### **Prioridades:**
- `P1_CRITICO`: Fal√™ncias, M&A direto, crises de liquidez
- `P2_ESTRATEGICO`: Mudan√ßas regulat√≥rias, disputas corporativas
- `P3_MONITORAMENTO`: Contexto macro, tend√™ncias setoriais

---

## üìÅ **Formatos de Arquivo Suportados**

### **JSON (dump_crawlers)**
```json
[
  {
    "id_hash": "hash_unico",
    "titulo": "T√≠tulo da not√≠cia",
    "subtitulo": "Subt√≠tulo da not√≠cia",
    "texto_completo": "Texto completo da not√≠cia...",
    "link": "https://exemplo.com/noticia",
    "fonte": "Valor Econ√¥mico",
    "categoria": "Finan√ßas",
    "data_publicacao": "2025-08-01T10:00:00Z",
    "data_ultima_modificacao": "2025-08-01T10:00:00Z",
    "tags": ["tag1", "tag2"]
  }
]
```

### **PDF**
Arquivos PDF s√£o processados usando OCR (PyMuPDF) + LLM para extrair not√≠cias individuais no mesmo formato do JSON.

- Estrutura gerada por not√≠cia (artigo bruto):
  - `texto_bruto`: texto completo da not√≠cia detectada no PDF
  - `url_original`: sempre `null` (PDF n√£o cont√©m link)
  - `metadados`:
    - `titulo`: t√≠tulo extra√≠do para a not√≠cia
    - `fonte_original`: jornal (extra√≠do do PDF; fallback: nome do arquivo)
    - `arquivo_origem`: nome do PDF
    - `tipo_arquivo`: `pdf`
    - `data_processamento`: timestamp local GMT-3
    - Campos auxiliares quando dispon√≠veis: `jornal`, `pagina`, `autor`, `data_publicacao`, `categoria`
    - Campos apenas informativos vindos do LLM: `tag_ia`, `prioridade_ia`, `relevance_score_ia`, `relevance_reason_ia`

- Fallback sem IA (quando n√£o h√° `GEMINI_API_KEY` ou cliente indispon√≠vel):
  - O arquivo √© dividido por p√°ginas e cada p√°gina vira um artigo bruto com:
    - `metadados.fonte_original = <nome_do_arquivo_sem_extens√£o>`
    - `metadados.jornal = <nome_do_arquivo_sem_extens√£o>`
    - `metadados.pagina = <n√∫mero da p√°gina>`
  - Observa√ß√£o: a divis√£o por not√≠cias dentro da p√°gina requer IA. Sem IA, geramos 1 artigo por p√°gina para manter a ingest√£o.

---

## üîÑ **Pipeline de Processamento - ATUALIZADO**

### **ETAPA 1: Carregamento de Not√≠cias (load_news.py)**
- **JSONs**: Processamento direto dos dados originais (sem prioridade/tags)
- **PDFs**: OCR + LLM para extrair not√≠cias individuais no formato do JSON
  - Cada not√≠cia do PDF vira um artigo bruto separado
  - `metadados.fonte_original` √© sempre o jornal correto (ou derivado do arquivo)
  - `metadados.pagina` e `metadados.autor` s√£o preenchidos quando dispon√≠veis
- **Resultado**: Artigos brutos com dados originais e status "pendente" no banco
- **Sem processamento**: Apenas carregamento bruto, sem extra√ß√£o de dados ou agrupamento

### **ETAPA 2: Processar TODAS as Not√≠cias (process_articles.py)**
- Busca todos os artigos pendentes
- Processa cada artigo individualmente
- Extrai dados estruturados com LLM
- Gera embeddings para similaridade
- Atualiza status para "processado"

### **ETAPA 3: Criar Clusters/Agrupamentos**
- Busca todos os artigos processados hoje
- **Agrupamento Inteligente**: Usa LLM para agrupar por fato gerador (inspirado no `silva.py`)
- Cria novos clusters ou adiciona a existentes
- Atualiza embedding m√©dio dos clusters

### **ETAPA 4: Gerar Resumos Seletivos**
- **P1 - Cr√≠tico**: Resumo longo (par√°grafo completo, sem limites)
- **P2 - Estrat√©gico**: Resumo m√©dio (3-4 frases, com limite)
- **P3 - Monitoramento**: Resumo curto (uma frase)

### **ETAPA 5: Atualiza√ß√£o no Banco**
- Atualiza clusters com resumos
- Registra timestamps de processamento
- Cria logs detalhados

---

## üìà **Melhorias Implementadas**

### **1. Seletor de Data (NOVO)**
- ‚úÖ **Interface**: Seletor no topo do frontend
- ‚úÖ **Funcionalidade**: Filtro por data espec√≠fica
- ‚úÖ **API**: Endpoint suporta par√¢metro de data
- ‚úÖ **Backend**: Fun√ß√µes de busca por data
- ‚úÖ **UX**: Destaque visual para dados hist√≥ricos

### **2. Novo Fluxo de Processamento**
- ‚ùå **Antes**: Processamento incremental
- ‚úÖ **Agora**: Processamento em lote seguido de clusteriza√ß√£o

### **3. Resumos Diferenciados por Prioridade**
- ‚ùå **Antes**: Resumos iguais para todas as prioridades
- ‚úÖ **Agora**: 
  - **P1**: Resumo longo (par√°grafo completo)
  - **P2**: Resumo m√©dio (3-4 frases)
  - **P3**: Resumo curto (uma frase)

### **4. Rastreabilidade Total**
- ‚úÖ **Frontend**: Acesso aos artigos originais via drill-down
- ‚úÖ **Backend**: Relacionamentos corretos entre clusters e artigos
- ‚úÖ **API**: Endpoints para detalhes completos

### **5. Interface Administrativa**
- ‚úÖ **Settings**: CRUD completo para todos os dados
- ‚úÖ **Pagina√ß√£o**: Navega√ß√£o eficiente
- ‚úÖ **Filtros**: Busca e filtragem avan√ßada
- ‚úÖ **Edi√ß√£o**: Modifica√ß√£o inline de dados

### **6. Otimiza√ß√µes de Performance (NOVO)**
- ‚úÖ **√çndices de Performance**: √çndices por data para queries r√°pidas
- ‚úÖ **Pagina√ß√£o**: 20 itens por p√°gina no frontend
- ‚úÖ **Carregamento Lazy**: Textos completos carregados sob demanda
- ‚úÖ **Scroll Infinito**: Carrega mais not√≠cias automaticamente
- ‚úÖ **Modal de Detalhes**: Detalhes completos em modal elegante
- ‚úÖ **Queries Otimizadas**: √çndices compostos para filtros complexos

### **7. Robustez e Confiabilidade**
- ‚úÖ **Logs Detalhados**: Rastreamento completo de opera√ß√µes
- ‚úÖ **Tratamento de Erros**: Fallbacks e recupera√ß√£o
- ‚úÖ **Valida√ß√£o**: Pydantic para integridade de dados
- ‚úÖ **Deduplica√ß√£o**: Hash √∫nico para evitar duplicatas

### **8. Agrupamento P3 - Monitoramento (NOVO)**
- ‚úÖ **Agrupamento por Tag**: Not√≠cias P3 agrupadas por categoria
- ‚úÖ **Cards Consolidados**: Interface diferenciada para P3
- ‚úÖ **Lista de Bullets**: Apresenta√ß√£o simplificada das not√≠cias P3
- ‚úÖ **Formata√ß√£o Inteligente**: Primeiras palavras em negrito
- ‚úÖ **Interatividade Mantida**: Modal de detalhes funciona para bullets P3
- ‚úÖ **Estilo Distintivo**: Design diferenciado para cards P3

### **9. Chat Interativo com Clusters (NOVO)**
- ‚úÖ **Bot√£o Renomeado**: "Conversar com a Not√≠cia" em vez de "Aprofundar An√°lise"
- ‚úÖ **Modal Reorganizado**: Resumo e fontes sempre vis√≠veis
- ‚úÖ **Chat Persistente**: Sess√µes salvas no banco de dados
- ‚úÖ **LLM Integrado**: Prompt especializado para an√°lise de Special Situations
- ‚úÖ **Interface Responsiva**: Mensagens diferenciadas por tipo
- ‚úÖ **Loading States**: Indicadores de processamento
- ‚úÖ **Temperatura Zero**: LLM configurado para n√£o alucinar ou inventar informa√ß√µes
- ‚úÖ **Contexto Completo**: Hist√≥rico completo da conversa enviado para o LLM
- ‚úÖ **UX Melhorada**: Barra de chat posicionada acima das mensagens

### **10. Gerenciamento Avan√ßado de Clusters (NOVO)**
- ‚úÖ **Edi√ß√£o de Prioridade**: Altera√ß√£o entre P1, P2 e P3
- ‚úÖ **Gerenciamento de Tags**: Interface visual para adicionar/remover tags
- ‚úÖ **Hist√≥rico de Altera√ß√µes**: Registro completo de modifica√ß√µes
- ‚úÖ **Valida√ß√£o de Dados**: Verifica√ß√£o antes de salvar
- ‚úÖ **Auditoria**: Rastreamento de quem fez o qu√™ e quando

### **11. Upload e Processamento de Arquivos (NOVO)**
- ‚úÖ **Upload Multiplo**: Interface para upload de m√∫ltiplos arquivos PDF e JSON
- ‚úÖ **Processamento Inteligente**: PDFs processados com OCR e extra√ß√£o de not√≠cias via LLM
- ‚úÖ **Processamento Direto**: JSONs processados diretamente para extra√ß√£o de not√≠cias
- ‚úÖ **Processamento de Artigos**: Bot√£o para executar processamento equivalente ao `process_articles.py`
- ‚úÖ **Progresso Visual Detalhado**: Sistema de etapas visuais com indicadores de progresso
  - **Etapas Visuais**: Upload ‚Üí Processamento ‚Üí Banco ‚Üí Conclu√≠do
  - **Progresso em Tempo Real**: Contador de artigos processados
  - **Status Detalhado**: Informa√ß√µes sobre arquivo atual e progresso
  - **Logs Visuais**: Feedback completo sobre cada etapa do processamento
- ‚úÖ **Monitoramento Avan√ßado**: Interface com progresso granular e status em tempo real
- ‚úÖ **Resultados Detalhados**: Feedback completo sobre arquivos processados e artigos criados

### **12. Sistema de Progresso Visual (NOVO)**
- ‚úÖ **Etapas Visuais**: Sistema de 4 etapas com √≠cones e cores
  - **üì§ Enviando arquivo**: Etapa inicial de upload
  - **‚öôÔ∏è Processando conte√∫do**: Extra√ß√£o e an√°lise do arquivo
  - **üíæ Salvando no banco**: Persist√™ncia dos dados
  - **‚úÖ Conclu√≠do**: Finaliza√ß√£o do processo
- ‚úÖ **Indicadores de Status**: Cores din√¢micas (cinza ‚Üí azul ‚Üí verde)
- ‚úÖ **Progresso Real em Tempo Real**: Contador de artigos processados com dados reais do backend
- ‚úÖ **Informa√ß√µes Detalhadas**: Nome do arquivo, progresso atual, status do processamento
- ‚úÖ **Feedback Visual**: Transi√ß√µes suaves entre etapas
- ‚úÖ **Logs em Tempo Real**: Informa√ß√µes detalhadas sobre cada etapa do processamento
- ‚úÖ **Polling Inteligente**: Sistema de polling que verifica progresso real a cada segundo
- ‚úÖ **Tracking de Progresso**: Backend atualiza progresso em tempo real durante processamento
- ‚úÖ **Timeout Prote√ß√£o**: Sistema para evitar polling infinito (m√°ximo 5 minutos)

### **13. Agrupamento Incremental com Pivot Autom√°tico (RESTAURADO)**
- ‚úÖ **Processamento Inteligente**: Quando o bot√£o "Processar Artigos Pendentes" √© clicado ap√≥s upload de not√≠cias
- ‚úÖ **Pivot Autom√°tico**: Sistema escolhe automaticamente o algoritmo correto:
  - **Se h√° clusters existentes**: Usa agrupamento incremental (anexa a clusters existentes)
  - **Se n√£o h√° clusters**: Usa agrupamento original (cria clusters do zero)
- ‚úÖ **Identifica√ß√£o de Novos Artigos**: Sistema identifica automaticamente artigos processados hoje que n√£o foram associados a clusters
- ‚úÖ **Clusters Existentes**: Busca todos os clusters criados no mesmo dia para an√°lise
- ‚úÖ **Prompt Especializado**: Novo prompt `PROMPT_AGRUPAMENTO_INCREMENTAL_V1` para an√°lise incremental
- ‚úÖ **L√≥gica Id√™ntica**: Prompt mant√©m a mesma l√≥gica do agrupamento original para evitar bias
- ‚úÖ **Prote√ß√£o de Clusters**: LLM recebe instru√ß√µes expl√≠citas para N√ÉO alterar clusters existentes
- ‚úÖ **Classifica√ß√£o Inteligente**: Para cada artigo novo, o LLM decide:
  - **Anexar**: Se o artigo se refere ao mesmo fato gerador de um cluster existente
  - **Novo Cluster**: Se o artigo se refere a um fato gerador diferente
- ‚úÖ **Integridade Total**: Todos os artigos novos s√£o classificados (anexados ou em novos clusters)
- ‚úÖ **Logs Detalhados**: Rastreamento completo de anexa√ß√µes e novos clusters criados
- ‚úÖ **Performance Otimizada**: Processamento eficiente com mapeamento de IDs e valida√ß√µes
- ‚úÖ **Otimiza√ß√£o de Dados**: Passa apenas t√≠tulos e IDs para o LLM, evitando timeouts com centenas de artigos
- ‚úÖ **FUN√á√ÉO IMPLEMENTADA**: `agrupar_noticias_incremental()` agora est√° funcionando corretamente
- ‚úÖ **PIVOT AUTOM√ÅTICO**: `processar_artigos_pendentes()` agora escolhe automaticamente entre incremental e em lote

### **14. Corre√ß√£o de Bug de Duplica√ß√£o (NOVO)**
- ‚úÖ **Problema Identificado**: Clusters duplicados sendo criados devido a processamento duplo
- ‚úÖ **Causa Raiz**: `process_articles.py` fazia clusteriza√ß√£o tanto na ETAPA 1 quanto na ETAPA 2
- ‚úÖ **Solu√ß√£o Implementada**: 
  - **ETAPA 1**: Processamento de artigos sem clusteriza√ß√£o
  - **ETAPA 2**: Agrupamento inteligente com prompt (√∫nica clusteriza√ß√£o)
- ‚úÖ **Verifica√ß√µes de Duplica√ß√£o**: 
  - `create_cluster`: Verifica clusters existentes antes de criar
  - `associate_artigo_to_cluster`: Evita associa√ß√µes duplicadas
- ‚úÖ **Contagem Correta**: `total_artigos` agora reflete o n√∫mero real de artigos associados

### **15. Estrutura de Dados Unificada (NOVO)**
- ‚úÖ **Problema Identificado**: Dados de processamento (prioridade, tags) sendo salvos nos artigos brutos
- ‚úÖ **Solu√ß√£o Implementada**: Separa√ß√£o clara entre dados originais e dados processados
- ‚úÖ **Novos Campos no Banco**: 
  - `subtitulo`: Subt√≠tulo original da not√≠cia (TEMPORARIAMENTE COMENTADO)
  - `data_ultima_modificacao`: Data de √∫ltima modifica√ß√£o (TEMPORARIAMENTE COMENTADO)
  - `id_hash_original`: ID hash original do JSON (TEMPORARIAMENTE COMENTADO)
  - `fonte_original`: Fonte original (ex: "Valor Econ√¥mico") (TEMPORARIAMENTE COMENTADO)
  - `tags_originais`: Tags originais como array JSON (TEMPORARIAMENTE COMENTADO)
- ‚úÖ **Estrutura Unificada**: JSONs e PDFs agora usam a mesma estrutura de dados
- ‚úÖ **Processamento Limpo**: Prioridade e tags s√£o definidas apenas no processamento, n√£o no carregamento
- ‚ö†Ô∏è **NOTA**: Novos campos est√£o comentados temporariamente para evitar quebra do sistema. Ser√£o adicionados via migra√ß√£o quando necess√°rio.

### **16. Corre√ß√£o do Fluxo de Classifica√ß√£o (NOVO)**
- ‚úÖ **Problema Identificado**: ETAPA 2 estava mostrando prioridades (P3_MONITORAMENTO) quando deveria s√≥ agrupar
- ‚úÖ **Causa**: C√≥digo estava definindo prioridade na ETAPA 2 baseado nos artigos individuais
- ‚úÖ **Solu√ß√£o Implementada**: 
  - **ETAPA 2**: S√≥ agrupa not√≠cias por fato gerador (sem classificar)
  - **ETAPA 3**: Usa prompts do `prompts.py` para classificar cada cluster completo
  - **Fluxo Correto**: Agrupamento ‚Üí Classifica√ß√£o ‚Üí Resumo
- ‚úÖ **Tags Unificadas**: Modelos agora usam as 8 tags do `TAGS_SPECIAL_SITUATIONS` diretamente
- ‚úÖ **Modo Debug**: Adicionado debug detalhado para rastrear prompts enviados ao Gemini e respostas recebidas
- ‚úÖ **Tratamento de Irrelevantes**: Not√≠cias irrelevantes s√£o marcadas como "IRRELEVANTE" e omitidas do feed
- ‚úÖ **Resultado**: Separa√ß√£o clara entre agrupamento e classifica√ß√£o, usando prompts especializados

### **17. Migra√ß√£o de Tags Antigas (NOVO)**
- ‚úÖ **Problema Identificado**: Artigos existentes no banco tinham tags antigas que n√£o s√£o mais v√°lidas
- ‚úÖ **Causa**: Mudan√ßa de 4 tags antigas para 8 tags especializadas do `TAGS_SPECIAL_SITUATIONS`
- ‚úÖ **Solu√ß√£o Implementada**: 
  - **Fun√ß√£o de Migra√ß√£o**: `migrar_tag_antiga_para_nova()` converte tags antigas para novas
  - **Fun√ß√£o de Corre√ß√£o**: `corrigir_tag_invalida()` atualizada para usar as 8 tags especializadas
  - **Mapeamento**: 'Economia e Tecnologia' ‚Üí 'Internacional (Economia e Pol√≠tica)'
  - **Compatibilidade**: Artigos antigos s√£o processados sem erro de valida√ß√£o
- ‚úÖ **Resultado**: Sistema funciona com artigos antigos e novos, usando tags especializadas

### **18. Tags Din√¢micas no Frontend (NOVO)**
- ‚úÖ **Problema Identificado**: Frontend ainda usava tags fixas antigas (4 categorias)
- ‚úÖ **Causa**: HTML e JavaScript tinham tags hardcoded em vez de usar as tags reais dos clusters
- ‚úÖ **Solu√ß√£o Implementada**: 
  - **Carregamento dos Clusters**: Frontend busca clusters existentes para extrair tags √∫nicas
  - **Extra√ß√£o Din√¢mica**: Tags s√£o extra√≠das dos clusters reais no banco de dados
  - **Filtros Atualizados**: Categorias no frontend agora usam tags din√¢micas dos dados
  - **Fallback**: Se API falhar, usa tags antigas como backup
- ‚úÖ **Resultado**: Frontend agora exibe as categorias reais baseadas nos clusters existentes

### **19. Corre√ß√£o de Dados de Teste para Datas Vazias (NOVO)**
- ‚úÖ **Problema Identificado**: Datas futuras ou sem registros exibiam dados de teste antigos
- ‚úÖ **Causa**: API retornava dados de teste quando n√£o havia clusters reais na data solicitada
- ‚úÖ **Solu√ß√£o Implementada**: 
  - **Remo√ß√£o de Dados de Teste**: API agora retorna dados vazios para datas sem registros
  - **Limpeza de Cache**: Frontend limpa dados antigos quando n√£o h√° clusters
  - **M√©tricas Zeradas**: Retorna m√©tricas com valores 0 para datas vazias
- ‚úÖ **Resultado**: Datas futuras ou sem dados agora mostram corretamente "sem not√≠cias"

### **20. Padroniza√ß√£o de Fuso Hor√°rio GMT-3 (NOVO)**
- ‚úÖ **Problema Identificado**: Diferen√ßas de fuso hor√°rio entre frontend e backend causavam inconsist√™ncias de data
- ‚úÖ **Causa**: Sistema usava fuso hor√°rio local do servidor vs. fuso hor√°rio do navegador
- ‚úÖ **Solu√ß√£o Implementada**: 
  - **Backend Python**: Criadas fun√ß√µes utilit√°rias em `utils.py` para GMT-3 (S√£o Paulo/Bras√≠lia)
  - **Fun√ß√µes Criadas**: `get_datetime_brasil()`, `get_date_brasil()`, `get_datetime_brasil_str()`, `get_date_brasil_str()`
  - **Substitui√ß√µes**: Todas as ocorr√™ncias de `datetime.now()` e `date.today()` padronizadas
  - **Frontend JavaScript**: Ajustado para usar GMT-3 em `getTodayDate()` e compara√ß√µes de data
- ‚úÖ **Resultado**: Todas as datas e hor√°rios agora seguem o mesmo padr√£o GMT-3 (S√£o Paulo/Bras√≠lia)

### **21. Corre√ß√µes Cr√≠ticas do Processamento de Artigos (NOVO)**
- ‚úÖ **Problema Identificado**: `process_articles.py` falhava no agrupamento com JSON malformado
- ‚úÖ **Causa**: Fun√ß√£o `extrair_json_da_resposta` n√£o era robusta e prompt causava truncamento
- ‚úÖ **Solu√ß√£o Implementada**: 
  - **Fun√ß√£o Robusta**: `extrair_json_da_resposta` agora tem 3 tentativas de corre√ß√£o
  - **Prompt Otimizado**: Reduzido tamanho de trechos e aumentado `max_output_tokens` para 8192
  - **Configura√ß√£o Melhorada**: `temperature=0.1` (mais determin√≠stico) e `top_p=0.8`
  - **Debug Detalhado**: Logs com emojis e informa√ß√µes detalhadas de cada etapa
  - **Tratamento de Erros**: M√∫ltiplas tentativas de corre√ß√£o de JSON incompleto
- ‚úÖ **Resultado**: Agrupamento agora funciona de forma robusta e com debug detalhado

### **22. Restaura√ß√£o da Funcionalidade de Agrupamento Incremental (NOVO)**
- ‚úÖ **Problema Identificado**: Funcionalidade de agrupamento incremental foi perdida durante reestrutura√ß√£o
- ‚úÖ **Causa**: Durante as corre√ß√µes do `process_articles.py`, a l√≥gica de pivot autom√°tico n√£o foi implementada
- ‚úÖ **Solu√ß√£o Implementada**: 
  - **Fun√ß√£o Restaurada**: `agrupar_noticias_incremental()` implementada com l√≥gica completa
  - **Pivot Autom√°tico**: `processar_artigos_pendentes()` agora verifica clusters existentes e escolhe o modo correto
  - **Prompt Especializado**: Usa `PROMPT_AGRUPAMENTO_INCREMENTAL_V1` para decis√µes inteligentes
  - **L√≥gica Completa**: Anexa a clusters existentes ou cria novos clusters conforme necess√°rio
  - **Debug Detalhado**: Logs com emojis e informa√ß√µes detalhadas de cada decis√£o
  - **Corre√ß√£o do Prompt**: Chaves `{}` no JSON de exemplo foram escapadas para `{{}}` para evitar conflito com `.format()`
- ‚úÖ **Resultado**: Agrupamento incremental agora funciona corretamente, permitindo receber novas not√≠cias durante o dia

### **23. Corre√ß√µes Cr√≠ticas de Processamento e Status (NOVO)**
- ‚úÖ **Problema 1 Identificado**: Agrupamento incremental falhava com muitas not√≠cias (174 artigos) devido a truncamento de JSON
- ‚úÖ **Problema 2 Identificado**: `load_news.py` definia tags e prioridades inv√°lidas que poderiam causar erros de valida√ß√£o
- ‚úÖ **Problema 3 Identificado**: Artigos eram marcados como "processado" na ETAPA 1, impedindo reprocessamento se clusteriza√ß√£o falhasse
- ‚úÖ **Problema 4 Identificado**: ETAPA 3 processava clusters antigos em vez de apenas clusters novos sem resumo
- ‚úÖ **Problema 5 Identificado**: Inconsist√™ncia de status entre ETAPA 1 e ETAPA 2 causava falha no agrupamento
- ‚úÖ **Problema 6 Identificado**: Status "pronto_para_agrupamento" excedia limite de 20 caracteres do banco de dados
- ‚úÖ **Problema 7 Identificado**: Modelo Pydantic n√£o aceitava "PENDING" como valor v√°lido para tag e prioridade
- ‚úÖ **Problema 8 Identificado**: Fun√ß√£o `get_artigos_by_cluster` filtrava por status "processado", mas artigos estavam com status "pronto_agrupar"
- ‚úÖ **Problema 9 Identificado**: Clusters P3 n√£o apareciam no frontend devido a falta de logs de debug
- ‚úÖ **Problema 10 Identificado**: Frontend precisava de melhor aproveitamento de espa√ßo e experi√™ncia de usu√°rio
- ‚úÖ **Problema 11 Identificado**: Interface precisava de redesign completo com paleta de cores e layout otimizado
- ‚úÖ **Solu√ß√µes Implementadas**:
  - **Chunking Autom√°tico**: Agrupamento incremental agora processa em lotes de 50 not√≠cias para evitar truncamento
  - **Tags/Prioridades Neutras**: `load_news.py` e `process_articles.py` agora definem `tag: 'PENDING'` e `prioridade: 'PENDING'` em vez de valores inv√°lidos
  - **Modelos Pydantic Atualizados**: Adicionado "PENDING" aos tipos `TagType` e `PrioridadeType` para aceitar valores pendentes
  - **Import Corrigido**: Adicionado `get_datetime_brasil_str` ao import de `backend.utils`
  - **Fun√ß√£o CRUD Corrigida**: `get_artigos_by_cluster` agora busca artigos independente do status
  - **Logs de Debug Aprimorados**: Adicionados logs detalhados para debug de renderiza√ß√£o de clusters P3
  - **Script de Teste**: Criado `test_p3_debug.js` para verificar dados no console do navegador
  - **Paleta de Cores Pastel**: Implementada paleta de 10 cores pastel estilo Tableau para tags
  - **Layout Otimizado**: Footer dos cards reorganizado com timestamp, tags e bot√£o na mesma linha
  - **Tags Coloridas**: Cada tag tem cor √∫nica baseada em hash do nome, formato arredondado
  - **Sidebar Redesenhada**: Filtros de categoria agora s√£o tags coloridas clic√°veis
  - **Header Reorganizado**: Bot√£o Settings movido para canto superior direito
  - **M√©tricas Simplificadas**: Apenas Not√≠cias Coletadas, Eventos √önicos e Fontes Diferentes
  - **Contagem de Fontes**: Nova m√©trica com count distinct de fontes diferentes
  - **Status Inteligente**: Artigos s√≥ s√£o marcados como "processado" ap√≥s clusteriza√ß√£o bem-sucedida na ETAPA 2
  - **Status Intermedi√°rio**: Novo status "pronto_agrupar" (15 chars) entre "pendente" e "processado"
  - **Fun√ß√£o de Marca√ß√£o**: Nova fun√ß√£o `marcar_artigos_processados()` garante consist√™ncia de status
  - **Nova Fun√ß√£o CRUD**: `update_artigo_dados_sem_status()` atualiza dados sem alterar status
  - **Filtro de Resumo**: ETAPA 3 agora processa apenas clusters sem resumo (`resumo_cluster.is_(None)`)
  - **L√≥gica Consistente**: ETAPA 1 marca como "pronto_agrupar", ETAPA 2 busca esse status
- ‚úÖ **Resultado**: Sistema mais robusto, sem perda de dados, reprocessamento poss√≠vel, processamento eficiente apenas de clusters novos e agrupamento funcionando corretamente

---

## üõ†Ô∏è **Troubleshooting**

### **Problema: "Python was not found"**
**Solu√ß√£o**: Use o Anaconda Prompt em vez do CMD do Windows

### **Problema: "UnicodeEncodeError"**
**Solu√ß√£o**: Use os scripts sem emojis (j√° corrigidos)

### **Problema: "GEMINI_API_KEY n√£o configurada"**
**Solu√ß√£o**: Configure a chave no arquivo `backend/.env`. O sistema tem fallback para OCR completo se n√£o dispon√≠vel

### **Problema: "Erro de conex√£o com banco"**
**Solu√ß√£o**: Verifique se o PostgreSQL est√° rodando na porta 5433

### **Problema: "Nenhum artigo pendente encontrado"**
**Solu√ß√£o**: Execute primeiro o `load_news.py` para carregar artigos

### **Problema: "API n√£o est√° dispon√≠vel"**
**Solu√ß√£o**: Use `--direct` no `load_news.py` para conectar diretamente ao banco

### **Problema: "Erro no processamento"**
**Solu√ß√£o**: Verifique os logs e execute `test_fluxo_completo.py` para diagn√≥stico

### **Problema: "Seletor de data n√£o funciona"**
**Solu√ß√£o**: Execute `test_ajustes.py` para verificar se as altera√ß√µes est√£o funcionando

### **Problema: "PDFs n√£o s√£o processados corretamente"**
**Solu√ß√£o**: O `load_news.py` agora apenas carrega artigos brutos. Para processamento inteligente, execute `process_articles.py` ap√≥s o carregamento.

### **Problema: "Sistema lento com muitas not√≠cias"**
**Solu√ß√£o**: As otimiza√ß√µes de performance foram implementadas. Use pagina√ß√£o e carregamento lazy no frontend

---

## üìä **Monitoramento**

### **Verificar Status**
```bash
# Status da API
curl "http://localhost:8000/health"

# Estat√≠sticas do sistema
curl "http://localhost:8000/admin/stats"

# Feed com data espec√≠fica
curl "http://localhost:8000/api/feed?data=2025-01-15"
```

### **Logs**
Os logs s√£o salvos na tabela `logs_processamento` com n√≠veis:
- `INFO`: Opera√ß√µes normais
- `WARNING`: Situa√ß√µes inesperadas
- `ERROR`: Erros que requerem aten√ß√£o

---

## üîê **Seguran√ßa**

### **Configura√ß√µes de Seguran√ßa**
1. **Nunca** commitar arquivos `.env`
2. Usar senhas fortes para PostgreSQL
3. Configurar firewall para portas espec√≠ficas
4. Atualizar depend√™ncias regularmente
5. Configurar HTTPS em produ√ß√£o

### **Backup do Banco**
```bash
# Backup autom√°tico
pg_dump btg_alphafeed > backup_$(date +%Y%m%d).sql

# Restaurar backup
psql btg_alphafeed < backup_20250101.sql
```

---

## üß™ **Testes**

### **Teste de Conex√£o**
```bash
python tests/test_imports.py
```

### **Teste do Fluxo Completo**
```bash
python test_fluxo_completo.py
```

### **Teste das Altera√ß√µes (NOVO)**
```bash
python test_ajustes.py
```

### **Verifica√ß√£o Manual**
1. Execute o fluxo completo
2. Acesse o frontend
3. Teste o seletor de data
4. Verifique se os clusters aparecem
5. Teste o drill-down nos eventos
6. Confirme acesso aos artigos originais

---

## üéØ **Resultado Esperado**

Ap√≥s execu√ß√£o bem-sucedida:
1. ‚úÖ Artigos brutos carregados no banco (not√≠cias individuais dos PDFs)
2. ‚úÖ Artigos processados e classificados
3. ‚úÖ Clusters criados com prioridades corretas (agrupamento inteligente)
4. ‚úÖ Resumos gerados com tamanhos diferenciados por prioridade
5. ‚úÖ Frontend funcionando com seletor de data
6. ‚úÖ Acesso total aos artigos originais
7. ‚úÖ Interface administrativa dispon√≠vel
8. ‚úÖ Logs detalhados para monitoramento
9. ‚úÖ Visualiza√ß√£o hist√≥rica por data
10. ‚úÖ Performance otimizada com pagina√ß√£o e carregamento lazy
11. ‚úÖ Modal de detalhes para textos completos
12. ‚úÖ √çndices de performance aplicados no banco

---

## üìû **Suporte**

### **Checklist de Diagn√≥stico**
- [ ] Ambiente conda `pymc2` ativo
- [ ] PostgreSQL rodando e acess√≠vel
- [ ] Arquivo `.env` configurado corretamente
- [ ] Depend√™ncias Python instaladas
- [ ] Banco de dados inicializado
- [ ] API Gemini funcionando (ou fallback para OCR)
- [ ] Frontend acess√≠vel em localhost:8000
- [ ] Seletor de data funcionando
- [ ] Testes passando (`test_ajustes.py`)
- [ ] Extra√ß√£o inteligente de PDFs funcionando
- [ ] Agrupamento inteligente funcionando
- [ ] Performance otimizada (pagina√ß√£o e carregamento lazy)

### **Logs √öteis**
```bash
# Verificar logs de erro da aplica√ß√£o
grep ERROR /var/log/btg_alphafeed.log

# Verificar uso de recursos
ps aux | grep python
df -h
free -m
```

Para problemas n√£o resolvidos:
1. Verificar documenta√ß√£o neste README
2. Consultar logs detalhados
3. Testar componentes individualmente
4. Executar `test_ajustes.py` para diagn√≥stico
5. Reportar problema com logs completos